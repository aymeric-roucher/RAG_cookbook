{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from datasets import Dataset\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RAG\n",
    "\n",
    "RAG, or Retrieval Augmented Generation, is an architecture that uses a pre-existing knowledge base to answer queries.\n",
    "\n",
    "This process consists of two steps:\n",
    "\n",
    "1. A Retriever, functioning like an internal search engine, identifies the most relevant documents from the knowledge base to respond to the query.\n",
    "2. A Reader Language Model (LLM) reads these retrieved documents to produce an answer to the user’s query.\n",
    "\n",
    "Here is the workflow:\n",
    "\n",
    "<img src=\"rag_workflow.png\" height=\"700\">\n",
    "\n",
    "💡 As you can see, there are many steps to tune in this architecture: tuning the system properly will yield significant performance gains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "ds = datasets.load_dataset(\"A-Roucher/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Retriever - embeddings 🗂️\n",
    "The __retriever acts like an internal search engine__: given the user query, it returns the most relevant documents from your knowledge base.\n",
    "\n",
    "> For the knowledge base, we use Langchain vector databases since __it offers a convenient [FAISS](https://github.com/facebookresearch/faiss) index and allows us to keep document metadata throughout the processing__.\n",
    "\n",
    "🛠️ __Options included:__\n",
    "\n",
    "- Tune the chunking method:\n",
    "    - Size of the chunks\n",
    "    - Method: split on different separators, use [semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
    "- Change the embedding model\n",
    "\n",
    "👷‍♀️ __More could be considered:__\n",
    "- Try another chunking method, like semantic chunking\n",
    "- Change the index used (here, FAISS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the vector database: preprocessing\n",
    "\n",
    "- In this part, __we split the documents from our knowledge base into smaller chunks__ which will be the snippets on which the reader LLM will base its answer.\n",
    "- The goal is to have semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting each idea.\n",
    "\n",
    "> We use Langchain's `RecursiveCharacterTextSplitter`, which makes efficient use of code language detection to make better splits.\n",
    "\n",
    "> 👀 Depending on your embedding model, the documents you are trying to embed might be truncated over 512 tokens: so you may want to visualize the length of your chunks.\n",
    "\n",
    "💡 To help you tune this step, [this space](https://huggingface.co/spaces/A-Roucher/chunk_visualizer) lets you visualize how different chunking options affect the chunks you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9023b8a1161412d8797a33186ac77f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int, knowledge_base: List[LangchainDocument]\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    return docs_processed\n",
    "\n",
    "\n",
    "docs_processed = split_documents(1000, RAW_KNOWLEDGE_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in docs_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(lengths).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬇️🚨 The cell below takes a few minutes to run on A10G!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True\n",
    "        },  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = (\n",
    "        f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    )\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=100,\n",
    "            add_start_index=True,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        )\n",
    "        docs_processed = []\n",
    "        for doc in langchain_docs:\n",
    "            docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever\n",
    "- When the vector database is built, the retriever works by embedding the user query and searching for the top `k` most similar documents in the vector database.\n",
    "- Here, this operation is transparently performed by the method `KNOWLEDGE_INDEX.similarity_search(query)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index not found, generating it...\n",
      "\n",
      "Starting retrieval:\n",
      "Start by inheriting the base class `Pipeline` with the 4 methods needed to implement `preprocess`,\n",
      "`_forward`, `postprocess`, and `_sanitize_parameters`.\n",
      "\n",
      "\n",
      "```python\n",
      "from transformers import Pipeline\n",
      "\n",
      "\n",
      "class MyPipeline(Pipeline):\n",
      "    def _sanitize_parameters(self, **kwargs):\n",
      "        preprocess_kwargs = {}\n",
      "        if \"maybe_arg\" in kwargs:\n",
      "            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
      "        return preprocess_kwargs, {}, {}\n",
      "\n",
      "    def preprocess(self, inputs, maybe_arg=2):\n",
      "        model_input = Tensor(inputs[\"input_ids\"])\n",
      "        return {\"model_input\": model_input}\n",
      "\n",
      "    def _forward(self, model_inputs):\n",
      "        # model_inputs == {\"model_input\": model_input}\n",
      "        outputs = self.model(**model_inputs)\n",
      "        # Maybe {\"logits\": Tensor(...)}\n",
      "        return outputs\n",
      "\n",
      "    def postprocess(self, model_outputs):\n",
      "        best_class = model_outputs[\"logits\"].softmax(-1)\n",
      "        return best_class\n",
      "```\n",
      "\n",
      "The structure of this breakdown is to support relatively seamless support for CPU/GPU, while supporting doing\n",
      "pre/postprocessing on the CPU on different threads\n",
      "\n",
      "`preprocess` will take the originally defined inputs, and turn them into something feedable to the model. It might\n",
      "contain more information and is usually a `Dict`.\n",
      "{'source': 'huggingface/transformers/blob/main/docs/source/en/add_new_pipeline.md', 'start_index': 1360}\n"
     ]
    }
   ],
   "source": [
    "KNOWLEDGE_INDEX = load_embeddings(\n",
    "    RAW_KNOWLEDGE_BASE, chunk_size=1500, model_name=\"thenlper/gte-small\"\n",
    ")\n",
    "print(\"\\nStarting retrieval:\")\n",
    "docs = KNOWLEDGE_INDEX.similarity_search(query=\"How to create a pipeline object?\", k=5)\n",
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Reader - LLM 💬\n",
    "\n",
    "In this part, the __LLM Reader reads the retrieved documents to formulate its answer.__\n",
    "\n",
    "🛠️ Here we tried the following options to improve results:\n",
    "- Switch reranking on/off\n",
    "- Change the reader model\n",
    "\n",
    "💡 Many options could be considered here to further improve the results:\n",
    "- Tune the prompt template\n",
    "- Compression the retrieved context to keep only the relevant parts to answer the query.\n",
    "- Extend the RAG system to make it more user friendly:\n",
    "    - cite source\n",
    "    - make conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context, \n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\n",
    "    \"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", max_new_tokens=3500\n",
    ")\n",
    "\n",
    "llm(\"Ok,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "API_URL = \"https://ytjpei7t003tedav.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.getenv('HF_TOKEN')}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_llm_answer(question):\n",
    "    payload = {\n",
    "        \"inputs\": question,\n",
    "        \"max_new_tokens\": 3000,\n",
    "    }\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "\n",
    "def apply_reranking(\n",
    "    question: str,\n",
    "    documents: List[LangchainDocument],\n",
    "    reranker: RAGPretrainedModel,\n",
    "    num_docs_final: Optional[int] = 5,\n",
    ") -> List[LangchainDocument]:\n",
    "    # keep only the top k documents\n",
    "    documents_sorted_in_order = reranker.rerank(question, documents, k=num_docs_final)\n",
    "\n",
    "    # Even and odd documents are ordered in opposite directions to put the most relevant documents at the beginning and end\n",
    "    documents_even = documents_sorted_in_order[::2]\n",
    "    documents_odd = documents_sorted_in_order[1::2]\n",
    "    documents = documents_even + documents_odd[::-1]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: BaseChatModel,\n",
    "    knowledge_index: FAISS,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(\n",
    "        query=question, k=num_retrieved_docs\n",
    "    )\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        apply_reranking(\n",
    "            question, relevant_docs, reranker, num_docs_final=num_docs_final\n",
    "        )\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"{str(i)}: \" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "\n",
    "    # Redact an answer\n",
    "    output = llm(final_prompt)\n",
    "    try:\n",
    "        answer = output[0][\"generated_text\"]\n",
    "    except:\n",
    "        # Retry with shorter contect\n",
    "        context = \"\\nExtracted documents:\\n\"\n",
    "        context += \"\".join(\n",
    "            [f\"{str(i)}: \" + doc for i, doc in enumerate(relevant_docs[:-1])]\n",
    "        )\n",
    "\n",
    "        final_prompt = RAG_PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "\n",
    "        # Redact an answer\n",
    "        output = llm(final_prompt)\n",
    "        answer = output[0][\"generated_text\"]\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Test\n",
    "Let's see how our RAG pipeline answers a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how to create a pipeline object?\"\n",
    "answer, relevant_docs = answer_with_rag(\n",
    "    question, get_llm_answer, KNOWLEDGE_INDEX, reranker=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "To create a pipeline object, you need to inherit from the base `Pipeline` class provided by the `transformers` library. This involves defining four methods: `preprocess`, `_forward`, `postprocess`, and `_sanitize_parameters`. Here's an example:\n",
      "\n",
      "```python\n",
      "from transformers import Pipeline\n",
      "\n",
      "class MyPipeline(Pipeline):\n",
      "    def _sanitize_parameters(self, **kwargs):\n",
      "        preprocess_kwargs = {}\n",
      "\n",
      "\n",
      "Source documents:\n",
      "Start by inheriting the base class `Pipeline` with the 4 methods needed to implement `preprocess`,\n",
      "`_forward`, `postprocess`, and `_sanitize_parameters`.\n",
      "\n",
      "\n",
      "```python\n",
      "from transformers import Pipeline\n",
      "\n",
      "\n",
      "class MyPipeline(Pipeline):\n",
      "    def _sanitize_parameters(self, **kwargs):\n",
      "        preprocess_kwargs = {}\n",
      "        if \"maybe_arg\" in kwargs:\n",
      "            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
      "        return preprocess_kwargs, {}, {}\n",
      "\n",
      "    def preprocess(self, inputs, maybe_arg=2):\n",
      "        model_input = Tensor(inputs[\"input_ids\"])\n",
      "        return {\"model_input\": model_input}\n",
      "\n",
      "    def _forward(self, model_inputs):\n",
      "        # model_inputs == {\"model_input\": model_input}\n",
      "        outputs = self.model(**model_inputs)\n",
      "        # Maybe {\"logits\": Tensor(...)}\n",
      "        return outputs\n",
      "\n",
      "    def postprocess(self, model_outputs):\n",
      "        best_class = model_outputs[\"logits\"].softmax(-1)\n",
      "        return best_class\n",
      "```\n",
      "\n",
      "The structure of this breakdown is to support relatively seamless support for CPU/GPU, while supporting doing\n",
      "pre/postprocessing on the CPU on different threads\n",
      "\n",
      "`preprocess` will take the originally defined inputs, and turn them into something feedable to the model. It might\n",
      "contain more information and is usually a `Dict`.\n",
      "- **Self-contained**: A pipeline shall be as self-contained as possible. More specifically, this means that all functionality should be either directly defined in the pipeline file itself, should be inherited from (and only from) the [`DiffusionPipeline` class](https://github.com/huggingface/diffusers/blob/5cbed8e0d157f65d3ddc2420dfd09f2df630e978/src/diffusers/pipeline_utils.py#L56) or be directly attached to the model and scheduler components of the pipeline.\n",
      "- **Easy-to-use**: Pipelines should be extremely easy to use - one should be able to load the pipeline and\n",
      "use it for its designated task, *e.g.* text-to-image generation, in just a couple of lines of code. Most\n",
      "logic including pre-processing, an unrolled diffusion loop, and post-processing should all happen inside the `__call__` method.\n",
      "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# How to create a custom pipeline?\n",
      "\n",
      "In this guide, we will see how to create a custom pipeline and share it on the [Hub](hf.co/models) or add it to the\n",
      "🤗 Transformers library.\n",
      "\n",
      "First and foremost, you need to decide the raw entries the pipeline will be able to take. It can be strings, raw bytes,\n",
      "dictionaries or whatever seems to be the most likely desired input. Try to keep these inputs as pure Python as possible\n",
      "as it makes compatibility easier (even through other languages via JSON). Those will be the `inputs` of the\n",
      "pipeline (`preprocess`).\n",
      "\n",
      "Then define the `outputs`. Same policy as the `inputs`. The simpler, the better. Those will be the outputs of\n",
      "`postprocess` method.\n",
      "<Tip>\n",
      "\n",
      "✏️ **Try it out!** Search for translation models in other languages and try to translate the previous sentence into a few different languages.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "The pipelines shown so far are mostly for demonstrative purposes. They were programmed for specific tasks and cannot perform variations of them. In the next chapter, you'll learn what's inside a `pipeline()` function and how to customize its behavior.\n",
      "# Create the transformers pipeline\n",
      ">>> onnx_clx = pipeline(\"text-classification\", model=model, accelerator=\"ort\")\n",
      ">>> text = \"I like the new ORT pipeline\"\n",
      ">>> pred = onnx_clx(text)\n",
      ">>> print(pred)  # doctest: +IGNORE_RESULT\n",
      ">>> # [{'label': 'POSITIVE', 'score': 0.9973127245903015}]\n",
      "\n",
      "# Save and push the model to the hub\n",
      ">>> tokenizer.save_pretrained(\"new_path_for_directory\")  # doctest: +IGNORE_RESULT\n",
      ">>> model.save_pretrained(\"new_path_for_directory\")\n",
      ">>> model.push_to_hub(\"new_path_for_directory\", repository_id=\"my-onnx-repo\", use_auth_token=True)  # doctest: +SKIP\n",
      "```\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# Overview\n",
      "\n",
      "A pipeline is an end-to-end class that provides a quick and easy way to use a diffusion system for inference by bundling independently trained models and schedulers together. Certain combinations of models and schedulers define specific pipeline types, like [`StableDiffusionXLPipeline`] or [`StableDiffusionControlNetPipeline`], with specific capabilities. All pipeline types inherit from the base [`DiffusionPipeline`] class; pass it any checkpoint, and it'll automatically detect the pipeline type and load the necessary components.\n",
      "\n",
      "This section demonstrates how to use specific pipelines such as Stable Diffusion XL, ControlNet, and DiffEdit. You'll also learn how to use a distilled version of the Stable Diffusion model to speed up inference, how to create reproducible pipelines, and how to use and contribute community pipelines.\n",
      "- Pipelines should be used **only** for inference.\n",
      "- Pipelines should be very readable, self-explanatory, and easy to tweak.\n",
      "- Pipelines should be designed to build on top of each other and be easy to integrate into higher-level APIs.\n",
      "- Pipelines are **not** intended to be feature-complete user interfaces. For future complete user interfaces one should rather have a look at [InvokeAI](https://github.com/invoke-ai/InvokeAI), [Diffuzers](https://github.com/abhishekkrthakur/diffuzers), and [lama-cleaner](https://github.com/Sanster/lama-cleaner).\n",
      "- Every pipeline should have one and only one way to run it via a `__call__` method. The naming of the `__call__` arguments should be shared across all pipelines.\n",
      "- Pipelines should be named after the task they are intended to solve.\n",
      "- In almost all cases, novel diffusion pipelines shall be implemented in a new pipeline folder/file.\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer:\")\n",
    "print(f\"{answer}\")\n",
    "print(\"\\n\\nSource documents:\")\n",
    "for doc in relevant_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Benchmarking the RAG system\n",
    "\n",
    "Since there are so many options to tune with a big impact on parameters, benchmarking the RAG system is crucial.\n",
    "\n",
    "But to do so we will need:\n",
    "- an evaluation dataset\n",
    "- an evaluator to compute the accuracy of our system.\n",
    "\n",
    "➡️ It turns out, we can use LLMs to help us all along the way!\n",
    "- the evaluation dataset will be synthetically generated by an LLM 🤖, and questions will be filtered out by other LLMs 🤖\n",
    "- the evaluation will then be run on this synthetic dataset by a LLM-as-a-judge agent 🤖."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Build a synthetic dataset for evaluation\n",
    "We first build a synthetic dataset of questions and associated contexts.\n",
    "\n",
    "The idea is to randomly get elements from our knowledge base, and ask a LLM to generate questions based on these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PREGENERATED_EVALUATION_DATASET = True\n",
    "\n",
    "if LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    eval_dataset = datasets.load_dataset(\"A-Roucher/huggingface_doc_qa_eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "    langchain_docs = [\n",
    "        LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "        for doc in tqdm(ds)\n",
    "    ]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in langchain_docs:\n",
    "        docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup agents for question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    QA_generation_prompt = \"\"\"\n",
    "    Your task is to write a factoid question and an answer given a context.\n",
    "    Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "    Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "    This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Output:::\n",
    "    Factoid question: (your factoid question)\n",
    "    Answer: (your answer to the factoid question)\n",
    "\n",
    "    Now here is the context.\n",
    "\n",
    "    Context: {context}\\n\n",
    "    Output:::\"\"\"\n",
    "\n",
    "    chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "    QA_generation_prompt = ChatPromptTemplate.from_template(QA_generation_prompt)\n",
    "    QA_generation_agent = QA_generation_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    N_GENERATIONS = 30\n",
    "\n",
    "    print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "    outputs = []\n",
    "    for context in tqdm(random.sample(langchain_docs, N_GENERATIONS)):\n",
    "        # Generate QA couple\n",
    "        output_QA_couple = QA_generation_agent.invoke(\n",
    "            {\"context\": context.page_content}\n",
    "        ).content\n",
    "        try:\n",
    "            question = output_QA_couple.split(\"Factoid question: \")[1].split(\n",
    "                \"Answer: \"\n",
    "            )[0]\n",
    "            answer = output_QA_couple.split(\"Answer: \")[1]\n",
    "            outputs.append(\n",
    "                {\n",
    "                    \"context\": context.page_content,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"source_doc\": context.metadata[\"source\"],\n",
    "                }\n",
    "            )\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup critique agents\n",
    "We now build and run the critique agents that will filter out questions generated by the previous LLM agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    question_relatedness_critique_prompt = \"\"\"\n",
    "    You will be given a context and a question.\n",
    "    Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "    Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating)\n",
    "    Total rating: (your rating)\n",
    "\n",
    "    Now here are the question and context.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Context: {context}\\n\n",
    "    Answer::: \"\"\"\n",
    "\n",
    "    question_relevance_critique_prompt = \"\"\"\n",
    "    You will be given a question.\n",
    "    Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "    Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "    Rating must be minimum, i.e. 1, if the questions refers to a particular setting, like 'in the context' or 'in the document'.\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating)\n",
    "    Total rating: (your rating)\n",
    "\n",
    "    Now here is the question.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Answer::: \"\"\"\n",
    "\n",
    "    question_standalone_critique_prompt = \"\"\"\n",
    "    You will be given a question.\n",
    "    Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "    Give your answer on a scale of 1 to 5, where 1 means that the question only makes sense in a specific context, and 5 means that the question makes sense by itself.\n",
    "    For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "    The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating)\n",
    "    Total rating: (your rating)\n",
    "\n",
    "    Now here is the question.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Answer::: \"\"\"\n",
    "\n",
    "    question_relatedness_critique_prompt = ChatPromptTemplate.from_template(\n",
    "        question_relatedness_critique_prompt\n",
    "    )\n",
    "    question_relatedness_critique_agent = (\n",
    "        question_relatedness_critique_prompt | chat_model\n",
    "    )\n",
    "\n",
    "    question_relevance_critique_prompt = ChatPromptTemplate.from_template(\n",
    "        question_relevance_critique_prompt\n",
    "    )\n",
    "    question_relevance_critique_agent = question_relevance_critique_prompt | chat_model\n",
    "\n",
    "    question_standalone_critique_prompt = ChatPromptTemplate.from_template(\n",
    "        question_standalone_critique_prompt\n",
    "    )\n",
    "    question_standalone_critique_agent = (\n",
    "        question_standalone_critique_prompt | chat_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    print(\"Generating critique for each QA couple...\")\n",
    "    for output in tqdm(outputs):\n",
    "        # Critique the generated QA couple\n",
    "        question_relatedness_evaluation = question_relatedness_critique_agent.invoke(\n",
    "            {\"context\": output[\"context\"], \"question\": output[\"question\"]}\n",
    "        ).content\n",
    "        question_relevance_evaluation = question_relevance_critique_agent.invoke(\n",
    "            {\"question\": output[\"question\"]}\n",
    "        ).content\n",
    "        question_standalone_evaluation = question_standalone_critique_agent.invoke(\n",
    "            {\"question\": output[\"question\"]}\n",
    "        ).content\n",
    "\n",
    "        try:\n",
    "            relatedness_score = int(\n",
    "                question_relatedness_evaluation.split(\"Total rating: \")[1][0]\n",
    "            )\n",
    "            relatedness_eval = question_relatedness_evaluation.split(\"Total rating: \")[\n",
    "                0\n",
    "            ].split(\"Evaluation: \")[1]\n",
    "            relevance_score = int(\n",
    "                question_relevance_evaluation.split(\"Total rating: \")[1][0]\n",
    "            )\n",
    "            relevance_eval = question_relevance_evaluation.split(\"Total rating: \")[\n",
    "                0\n",
    "            ].split(\"Evaluation: \")[1]\n",
    "            standalone_score = int(\n",
    "                question_standalone_evaluation.split(\"Total rating: \")[1][0]\n",
    "            )\n",
    "            standalone_eval = question_standalone_evaluation.split(\"Total rating: \")[\n",
    "                0\n",
    "            ].split(\"Evaluation: \")[1]\n",
    "            output.update(\n",
    "                {\n",
    "                    \"relatedness_score\": relatedness_score,\n",
    "                    \"relatedness_eval\": relatedness_eval,\n",
    "                    \"relevance_score\": relevance_score,\n",
    "                    \"relevance_eval\": relevance_eval,\n",
    "                    \"standalone_score\": standalone_score,\n",
    "                    \"standalone_eval\": standalone_eval,\n",
    "                }\n",
    "            )\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter down questions on the chosen criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    import pandas as pd\n",
    "\n",
    "    pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "    generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "    display(generated_questions.head())\n",
    "    generated_questions = generated_questions.loc[\n",
    "        (generated_questions[\"relatedness_score\"] >= 4)\n",
    "        & (generated_questions[\"relevance_score\"] >= 3)\n",
    "        & (generated_questions[\"standalone_score\"] >= 4)\n",
    "    ]\n",
    "    generated_questions.to_excel(\"qa_eval.xlsx\")\n",
    "    display(generated_questions.head())\n",
    "\n",
    "    eval_dataset = datasets.Dataset.from_pandas(\n",
    "        generated_questions, split=\"train\", preserve_index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the synthetic evaluation dataset is complete! We can evaluate different RAG systems on this evaluation dataset.\n",
    "\n",
    "## 2.2 Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: Dataset,\n",
    "    llm: BaseChatModel,\n",
    "    knowledge_index: FAISS,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "):\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "    for example in tqdm(eval_dataset[\"train\"]):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(\n",
    "            question, llm, knowledge_index, reranker=reranker\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\":::::::\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"true_answer\": example[\"answer\"],\n",
    "                \"source_doc\": example[\"source_doc\"],\n",
    "                \"generated_answer\": answer,\n",
    "                \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "evaluator_name = \"GPT4\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(answer_path, \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "    except:\n",
    "        answers = []\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [\n",
    "            item.strip() for item in eval_result.content.split(\"[RESULT]\")\n",
    "        ]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_SETTINGS = {}\n",
    "for chunk_size in [500, 700, 1000, 1500]:\n",
    "    for embeddings in [\"thenlper/gte-small\", \"BAAI/bge-base-en-v1.5\"]:\n",
    "        for rerank in [True, False]:\n",
    "            RAG_SETTINGS[\n",
    "                f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader:mistral-7b\"\n",
    "            ] = {\n",
    "                \"retriever\": {\n",
    "                    \"chunk_size\": chunk_size,\n",
    "                    \"embeddings\": embeddings,\n",
    "                },\n",
    "                \"reader\": {\n",
    "                    \"rerank_results\": rerank,\n",
    "                },\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:1500_embeddings:thenlper~gte-small_rerank:False_reader:mistral-7b:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c91c1e3af5043b983561a1b767f9e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a44fef5e284b979a3d3771e61a0b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:1500_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader:mistral-7b:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9722c1e8ad244ff9e9b17280547e838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6a820043ab447d81e7d0af592e3b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for settings_name, settings in RAG_SETTINGS.items():\n",
    "    print(f\"Running evaluation for {settings_name}:\")\n",
    "    output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "    print(\"Loading knowledge base embeddings...\")\n",
    "    knowledge_index = load_embeddings(\n",
    "        RAW_KNOWLEDGE_BASE,\n",
    "        chunk_size=settings[\"retriever\"][\"chunk_size\"],\n",
    "        embedding_model_name=settings[\"retriever\"][\"embeddings\"],\n",
    "    )\n",
    "\n",
    "    print(\"Running RAG...\")\n",
    "    if settings[\"reader\"][\"rerank_results\"]:\n",
    "        reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "    else:\n",
    "        reranker = None\n",
    "    run_rag_tests(\n",
    "        eval_dataset=eval_dataset,\n",
    "        llm=get_llm_answer,\n",
    "        knowledge_index=knowledge_index,\n",
    "        output_file=output_file_name,\n",
    "        reranker=reranker,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    print(\"Running evaluation...\")\n",
    "    evaluate_answers(\n",
    "        output_file_name,\n",
    "        eval_chat_model,\n",
    "        evaluator_name,\n",
    "        evaluation_prompt_template,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/rag_chunk:1500_embeddings:thenlper~gte-small_rerank:False_reader:mistral-7b.json\n",
      "output/rag_chunk:700_embeddings:BAAI~bge-base-en-v1.5_rerank:True_reader:mistral-7b.json\n",
      "output/rag_chunk:1000_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader:mistral-7b.json\n",
      "output/rag_chunk:500_embeddings:thenlper~gte-small_rerank:False_reader:mistral-7b.json\n",
      "output/rag_chunk:700_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader:mistral-7b.json\n",
      "output/rag_chunk:500_embeddings:BAAI~bge-base-en-v1.5_rerank:True_reader:mistral-7b.json\n",
      "output/rag_chunk:300_embeddings:BAAI~bge-base-en-v1.5_rerank:True_reader:mistral-7b.json\n",
      "output/rag_chunk:500_embeddings:thenlper~gte-small_rerank:True_reader:mistral-7b.json\n",
      "output/rag_chunk:1000_embeddings:BAAI~bge-base-en-v1.5_rerank:True_reader:mistral-7b.json\n",
      "output/rag_chunk:700_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader:mixtral-8x7b.json\n",
      "output/rag_chunk:1500_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader:mistral-7b.json\n",
      "output/rag_chunk:500_embeddings:BAAI~bge-base-en-v1.5_rerank:True_reader:mixtral-8x7b.json\n",
      "output/rag_chunk:700_embeddings:BAAI~bge-base-en-v1.5_rerank:True_reader:mixtral-8x7b.json\n",
      "output/rag_chunk:500_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader:mixtral-8x7b.json\n",
      "output/rag_chunk:1000_embeddings:BAAI~bge-base-en-v1.5_rerank:True_reader:mixtral-8x7b.json\n",
      "output/rag_chunk:300_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader:mistral-7b.json\n",
      "output/rag_chunk:1500_embeddings:BAAI~bge-base-en-v1.5_rerank:True_reader:mistral-7b.json\n",
      "output/rag_chunk:500_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader:mistral-7b.json\n",
      "output/rag_chunk:1000_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader:mixtral-8x7b.json\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "results = []\n",
    "for file in glob.glob(\"output/*.json\"):\n",
    "    print(file)\n",
    "    with open(file, \"r\") as f:\n",
    "        sub_df = pd.read_json(f)\n",
    "        sub_df[\"settings\"] = file.split(\"rag_\")[1].split(\".json\")[0]\n",
    "        results.append(sub_df)\n",
    "\n",
    "results_df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_result(x):\n",
    "    try:\n",
    "        return (int(x) - 1) / 4\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "results_df[\"normalized_score\"] = results_df[\"eval_score_GPT4\"].apply(interpret_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = results_df.groupby(\"settings\")[\"normalized_score\"].mean().sort_values() * 100\n",
    "scores = scores.rename(\n",
    "    index={\n",
    "        \"chunk:1500_embeddings:thenlper~gte-small_rerank:False_reader:mistral-7b\": \"Baseline RAG\",\n",
    "        \"chunk:500_embeddings:thenlper~gte-small_rerank:False_reader:mistral-7b\": \"+ Tune chunk size\",\n",
    "        \"chunk:500_embeddings:thenlper~gte-small_rerank:True_reader:mistral-7b\": \"+ Rerank results\",\n",
    "        \"chunk:500_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader:mistral-7b\": \"+ Better embeddings\",\n",
    "        \"chunk:700_embeddings:BAAI~bge-base-en-v1.5_rerank:True_reader:mixtral-8x7b\": \"+ Better reader LLM\",\n",
    "    }\n",
    ")\n",
    "scores = scores.loc[\n",
    "    [\n",
    "        \"Baseline RAG\",\n",
    "        \"+ Tune chunk size\",\n",
    "        \"+ Rerank results\",\n",
    "        \"+ Better embeddings\",\n",
    "        \"+ Better reader LLM\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Configuration=%{x}<br>Accuracy=%{y}<br>text=%{text}<br>color=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           76.11940298507463,
           81.71641791044776,
           82.83582089552239,
           86.19402985074626,
           89.17910447761194
          ],
          "coloraxis": "coloraxis",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          76.11940298507463,
          81.71641791044776,
          82.83582089552239,
          86.19402985074626,
          89.17910447761194
         ],
         "textposition": "outside",
         "texttemplate": "%{y:.0f}",
         "type": "bar",
         "x": [
          "Baseline RAG",
          "+ Tune chunk size",
          "+ Rerank results",
          "+ Better embeddings",
          "+ Better reader LLM"
         ],
         "xaxis": "x",
         "y": [
          76.11940298507463,
          81.71641791044776,
          82.83582089552239,
          86.19402985074626,
          89.17910447761194
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "color"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(0,0,255)"
          ],
          [
           1,
           "rgb(255,0,0)"
          ]
         ],
         "showscale": false
        },
        "height": 600,
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Accuracy of different RAG configurations"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Configuration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          100
         ],
         "ticksuffix": "%",
         "title": {
          "text": "Accuracy"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"c7ddad7b-1e2c-4f66-9256-f8ed4f49f572\" class=\"plotly-graph-div\" style=\"height:600px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c7ddad7b-1e2c-4f66-9256-f8ed4f49f572\")) {                    Plotly.newPlot(                        \"c7ddad7b-1e2c-4f66-9256-f8ed4f49f572\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Configuration=%{x}\\u003cbr\\u003eAccuracy=%{y}\\u003cbr\\u003etext=%{text}\\u003cbr\\u003ecolor=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[76.11940298507463,81.71641791044776,82.83582089552239,86.19402985074626,89.17910447761194],\"coloraxis\":\"coloraxis\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[76.11940298507463,81.71641791044776,82.83582089552239,86.19402985074626,89.17910447761194],\"textposition\":\"outside\",\"x\":[\"Baseline RAG\",\"+ Tune chunk size\",\"+ Rerank results\",\"+ Better embeddings\",\"+ Better reader LLM\"],\"xaxis\":\"x\",\"y\":[76.11940298507463,81.71641791044776,82.83582089552239,86.19402985074626,89.17910447761194],\"yaxis\":\"y\",\"type\":\"bar\",\"texttemplate\":\"%{y:.0f}\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Configuration\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Accuracy\"},\"range\":[0,100],\"ticksuffix\":\"%\"},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"color\"}},\"colorscale\":[[0.0,\"rgb(0,0,255)\"],[1.0,\"rgb(255,0,0)\"]],\"showscale\":false},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"group\",\"width\":1000,\"height\":600,\"title\":{\"text\":\"Accuracy of different RAG configurations\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('c7ddad7b-1e2c-4f66-9256-f8ed4f49f572');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(\n",
    "    scores,\n",
    "    color=scores,\n",
    "    text=scores,\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 100],\n",
    "    title=\"Accuracy of different RAG configurations\",\n",
    ")\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.0f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
