{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Collect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "LOAD_LOCALLY = False\n",
    "\n",
    "if LOAD_LOCALLY:\n",
    "    from pathlib import Path\n",
    "    from datasets import Dataset\n",
    "\n",
    "    docs = []\n",
    "    sources = []\n",
    "    for p in Path(\"./data/datasets/huggingface_docs/\").iterdir():\n",
    "        if not p.is_dir():\n",
    "            with open(p) as f:\n",
    "                # the first line is the source of the text\n",
    "                source = f.readline().strip().replace('source: ', '').replace('https://github.com/', '')\n",
    "                content = f.read()[2:] # Remove the initial '\\n'\n",
    "                if len(content) > 0:\n",
    "                    docs.append(content)\n",
    "                    sources.append(source)\n",
    "        # break\n",
    "\n",
    "    ds = Dataset.from_dict({\"text\": docs, \"source\": sources})\n",
    "    ds.to_csv('./data/huggingface_doc.csv')\n",
    "    print(f'number of documents: {len(ds)}')\n",
    "\n",
    "else:\n",
    "    ds = datasets.load_dataset(\"A-Roucher/huggingface_doc\", split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup evaluation pipeline\n",
    "In this part, we build a synthetic dataset of questions and associated contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'question', 'answer', 'source_doc', 'standalone_score', 'standalone_eval', 'relatedness_score', 'relatedness_eval', 'relevance_score', 'relevance_eval'],\n",
       "        num_rows: 81\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ef264d83e247cba0092d75da67a0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c349e547bc44cbbb97f4ecad4c33438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/297k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac74b3c01ea4a15856ca3b77289d376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "REBUILD_EVALUATION_DATASET = False\n",
    "\n",
    "if not REBUILD_EVALUATION_DATASET:\n",
    "    eval_dataset = datasets.load_dataset('A-Roucher/huggingface_doc_qa_eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare source documents\n",
    "\n",
    "We use Langchain's `RecursiveCharacterTextSplitter`, which makes efficient use of code language detection to make better splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REBUILD_EVALUATION_DATASET:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "    langchain_docs = [LangchainDocument(page_content=doc['text'], metadata={'source': doc['source']}) for doc in tqdm(ds)]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000, chunk_overlap=200, add_start_index=True, separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in langchain_docs:\n",
    "        docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup chains for question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REBUILD_EVALUATION_DATASET:\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    QA_generation_prompt = \"\"\"\n",
    "    Your task is to write a factoid question and an answer given a context.\n",
    "    Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "    Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "    This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Output:::\n",
    "    Factoid question: (your factoid question)\n",
    "    Answer: (your answer to the factoid question)\n",
    "\n",
    "    Now here is the context.\n",
    "\n",
    "    Context: {context}\\n\n",
    "    Output:::\"\"\"\n",
    "\n",
    "    question_relatedness_critique_prompt = \"\"\"\n",
    "    You will be given a context and a question.\n",
    "    Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "    Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating)\n",
    "    Total rating: (your rating)\n",
    "\n",
    "    Now here are the question and context.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Context: {context}\\n\n",
    "    Answer::: \"\"\"\n",
    "\n",
    "    question_relevance_critique_prompt = \"\"\"\n",
    "    You will be given a question.\n",
    "    Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "    Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "    Rating must be minimum, i.e. 1, if the questions refers to a particular setting, like 'in the context' or 'in the document'.\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating)\n",
    "    Total rating: (your rating)\n",
    "\n",
    "    Now here is the question.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Answer::: \"\"\"\n",
    "\n",
    "    question_standalone_critique_prompt = \"\"\"\n",
    "    You will be given a question.\n",
    "    Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "    Give your answer on a scale of 1 to 5, where 1 means that the question only makes sense in a specific context, and 5 means that the question makes sense by itself.\n",
    "    For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "    The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating)\n",
    "    Total rating: (your rating)\n",
    "\n",
    "    Now here is the question.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Answer::: \"\"\"\n",
    "\n",
    "    chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0.2)\n",
    "    QA_generation_prompt = ChatPromptTemplate.from_template(QA_generation_prompt)\n",
    "    QA_generation_chain = QA_generation_prompt | chat_model\n",
    "\n",
    "    question_relatedness_critique_prompt = ChatPromptTemplate.from_template(question_relatedness_critique_prompt)\n",
    "    question_relatedness_critique_chain = question_relatedness_critique_prompt | chat_model\n",
    "\n",
    "    question_relevance_critique_prompt = ChatPromptTemplate.from_template(question_relevance_critique_prompt)\n",
    "    question_relevance_critique_chain = question_relevance_critique_prompt | chat_model\n",
    "\n",
    "    question_standalone_critique_prompt = ChatPromptTemplate.from_template(question_standalone_critique_prompt)\n",
    "    question_standalone_critique_chain = question_standalone_critique_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "if REBUILD_EVALUATION_DATASET:\n",
    "    N_GENERATIONS = 30\n",
    "\n",
    "    print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "    outputs = []\n",
    "    for context in tqdm(random.sample(langchain_docs, N_GENERATIONS)):\n",
    "        # Generate QA couple\n",
    "        output_QA_couple = QA_generation_chain.invoke({\"context\": context.page_content}).content\n",
    "        try:\n",
    "            question = output_QA_couple.split('Factoid question: ')[1].split('Answer: ')[0]\n",
    "            answer = output_QA_couple.split('Answer: ')[1]\n",
    "            outputs.append(\n",
    "                {\n",
    "                    \"context\": context.page_content,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"source_doc\": context.metadata['source'],\n",
    "                }\n",
    "            )\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REBUILD_EVALUATION_DATASET:\n",
    "    print(\"Generating critique for each QA couple...\")\n",
    "    for output in tqdm(outputs):\n",
    "        # Critique the generated QA couple\n",
    "        question_relatedness_evaluation = question_relatedness_critique_chain.invoke({\"context\": output['context'], \"question\": output['question']}).content\n",
    "        question_relevance_evaluation = question_relevance_critique_chain.invoke({\"question\": output['question']}).content\n",
    "        question_standalone_evaluation = question_standalone_critique_chain.invoke({\"question\": output['question']}).content\n",
    "\n",
    "        try:\n",
    "            relatedness_score = int(question_relatedness_evaluation.split('Total rating: ')[1][0])\n",
    "            relatedness_eval = question_relatedness_evaluation.split('Total rating: ')[0].split('Evaluation: ')[1]\n",
    "            relevance_score = int(question_relevance_evaluation.split('Total rating: ')[1][0])\n",
    "            relevance_eval = question_relevance_evaluation.split('Total rating: ')[0].split('Evaluation: ')[1]\n",
    "            standalone_score = int(question_standalone_evaluation.split('Total rating: ')[1][0])\n",
    "            standalone_eval = question_standalone_evaluation.split('Total rating: ')[0].split('Evaluation: ')[1]\n",
    "            output.update({\n",
    "                \"relatedness_score\": relatedness_score,\n",
    "                \"relatedness_eval\": relatedness_eval,\n",
    "                \"relevance_score\": relevance_score,\n",
    "                \"relevance_eval\": relevance_eval,\n",
    "                \"standalone_score\": standalone_score,\n",
    "                \"standalone_eval\": standalone_eval,\n",
    "            })\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter down questions on the chosen criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REBUILD_EVALUATION_DATASET:\n",
    "    import pandas as pd\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "    generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "    display(generated_questions.head())\n",
    "    generated_questions = generated_questions.loc[(generated_questions['relatedness_score'] >= 4) & (generated_questions['relevance_score'] >= 3) & (generated_questions['standalone_score'] >= 4)]\n",
    "    !pip install openpyxl\n",
    "    generated_questions.to_excel(\"qa_eval.xlsx\")\n",
    "    display(generated_questions.head())\n",
    "\n",
    "    eval_dataset = datasets.Dataset.from_pandas(generated_questions, split='train', preserve_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Retriever - embeddings\n",
    "Here we use Langchain vector databases since it offers a convenient FAISS index and allows us to keep document metadata throughout the processing.\n",
    "\n",
    "\n",
    "Options:\n",
    "- change embedding model\n",
    "- normal embeddings vs instruct embeddings\n",
    "- Hyde\n",
    "- reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "In this part, we split the documennts from our knowledge base into smaller chunks which will be the snippets that will support our answer. The goal is to have semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large in order to be centered around a key information.\n",
    "\n",
    "We do this chunking with the `haystack` library, which offers a good `PreProcessor` class.\n",
    "\n",
    "[This space](https://huggingface.co/spaces/A-Roucher/chunk_visualizer) lets you visualize how different chunking options affect the chunks you get, to help you tune this step.\n",
    "\n",
    "Options:\n",
    "- split respecting sentence boundaries\n",
    "- semantic splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd65fe15ebd4d49a8f2dbff136c2e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [LangchainDocument(page_content=doc['text'], metadata={'source': doc['source']}) for doc in tqdm(ds)]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100, add_start_index=True, separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series([len(doc.page_content.split(' ')) for doc in docs_processed]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "\n",
    "USE_INSTRUCT_EMBEDDINGS = False\n",
    "\n",
    "if not USE_INSTRUCT_EMBEDDINGS:\n",
    "    model_name = 'BAAI/bge-base-en-v1.5'\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=model_name, multi_process=True)\n",
    "\n",
    "else:\n",
    "    model_name = \"hkunlp/instructor-large\"\n",
    "    embed_instruction = \"Represent the Hugging Face library documentation\"\n",
    "    query_instruction = \"Query the most relevant piece of information from the Hugging Face documentation\"\n",
    "\n",
    "    embedding_model = HuggingFaceInstructEmbeddings(\n",
    "        model_name=model_name,\n",
    "        embed_instruction=embed_instruction,\n",
    "        query_instruction=query_instruction\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â¬‡ï¸ðŸš¨ The cell below takes 15min to run on A10G!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "index = FAISS.from_documents(docs_processed, embedding_model)\n",
    "\n",
    "index_name = 'index_1000'\n",
    "index.save_local(f'./data/indexes/{index_name}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "-->\n",
      "\n",
      "# Using pipelines for a webserver\n",
      "\n",
      "<Tip>\n",
      "Creating an inference engine is a complex topic, and the \"best\" solution \n",
      "will most likely depend on your problem space. Are you on CPU or GPU? Do\n",
      "you want the lowest latency, the highest throughput, support for\n",
      "many models, or just highly optimize 1 specific model?\n",
      "There are many ways to tackle this topic, so what we are going to present is a good default\n",
      "to get started which may not necessarily be the most optimal solution for you.\n",
      "</Tip>\n",
      "\n",
      "\n",
      "The key thing to understand is that we can use an iterator, just like you would [on a\n",
      "dataset](pipeline_tutorial#using-pipelines-on-a-dataset), since a webserver is basically a system that waits for requests and\n",
      "treats them as they come in.\n",
      "\n",
      "Usually webservers are multiplexed (multithreaded, async, etc..) to handle various\n",
      "requests concurrently. Pipelines on the other hand (and mostly the underlying models)\n",
      "are not really great for parallelism; they take up a lot of RAM, so it's best to give them all the available resources when they are running or it's a compute-intensive job.\n",
      "\n",
      "We are going to solve that by having the webserver handle the light load of receiving\n",
      "and sending requests, and having a single thread handling the actual work.\n",
      "This example is going to use `starlette`. The actual framework is not really\n",
      "important, but you might have to tune or change the code if you are using another\n",
      "one to achieve the same effect.\n",
      "\n",
      "Create `server.py`:\n",
      "\n",
      "```py\n",
      "from starlette.applications import Starlette\n",
      "from starlette.responses import JSONResponse\n",
      "from starlette.routing import Route\n",
      "from transformers import pipeline\n",
      "import asyncio\n",
      "\n",
      "\n",
      "async def homepage(request):\n",
      "    payload = await request.body()\n",
      "    string = payload.decode(\"utf-8\")\n",
      "    response_q = asyncio.Queue()\n",
      "    await request.app.model_queue.put((string, response_q))\n",
      "    output = await response_q.get()\n",
      "    return JSONResponse(output)\n",
      "\n",
      "\n",
      "async def server_loop(q):\n",
      "    pipe = pipeline(model=\"bert-base-uncased\")\n",
      "    while True:\n",
      "        (string, response_q) = await q.get()\n",
      "        out = pipe(string)\n",
      "        await response_q.put(out)\n",
      "\n",
      "\n",
      "app = Starlette(\n",
      "    routes=[\n",
      "        Route(\"/\", homepage, methods=[\"POST\"]),\n",
      "    ],\n",
      ")\n",
      "\n",
      "\n",
      "@app.on_event(\"startup\")\n",
      "async def startup_event():\n",
      "    q = asyncio.Queue()\n",
      "    app.model_queue = q\n",
      "    asyncio.create_task(server_loop(q))\n",
      "```\n",
      "\n",
      "Now you can start it with:\n",
      "```bash\n",
      "uvicorn server:app\n",
      "```\n",
      "\n",
      "And you can query it:\n",
      "```bash\n",
      "curl -X POST -d \"test [MASK]\" http://localhost:8000/\n",
      "#[{\"score\":0.7742936015129089,\"token\":1012,\"token_str\":\".\",\"sequence\":\"test.\"},...]\n",
      "```\n",
      "\n",
      "And there you go, now you have a good idea of how to create a webserver!\n",
      "\n",
      "What is really important is that we load the model only **once**, so there are no copies\n",
      "of the model on the webserver. This way, no unnecessary RAM is being used.\n",
      "Then the queuing mechanism allows you to do fancy stuff like maybe accumulating a few\n",
      "items before inferring to use dynamic batching:\n",
      "\n",
      "<Tip warning={true}>\n",
      "\n",
      "The code sample below is intentionally written like pseudo-code for readability.\n",
      "Do not run this without checking if it makes sense for your system resources!\n",
      "\n",
      "</Tip>\n",
      "\n",
      "```py\n",
      "(string, rq) = await q.get()\n",
      "strings = []\n",
      "queues = []\n",
      "while True:\n",
      "    try:\n",
      "        (string, rq) = await asyncio.wait_for(q.get(), timeout=0.001)  # 1ms\n",
      "    except asyncio.exceptions.TimeoutError:\n",
      "        break\n",
      "    strings.append(string)\n",
      "    queues.append(rq)\n",
      "strings\n",
      "outs = pipe(strings, batch_size=len(strings))\n",
      "for rq, out in zip(queues, outs):\n",
      "    await rq.put(out)\n",
      "```\n",
      "\n",
      "Again, the proposed code is optimized for readability, not for being the best code.\n",
      "First of all, there's no batch size limit which is usually not a \n",
      "great idea. Next, the timeout is reset on every queue fetch, meaning you could\n",
      "wait much more than 1ms before running the inference (delaying the first request \n",
      "by that much). \n",
      "\n",
      "It would be better to have a single 1ms deadline.\n",
      "\n",
      "This will always wait for 1ms even if the queue is empty, which might not be the\n",
      "best since you probably want to start doing inference if there's nothing in the queue.\n",
      "But maybe it does make sense if batching is really crucial for your use case.\n",
      "Again, there's really no one best solution.\n",
      "\n",
      "\n",
      "## Few things you might want to consider\n",
      "\n",
      "### Error checking\n",
      "\n",
      "There's a lot that can go wrong in production: out of memory, out of space,\n",
      "loading the model might fail, the query might be wrong, the query might be\n",
      "correct but still fail to run because of a model misconfiguration, and so on.\n",
      "\n",
      "Generally, it's good if the server outputs the errors to the user, so\n",
      "adding a lot of `try..except` statements to show those errors is a good\n",
      "idea. But keep in mind it may also be a security risk to reveal all those errors depending \n",
      "on your security context.\n",
      "\n",
      "### Circuit breaking\n",
      "\n",
      "Webservers usually look better when they do circuit breaking. It means they \n",
      "return proper errors when they're overloaded instead of just waiting for the query indefinitely. Return a 503 error instead of waiting for a super long time or a 504 after a long time.\n",
      "\n",
      "This is relatively easy to implement in the proposed code since there is a single queue.\n",
      "Looking at the queue size is a basic way to start returning errors before your \n",
      "webserver fails under load.\n",
      "\n",
      "### Blocking the main thread\n",
      "\n",
      "Currently PyTorch is not async aware, and computation will block the main\n",
      "thread while running. That means it would be better if PyTorch was forced to run\n",
      "on its own thread/process. This wasn't done here because the code is a lot more\n",
      "complex (mostly because threads and async and queues don't play nice together).\n",
      "But ultimately it does the same thing.\n",
      "\n",
      "This would be important if the inference of single items were long (> 1s) because \n",
      "in this case, it means every query during inference would have to wait for 1s before\n",
      "even receiving an error.\n",
      "\n",
      "### Dynamic batching\n",
      "\n",
      "In general, batching is not necessarily an improvement over passing 1 item at \n",
      "a time (see [batching details](./main_classes/pipelines#pipeline-batching) for more information). But it can be very effective\n",
      "when used in the correct setting. In the API, there is no dynamic\n",
      "batching by default (too much opportunity for a slowdown). But for BLOOM inference -\n",
      "which is a very large model - dynamic batching is **essential** to provide a decent experience for everyone.\n",
      "\n",
      "{'source': 'huggingface/transformers/blob/main/docs/source/en/pipeline_webserver.md'}\n"
     ]
    }
   ],
   "source": [
    "docs = index.similarity_search(query='how to create a pipeline object?', k=5)\n",
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_INDEX = FAISS.load_local(f'./data/indexes/{index_name}/', embedding_model)\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "RERANKER = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Reader - LLM\n",
    "Options:\n",
    "- zero-shot vs few-shot prompting (cf [resource](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant#6-using-qdrant-to-improve-rag-prompt))\n",
    "- tune the number of examples retrieved\n",
    "- make conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context, \n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "  </s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c716be70cb30499498b7f7e202740790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/ml2/lib/python3.10/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/RAG_cookbook/retrieval_augmented_generation.ipynb Cell 29\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/RAG_cookbook/retrieval_augmented_generation.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/RAG_cookbook/retrieval_augmented_generation.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m llm \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mHuggingFaceH4/zephyr-7b-beta\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/RAG_cookbook/retrieval_augmented_generation.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m llm(\u001b[39m'\u001b[39;49m\u001b[39mOk,\u001b[39;49m\u001b[39m'\u001b[39;49m, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m)\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(text_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    272\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/transformers/generation/utils.py:1718\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1701\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1702\u001b[0m         input_ids,\n\u001b[1;32m   1703\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1714\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1715\u001b[0m     )\n\u001b[1;32m   1716\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1717\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1718\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1719\u001b[0m         input_ids,\n\u001b[1;32m   1720\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1721\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1722\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1723\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1724\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1725\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1726\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1727\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1728\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1729\u001b[0m     )\n\u001b[1;32m   1731\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1732\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/transformers/generation/utils.py:2579\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2576\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2578\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2579\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2580\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2581\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2582\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2583\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2584\u001b[0m )\n\u001b[1;32m   2586\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2587\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1053\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1052\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1054\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1055\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1056\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1057\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1058\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1059\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1060\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1061\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1062\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1063\u001b[0m )\n\u001b[1;32m   1065\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1066\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:938\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    928\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    929\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    930\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m         use_cache,\n\u001b[1;32m    936\u001b[0m     )\n\u001b[1;32m    937\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 938\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    939\u001b[0m         hidden_states,\n\u001b[1;32m    940\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    941\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    942\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    943\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    944\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    945\u001b[0m     )\n\u001b[1;32m    947\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    949\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:663\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    662\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 663\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    664\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    665\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    666\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    667\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    668\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    669\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    670\u001b[0m )\n\u001b[1;32m    671\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    673\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:313\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    311\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mreshape(bsz, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\n\u001b[0;32m--> 313\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mo_proj(attn_output)\n\u001b[1;32m    315\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m output_attentions:\n\u001b[1;32m    316\u001b[0m     attn_weights \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/ml2/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\"text-generation\", model='HuggingFaceH4/zephyr-7b-beta')\n",
    "\n",
    "llm('Ok,', max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "API_URL = \"https://ytjpei7t003tedav.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "headers = {\n",
    "\t\"Authorization\": f\"Bearer {os.getenv('HF_TOKEN')}\",\n",
    "\t\"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def get_llm_answer(question):\n",
    "\tpayload = {\n",
    "\t\t\"inputs\": question,\n",
    "\t\t\"max_new_tokens\": 3000,\n",
    "\t}\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, llm, knowledge_index, reranker, num_retrieved_docs: int = 15, num_reranked_docs: int = 7, use_reranker=True):\n",
    "    # Gather documents with retriever\n",
    "        \n",
    "    relevant_docs = knowledge_index.similarity_search(\n",
    "        query=question,\n",
    "        k=num_retrieved_docs\n",
    "    )\n",
    "\n",
    "    if use_reranker:\n",
    "        # Select the most relevant documents with reranker\n",
    "        cross_encoding_predictions = reranker.predict(\n",
    "            [(question, doc.page_content) for doc in relevant_docs]\n",
    "        )\n",
    "        relevant_docs = [\n",
    "            doc for _, doc in sorted(\n",
    "                zip(cross_encoding_predictions, relevant_docs),\n",
    "                reverse=True, key = lambda x: x[0]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_reranked_docs]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = '\\nExtracted documents:\\n'\n",
    "    context += ''.join([f\"{str(i)}: \" + doc.page_content for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = prompt_template.format(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "    print('Finished retrieving. Redacting answer...')\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)[0]['generated_text']\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished retrieving. Redacting answer...\n"
     ]
    }
   ],
   "source": [
    "question = \"how to create a pipeline object?\"\n",
    "answer, relevant_docs = answer_question(question, get_llm_answer, KNOWLEDGE_INDEX, RERANKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Answer: {answer}')\n",
    "print('\\n\\nSource documents:')\n",
    "for doc in relevant_docs:\n",
    "    print(f'{doc.metadata[\"source\"]}')\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking the final system on your evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "try: # load previous generations if they exist\n",
    "    with open(f'output/rag_evaluation_results_noreranker.json', 'r') as f:\n",
    "        outputs = json.load(f)\n",
    "except:\n",
    "    outputs = []\n",
    "\n",
    "\n",
    "for example in tqdm(eval_dataset['train']):\n",
    "    question = example['question']\n",
    "    if question in [output['question'] for output in outputs]:\n",
    "        continue\n",
    "\n",
    "    answer, relevant_docs = answer_question(question, get_llm_answer, KNOWLEDGE_INDEX, RERANKER, use_reranker=False)\n",
    "    print(':::::::')\n",
    "    print(f'Question: {question}')\n",
    "    print(f'Answer: {answer}')\n",
    "    print(f'True answer: {example[\"answer\"]}')\n",
    "    outputs.append({\n",
    "        \"question\": question,\n",
    "        \"true_answer\": example['answer'],\n",
    "        \"source_doc\": example['source_doc'],\n",
    "        \"generated_answer\": answer,\n",
    "        \"retrieved_docs\": [doc.page_content for doc in relevant_docs],\n",
    "    })\n",
    "\n",
    "    with open(f'output/rag_evaluation_results_noreranker.json', 'w') as f:\n",
    "        json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541513d26ea143fa9225beb86714d5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model='gpt-4-1106-preview', temperature=0)\n",
    "evaluator_name = \"GPT4\"\n",
    "\n",
    "for experiment in tqdm(outputs):\n",
    "    if f'eval_score_{evaluator_name}' in experiment:\n",
    "        continue\n",
    "\n",
    "    eval_prompt = evaluation_prompt_template.format_messages(\n",
    "        instruction=experiment['question'],\n",
    "        response=experiment['generated_answer'],\n",
    "        reference_answer=experiment[\"true_answer\"],\n",
    "    )\n",
    "    eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "    feedback, score = [\n",
    "        item.strip() for item in eval_result.content.split(\"[RESULT]\")\n",
    "    ]\n",
    "    experiment[f'eval_score_{evaluator_name}'] = score\n",
    "    experiment[f'eval_feedback_{evaluator_name}'] = feedback\n",
    "\n",
    "    with open(f'output/rag_evaluation_results.json', 'w') as f:\n",
    "        json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(outputs)\n",
    "results_df = results_df.loc[~results_df['eval_feedback_GPT4'].isna()]\n",
    "results_df['eval_score_GPT4'] = results_df['eval_score_GPT4'].astype(int)\n",
    "performance_percentage = (results_df['eval_score_GPT4'] - 1).mean() / 4 # We convert the 1-5 int scale to a 0-1 float scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>source_doc</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>retrieved_docs</th>\n",
       "      <th>eval_score_GPT4</th>\n",
       "      <th>eval_feedback_GPT4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\\n</td>\n",
       "      <td>Beautiful Soup</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md</td>\n",
       "      <td>The library used by MarkupLMFeatureExtractor to extract data from HTML and XML files is Beautiful Soup, as mentioned in the context provided. This information can be found in the usage instructions for MarkupLMProcessor, specifically in the documentation for the `MarkupLMFeatureExtractor` class. However, it should be noted that while using Beautiful Soup is the easiest way to prepare data for the model, it is not a requirement. The context also explains that users can still provide their</td>\n",
       "      <td>[In short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\\nand it will create the inputs expected by the model. Internally, the processor first uses\\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\\nto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\\nOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.\\n\\n[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]., ## Usage: MarkupLMProcessor\\n\\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\\nif you only want to handle one of the two tasks.\\n\\n```python\\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\\n\\nfeature_extractor = MarkupLMFeatureExtractor()\\ntokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\\nprocessor = MarkupLMProcessor(feature_extractor, tokenizer)\\n```, In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).\\n\\n**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**\\n\\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML.\\n\\n```python\\n&gt;&gt;&gt; from transformers import MarkupLMProcessor\\n\\n&gt;&gt;&gt; processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\\n\\n&gt;&gt;&gt; html_string = \"\"\"\\n...  &lt;!DOCTYPE html&gt;\\n...  &lt;html&gt;\\n...  &lt;head&gt;\\n...  &lt;title&gt;Hello world&lt;/title&gt;\\n...  &lt;/head&gt;\\n...  &lt;body&gt;\\n...  &lt;h1&gt;Welcome&lt;/h1&gt;\\n...  &lt;p&gt;Here is my website.&lt;/p&gt;\\n...  &lt;/body&gt;\\n...  &lt;/html&gt;\"\"\", --&gt;\\n\\n# MarkupLM\\n\\n## Overview\\n\\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\\nperformance, similar to [LayoutLM](layoutlm).\\n\\nThe model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\\nstate-of-the-art results on 2 important benchmarks:\\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\\n- [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\\nfor information extraction from web pages (basically named-entity recogntion on web pages), ```python\\n&gt;&gt;&gt; from transformers import MarkupLMProcessor\\n\\n&gt;&gt;&gt; processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\\n\\n&gt;&gt;&gt; html_string = \"\"\"\\n...  &lt;!DOCTYPE html&gt;\\n...  &lt;html&gt;\\n...  &lt;head&gt;\\n...  &lt;title&gt;Hello world&lt;/title&gt;\\n...  &lt;/head&gt;\\n...  &lt;body&gt;\\n...  &lt;h1&gt;Welcome&lt;/h1&gt;\\n...  &lt;p&gt;My name is Niels.&lt;/p&gt;\\n...  &lt;/body&gt;\\n...  &lt;/html&gt;\"\"\"\\n\\n&gt;&gt;&gt; question = \"What's his name?\"\\n&gt;&gt;&gt; encoding = processor(html_string, questions=question, return_tensors=\"pt\")\\n&gt;&gt;&gt; print(encoding.keys())\\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\\n```\\n\\n**Use case 5: web page question answering (inference), parse_html=False**\\n\\nFor question answering tasks (such as WebSRC), you can provide a question to the processor. If you have extracted\\nall nodes and xpaths yourself, you can provide them directly to the processor. Make sure to set `parse_html` to `False`.\\n\\n```python\\n&gt;&gt;&gt; from transformers import MarkupLMProcessor, ```python\\n&gt;&gt;&gt; from transformers import MarkupLMProcessor\\n\\n&gt;&gt;&gt; processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\\n&gt;&gt;&gt; processor.parse_html = False\\n\\n&gt;&gt;&gt; nodes = [\"hello\", \"world\", \"how\", \"are\"]\\n&gt;&gt;&gt; xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\\n&gt;&gt;&gt; question = \"What's his name?\"\\n&gt;&gt;&gt; encoding = processor(nodes=nodes, xpaths=xpaths, questions=question, return_tensors=\"pt\")\\n&gt;&gt;&gt; print(encoding.keys())\\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\\n```\\n\\n## Resources\\n\\n- [Demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)\\n- [Text classification task guide](../tasks/sequence_classification)\\n- [Token classification task guide](../tasks/token_classification)\\n- [Question answering task guide](../tasks/question_answering)\\n\\n## MarkupLMConfig\\n\\n[[autodoc]] MarkupLMConfig\\n    - all\\n\\n## MarkupLMFeatureExtractor, &gt;&gt;&gt; # note that you can also add provide all tokenizer parameters here such as padding, truncation\\n&gt;&gt;&gt; encoding = processor(html_string, return_tensors=\"pt\")\\n&gt;&gt;&gt; print(encoding.keys())\\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\\n```\\n\\n**Use case 2: web page classification (training, inference) + token classification (inference), parse_html=False**\\n\\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to `False`.\\n\\n```python\\n&gt;&gt;&gt; from transformers import MarkupLMProcessor\\n\\n&gt;&gt;&gt; processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\\n&gt;&gt;&gt; processor.parse_html = False]</td>\n",
       "      <td>1</td>\n",
       "      <td>Feedback: The response provided is incorrect. The reference answer states that the correct library is \"Beautiful Soup,\" but the response inaccurately attributes the functionality to MarkupLMFeatureExtractor, which is not a library but a class within a library. The response does not directly answer the question about which library is used, instead, it discusses the usage of the MarkupLMFeatureExtractor class and mentions Beautiful Soup in an incorrect context. Based on the score rubric, the response is not factual as it confuses the class with the library and does not correctly identify the library used.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>What is the command to upload an ESPnet model to a Hugging Face repository?\\n</td>\n",
       "      <td>./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo</td>\n",
       "      <td>huggingface/hub-docs/blob/main/docs/hub/espnet.md</td>\n",
       "      <td>The command to upload an ESPnet model to a Hugging Face repository is:\\n\\n```bash\\nespnet_model.push_to_hub(\"my-awesome-model\")\\n```\\n\\nThis command is part of the `espnet` toolkit for speech processing, which can be found in the Hugging Face Hub. To learn more about sharing models with Hugging Face, refer to the official guide provided in the context. Additionally, you can use the</td>\n",
       "      <td>[The `run.sh` script allows to upload a given model to a Hugging Face repository.\\n\\n```bash\\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\\n```\\n\\n## Additional resources\\n\\n* ESPnet [docs](https://espnet.github.io/espnet/index.html).\\n* ESPnet model zoo [repository](https://github.com/espnet/espnet_model_zoo).\\n* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md)., Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\\n\\n### Finding models\\n\\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\\n\\n\\n### Widgets, Using ESPnet at Hugging Face\\n\\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\\n\\n## Exploring ESPnet in the Hub\\n\\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&amp;sort=downloads). \\n\\nAll models on the Hub come up with useful features:\\n1. An automatically generated model card with a description, a training configuration, licenses and more.\\n2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\\n3. An interactive widget you can use to play out with the model directly in the browser.\\n4. An Inference API that allows to make inference requests., ```py\\nimport soundfile\\nfrom espnet2.bin.tts_inference import Text2Speech\\n\\ntext2speech = Text2Speech.from_pretrained(\"model_name\")\\nspeech = text2speech(\"foobar\")[\"wav\"]\\nsoundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM_16\")\\n```\\n\\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! \\n\\n&lt;div class=\"flex justify-center\"&gt;\\n&lt;img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png\"/&gt;\\n&lt;img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png\"/&gt;\\n&lt;/div&gt;\\n\\n## Sharing your models\\n\\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model))., ```py\\n&gt;&gt;&gt; tokenizer.push_to_hub(\"my-awesome-model\")\\n```\\n\\nOr perhaps you'd like to add the TensorFlow version of your fine-tuned PyTorch model:\\n\\n```py\\n&gt;&gt;&gt; tf_model.push_to_hub(\"my-awesome-model\")\\n```\\n\\nNow when you navigate to your Hugging Face profile, you should see your newly created model repository. Clicking on the **Files** tab will display all the files you've uploaded to the repository.\\n\\nFor more details on how to create and upload files to a repository, refer to the Hub documentation [here](https://huggingface.co/docs/hub/how-to-upstream).\\n\\n## Upload with the web interface\\n\\nUsers who prefer a no-code approach are able to upload a model through the Hub's web interface. Visit [huggingface.co/new](https://huggingface.co/new) to create a new repository:\\n\\n![new_model_repo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png)\\n\\nFrom here, add some information about your model:, ## Sharing your models\\n\\nYou can easily upload your models using `mlagents-push-to-hf`:\\n\\n```\\nmlagents-push-to-hf --run-id=\"First Training\" --local-dir=\"results/First Training\" --repo-id=\"ThomasSimonini/MLAgents-Pyramids\" --commit-message=\"Pyramids\"\\n```\\n\\nYou need to define four parameters:\\n- `--run-id`: the name of the training run id.\\n- `--local-dir`: where the model was saved.\\n- `--repo-id`: the name of the Hugging Face repo you want to create or update. Itâ€™s `&lt;your huggingface username&gt;/&lt;the repo name&gt;`.\\n- `--commit-message`.\\n\\n\\n## Additional resources\\n\\n* ML-Agents [documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Hugging-Face-Integration.md)\\n* Official Unity ML-Agents Spaces [demos](https://huggingface.co/unity), ```bash\\nsudo apt install tesseract-ocr\\npip install -q pytesseract\\n```\\n\\nOnce you have installed all of the dependencies, restart your runtime.\\n\\nWe encourage you to share your model with the community. Log in to your Hugging Face account to upload it to the ðŸ¤— Hub.\\nWhen prompted, enter your token to log in:\\n\\n```py\\n&gt;&gt;&gt; from huggingface_hub import notebook_login\\n\\n&gt;&gt;&gt; notebook_login()\\n```\\n\\nLet's define some global variables.\\n\\n```py\\n&gt;&gt;&gt; model_checkpoint = \"microsoft/layoutlmv2-base-uncased\"\\n&gt;&gt;&gt; batch_size = 4\\n```\\n\\n## Load the data\\n\\nIn this guide we use a small sample of preprocessed DocVQA that you can find on ðŸ¤— Hub. If you'd like to use the full\\nDocVQA dataset, you can register and download it on [DocVQA homepage](https://rrc.cvc.uab.es/?ch=17). If you do so, to\\nproceed with this guide check out [how to load files into a ðŸ¤— dataset](https://huggingface.co/docs/datasets/loading#local-and-remote-files).\\n\\n```py\\n&gt;&gt;&gt; from datasets import load_dataset]</td>\n",
       "      <td>1</td>\n",
       "      <td>The response provided does not accurately reflect the correct command as per the reference answer. The response suggests using a method `espnet_model.push_to_hub(\"my-awesome-model\")`, which implies a Python function call rather than a command-line instruction. The reference answer indicates that the correct command should be a shell command, specifically `./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo`. There is no mention of a `run.sh` script or the associated parameters in the evaluated response. Therefore, the response does not meet the criteria for being correct, accurate, and factual according to the provided score rubric.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>What parameter is used to ensure that elements in a row have the same height in Gradio?\\n</td>\n",
       "      <td>equal_height</td>\n",
       "      <td>gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md</td>\n",
       "      <td>The parameter used to ensure that elements in a row have the same height in Gradio is the `grow` parameter in the `Row` component. When set to True for a component, it will grow to fill any remaining vertical space in the row. This ensures that all components in the row have the same height. Here's an example:\\n\\n```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    with gr.Row(grow=</td>\n",
       "      <td>[Learn more about Columns in the [docs](https://gradio.app/docs/#column).\\n\\n# Dimensions\\n\\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\\n\\nBelow is an example illustrating the use of viewport width (vw):\\n\\n```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    im = gr.ImageEditor(\\n        width=\"50vw\",\\n    )\\n\\ndemo.launch()\\n```\\n\\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\\n\\n\\n```python\\nimport gradio as gr\\n\\ncss = \"\"\"\\n.container {\\n    height: 100vh;\\n}\\n\"\"\", ```python\\nimport gradio as gr\\n\\ncss = \"\"\"\\n.container {\\n    height: 100vh;\\n}\\n\"\"\"\\n\\nwith gr.Blocks(css=css) as demo:\\n    with gr.Column(elem_classes=[\"container\"]):\\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\\n\\ndemo.launch()\\n```\\n\\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.\\n\\nYou can apply any valid CSS unit for these parameters. For a comprehensive list of CSS units, refer to [this guide](https://www.w3schools.com/cssref/css_units.php). We recommend you always consider responsiveness and test your interfaces on various screen sizes to ensure a consistent user experience.\\n\\n\\n\\n## Tabs and Accordions, - [#5283](https://github.com/gradio-app/gradio/pull/5283) [`a7460557`](https://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d) - Add height parameter and scrolling to `gr.Dataframe`. Thanks [@abidlabs](https://github.com/abidlabs)!, Here's an example of how to use these attributes to create a Gradio app that does not lazy load and has an initial height of 0px.\\n\\n```html\\n&lt;gradio-app\\n\\tspace=\"gradio/Echocardiogram-Segmentation\"\\n\\teager=\"true\"\\n\\tinitial_height=\"0px\"\\n&gt;&lt;/gradio-app&gt;\\n```\\n\\nHere's another example of how to use the `render` event. An event listener is used to capture the `render` event and will call the `handleLoadComplete()` function once rendering is complete. \\n\\n```html\\n&lt;script&gt;\\n\\tfunction handleLoadComplete() {\\n\\t\\tconsole.log(\"Embedded space has finished rendering\");\\n\\t}\\n\\n\\tconst gradioApp = document.querySelector(\"gradio-app\");\\n\\tgradioApp.addEventListener(\"render\", handleLoadComplete);\\n&lt;/script&gt;\\n```, - Fixed Dropdown height rendering in Columns by [@aliabid94](https://github.com/aliabid94) in [PR 4584](https://github.com/gradio-app/gradio/pull/4584)\\n- Fixed bug where `AnnotatedImage` css styling was causing the annotation masks to not be displayed correctly by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4628](https://github.com/gradio-app/gradio/pull/4628)\\n- Ensure that Gradio does not silently fail when running on a port that is occupied by [@abidlabs](https://github.com/abidlabs) in [PR 4624](https://github.com/gradio-app/gradio/pull/4624).\\n- Fix double upload bug that caused lag in file uploads by [@aliabid94](https://github.com/aliabid94) in [PR 4661](https://github.com/gradio-app/gradio/pull/4661)\\n- `Progress` component now appears even when no `iterable` is specified in `tqdm` constructor by [@itrushkin](https://github.com/itrushkin) in [PR 4475](https://github.com/gradio-app/gradio/pull/4475), - Fixed Dropdown height rendering in Columns by [@aliabid94](https://github.com/aliabid94) in [PR 4584](https://github.com/gradio-app/gradio/pull/4584)\\n- Fixed bug where `AnnotatedImage` css styling was causing the annotation masks to not be displayed correctly by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4628](https://github.com/gradio-app/gradio/pull/4628)\\n- Ensure that Gradio does not silently fail when running on a port that is occupied by [@abidlabs](https://github.com/abidlabs) in [PR 4624](https://github.com/gradio-app/gradio/pull/4624).\\n- Fix double upload bug that caused lag in file uploads by [@aliabid94](https://github.com/aliabid94) in [PR 4661](https://github.com/gradio-app/gradio/pull/4661)\\n- `Progress` component now appears even when no `iterable` is specified in `tqdm` constructor by [@itrushkin](https://github.com/itrushkin) in [PR 4475](https://github.com/gradio-app/gradio/pull/4475), Learn more about Rows in the [docs](https://gradio.app/docs/#row).\\n\\n## Columns and Nesting\\n\\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\\n\\n$code_rows_and_columns\\n$demo_rows_and_columns\\n\\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\\n\\nLearn more about Columns in the [docs](https://gradio.app/docs/#column).\\n\\n# Dimensions]</td>\n",
       "      <td>1</td>\n",
       "      <td>Feedback: The response provided is incorrect. The parameter mentioned, `grow`, does not exist in the context of Gradio's `Row` component as per the reference answer. The correct parameter for ensuring that elements in a row have the same height in Gradio is `equal_height`. The response does not align with the reference answer and is therefore not factual or accurate in the context of the question.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     question  \\\n",
       "33  What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\\n   \n",
       "59              What is the command to upload an ESPnet model to a Hugging Face repository?\\n   \n",
       "65  What parameter is used to ensure that elements in a row have the same height in Gradio?\\n   \n",
       "\n",
       "                                                                 true_answer  \\\n",
       "33                                                            Beautiful Soup   \n",
       "59  ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo   \n",
       "65                                                              equal_height   \n",
       "\n",
       "                                                                                source_doc  \\\n",
       "33                 huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md   \n",
       "59                                       huggingface/hub-docs/blob/main/docs/hub/espnet.md   \n",
       "65  gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                generated_answer  \\\n",
       "33  The library used by MarkupLMFeatureExtractor to extract data from HTML and XML files is Beautiful Soup, as mentioned in the context provided. This information can be found in the usage instructions for MarkupLMProcessor, specifically in the documentation for the `MarkupLMFeatureExtractor` class. However, it should be noted that while using Beautiful Soup is the easiest way to prepare data for the model, it is not a requirement. The context also explains that users can still provide their   \n",
       "59                                                                                                              The command to upload an ESPnet model to a Hugging Face repository is:\\n\\n```bash\\nespnet_model.push_to_hub(\"my-awesome-model\")\\n```\\n\\nThis command is part of the `espnet` toolkit for speech processing, which can be found in the Hugging Face Hub. To learn more about sharing models with Hugging Face, refer to the official guide provided in the context. Additionally, you can use the   \n",
       "65                                                                                                 The parameter used to ensure that elements in a row have the same height in Gradio is the `grow` parameter in the `Row` component. When set to True for a component, it will grow to fill any remaining vertical space in the row. This ensures that all components in the row have the same height. Here's an example:\\n\\n```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    with gr.Row(grow=   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    retrieved_docs  \\\n",
       "33  [In short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\\nand it will create the inputs expected by the model. Internally, the processor first uses\\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\\nto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\\nOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.\\n\\n[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]., ## Usage: MarkupLMProcessor\\n\\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\\nif you only want to handle one of the two tasks.\\n\\n```python\\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\\n\\nfeature_extractor = MarkupLMFeatureExtractor()\\ntokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\\nprocessor = MarkupLMProcessor(feature_extractor, tokenizer)\\n```, In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).\\n\\n**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**\\n\\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML.\\n\\n```python\\n>>> from transformers import MarkupLMProcessor\\n\\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\\n\\n>>> html_string = \"\"\"\\n...  <!DOCTYPE html>\\n...  <html>\\n...  <head>\\n...  <title>Hello world</title>\\n...  </head>\\n...  <body>\\n...  <h1>Welcome</h1>\\n...  <p>Here is my website.</p>\\n...  </body>\\n...  </html>\"\"\", -->\\n\\n# MarkupLM\\n\\n## Overview\\n\\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\\nperformance, similar to [LayoutLM](layoutlm).\\n\\nThe model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\\nstate-of-the-art results on 2 important benchmarks:\\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\\n- [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\\nfor information extraction from web pages (basically named-entity recogntion on web pages), ```python\\n>>> from transformers import MarkupLMProcessor\\n\\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\\n\\n>>> html_string = \"\"\"\\n...  <!DOCTYPE html>\\n...  <html>\\n...  <head>\\n...  <title>Hello world</title>\\n...  </head>\\n...  <body>\\n...  <h1>Welcome</h1>\\n...  <p>My name is Niels.</p>\\n...  </body>\\n...  </html>\"\"\"\\n\\n>>> question = \"What's his name?\"\\n>>> encoding = processor(html_string, questions=question, return_tensors=\"pt\")\\n>>> print(encoding.keys())\\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\\n```\\n\\n**Use case 5: web page question answering (inference), parse_html=False**\\n\\nFor question answering tasks (such as WebSRC), you can provide a question to the processor. If you have extracted\\nall nodes and xpaths yourself, you can provide them directly to the processor. Make sure to set `parse_html` to `False`.\\n\\n```python\\n>>> from transformers import MarkupLMProcessor, ```python\\n>>> from transformers import MarkupLMProcessor\\n\\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\\n>>> processor.parse_html = False\\n\\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\\n>>> question = \"What's his name?\"\\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, questions=question, return_tensors=\"pt\")\\n>>> print(encoding.keys())\\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\\n```\\n\\n## Resources\\n\\n- [Demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)\\n- [Text classification task guide](../tasks/sequence_classification)\\n- [Token classification task guide](../tasks/token_classification)\\n- [Question answering task guide](../tasks/question_answering)\\n\\n## MarkupLMConfig\\n\\n[[autodoc]] MarkupLMConfig\\n    - all\\n\\n## MarkupLMFeatureExtractor, >>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\\n>>> encoding = processor(html_string, return_tensors=\"pt\")\\n>>> print(encoding.keys())\\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\\n```\\n\\n**Use case 2: web page classification (training, inference) + token classification (inference), parse_html=False**\\n\\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to `False`.\\n\\n```python\\n>>> from transformers import MarkupLMProcessor\\n\\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\\n>>> processor.parse_html = False]   \n",
       "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [The `run.sh` script allows to upload a given model to a Hugging Face repository.\\n\\n```bash\\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\\n```\\n\\n## Additional resources\\n\\n* ESPnet [docs](https://espnet.github.io/espnet/index.html).\\n* ESPnet model zoo [repository](https://github.com/espnet/espnet_model_zoo).\\n* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md)., Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\\n\\n### Finding models\\n\\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\\n\\n\\n### Widgets, Using ESPnet at Hugging Face\\n\\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\\n\\n## Exploring ESPnet in the Hub\\n\\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). \\n\\nAll models on the Hub come up with useful features:\\n1. An automatically generated model card with a description, a training configuration, licenses and more.\\n2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\\n3. An interactive widget you can use to play out with the model directly in the browser.\\n4. An Inference API that allows to make inference requests., ```py\\nimport soundfile\\nfrom espnet2.bin.tts_inference import Text2Speech\\n\\ntext2speech = Text2Speech.from_pretrained(\"model_name\")\\nspeech = text2speech(\"foobar\")[\"wav\"]\\nsoundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM_16\")\\n```\\n\\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! \\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png\"/>\\n</div>\\n\\n## Sharing your models\\n\\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model))., ```py\\n>>> tokenizer.push_to_hub(\"my-awesome-model\")\\n```\\n\\nOr perhaps you'd like to add the TensorFlow version of your fine-tuned PyTorch model:\\n\\n```py\\n>>> tf_model.push_to_hub(\"my-awesome-model\")\\n```\\n\\nNow when you navigate to your Hugging Face profile, you should see your newly created model repository. Clicking on the **Files** tab will display all the files you've uploaded to the repository.\\n\\nFor more details on how to create and upload files to a repository, refer to the Hub documentation [here](https://huggingface.co/docs/hub/how-to-upstream).\\n\\n## Upload with the web interface\\n\\nUsers who prefer a no-code approach are able to upload a model through the Hub's web interface. Visit [huggingface.co/new](https://huggingface.co/new) to create a new repository:\\n\\n![new_model_repo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png)\\n\\nFrom here, add some information about your model:, ## Sharing your models\\n\\nYou can easily upload your models using `mlagents-push-to-hf`:\\n\\n```\\nmlagents-push-to-hf --run-id=\"First Training\" --local-dir=\"results/First Training\" --repo-id=\"ThomasSimonini/MLAgents-Pyramids\" --commit-message=\"Pyramids\"\\n```\\n\\nYou need to define four parameters:\\n- `--run-id`: the name of the training run id.\\n- `--local-dir`: where the model was saved.\\n- `--repo-id`: the name of the Hugging Face repo you want to create or update. Itâ€™s `<your huggingface username>/<the repo name>`.\\n- `--commit-message`.\\n\\n\\n## Additional resources\\n\\n* ML-Agents [documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Hugging-Face-Integration.md)\\n* Official Unity ML-Agents Spaces [demos](https://huggingface.co/unity), ```bash\\nsudo apt install tesseract-ocr\\npip install -q pytesseract\\n```\\n\\nOnce you have installed all of the dependencies, restart your runtime.\\n\\nWe encourage you to share your model with the community. Log in to your Hugging Face account to upload it to the ðŸ¤— Hub.\\nWhen prompted, enter your token to log in:\\n\\n```py\\n>>> from huggingface_hub import notebook_login\\n\\n>>> notebook_login()\\n```\\n\\nLet's define some global variables.\\n\\n```py\\n>>> model_checkpoint = \"microsoft/layoutlmv2-base-uncased\"\\n>>> batch_size = 4\\n```\\n\\n## Load the data\\n\\nIn this guide we use a small sample of preprocessed DocVQA that you can find on ðŸ¤— Hub. If you'd like to use the full\\nDocVQA dataset, you can register and download it on [DocVQA homepage](https://rrc.cvc.uab.es/?ch=17). If you do so, to\\nproceed with this guide check out [how to load files into a ðŸ¤— dataset](https://huggingface.co/docs/datasets/loading#local-and-remote-files).\\n\\n```py\\n>>> from datasets import load_dataset]   \n",
       "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [Learn more about Columns in the [docs](https://gradio.app/docs/#column).\\n\\n# Dimensions\\n\\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\\n\\nBelow is an example illustrating the use of viewport width (vw):\\n\\n```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    im = gr.ImageEditor(\\n        width=\"50vw\",\\n    )\\n\\ndemo.launch()\\n```\\n\\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\\n\\n\\n```python\\nimport gradio as gr\\n\\ncss = \"\"\"\\n.container {\\n    height: 100vh;\\n}\\n\"\"\", ```python\\nimport gradio as gr\\n\\ncss = \"\"\"\\n.container {\\n    height: 100vh;\\n}\\n\"\"\"\\n\\nwith gr.Blocks(css=css) as demo:\\n    with gr.Column(elem_classes=[\"container\"]):\\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\\n\\ndemo.launch()\\n```\\n\\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.\\n\\nYou can apply any valid CSS unit for these parameters. For a comprehensive list of CSS units, refer to [this guide](https://www.w3schools.com/cssref/css_units.php). We recommend you always consider responsiveness and test your interfaces on various screen sizes to ensure a consistent user experience.\\n\\n\\n\\n## Tabs and Accordions, - [#5283](https://github.com/gradio-app/gradio/pull/5283) [`a7460557`](https://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d) - Add height parameter and scrolling to `gr.Dataframe`. Thanks [@abidlabs](https://github.com/abidlabs)!, Here's an example of how to use these attributes to create a Gradio app that does not lazy load and has an initial height of 0px.\\n\\n```html\\n<gradio-app\\n\\tspace=\"gradio/Echocardiogram-Segmentation\"\\n\\teager=\"true\"\\n\\tinitial_height=\"0px\"\\n></gradio-app>\\n```\\n\\nHere's another example of how to use the `render` event. An event listener is used to capture the `render` event and will call the `handleLoadComplete()` function once rendering is complete. \\n\\n```html\\n<script>\\n\\tfunction handleLoadComplete() {\\n\\t\\tconsole.log(\"Embedded space has finished rendering\");\\n\\t}\\n\\n\\tconst gradioApp = document.querySelector(\"gradio-app\");\\n\\tgradioApp.addEventListener(\"render\", handleLoadComplete);\\n</script>\\n```, - Fixed Dropdown height rendering in Columns by [@aliabid94](https://github.com/aliabid94) in [PR 4584](https://github.com/gradio-app/gradio/pull/4584)\\n- Fixed bug where `AnnotatedImage` css styling was causing the annotation masks to not be displayed correctly by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4628](https://github.com/gradio-app/gradio/pull/4628)\\n- Ensure that Gradio does not silently fail when running on a port that is occupied by [@abidlabs](https://github.com/abidlabs) in [PR 4624](https://github.com/gradio-app/gradio/pull/4624).\\n- Fix double upload bug that caused lag in file uploads by [@aliabid94](https://github.com/aliabid94) in [PR 4661](https://github.com/gradio-app/gradio/pull/4661)\\n- `Progress` component now appears even when no `iterable` is specified in `tqdm` constructor by [@itrushkin](https://github.com/itrushkin) in [PR 4475](https://github.com/gradio-app/gradio/pull/4475), - Fixed Dropdown height rendering in Columns by [@aliabid94](https://github.com/aliabid94) in [PR 4584](https://github.com/gradio-app/gradio/pull/4584)\\n- Fixed bug where `AnnotatedImage` css styling was causing the annotation masks to not be displayed correctly by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4628](https://github.com/gradio-app/gradio/pull/4628)\\n- Ensure that Gradio does not silently fail when running on a port that is occupied by [@abidlabs](https://github.com/abidlabs) in [PR 4624](https://github.com/gradio-app/gradio/pull/4624).\\n- Fix double upload bug that caused lag in file uploads by [@aliabid94](https://github.com/aliabid94) in [PR 4661](https://github.com/gradio-app/gradio/pull/4661)\\n- `Progress` component now appears even when no `iterable` is specified in `tqdm` constructor by [@itrushkin](https://github.com/itrushkin) in [PR 4475](https://github.com/gradio-app/gradio/pull/4475), Learn more about Rows in the [docs](https://gradio.app/docs/#row).\\n\\n## Columns and Nesting\\n\\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\\n\\n$code_rows_and_columns\\n$demo_rows_and_columns\\n\\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\\n\\nLearn more about Columns in the [docs](https://gradio.app/docs/#column).\\n\\n# Dimensions]   \n",
       "\n",
       "    eval_score_GPT4  \\\n",
       "33                1   \n",
       "59                1   \n",
       "65                1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       eval_feedback_GPT4  \n",
       "33                                                     Feedback: The response provided is incorrect. The reference answer states that the correct library is \"Beautiful Soup,\" but the response inaccurately attributes the functionality to MarkupLMFeatureExtractor, which is not a library but a class within a library. The response does not directly answer the question about which library is used, instead, it discusses the usage of the MarkupLMFeatureExtractor class and mentions Beautiful Soup in an incorrect context. Based on the score rubric, the response is not factual as it confuses the class with the library and does not correctly identify the library used.  \n",
       "59  The response provided does not accurately reflect the correct command as per the reference answer. The response suggests using a method `espnet_model.push_to_hub(\"my-awesome-model\")`, which implies a Python function call rather than a command-line instruction. The reference answer indicates that the correct command should be a shell command, specifically `./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo`. There is no mention of a `run.sh` script or the associated parameters in the evaluated response. Therefore, the response does not meet the criteria for being correct, accurate, and factual according to the provided score rubric.  \n",
       "65                                                                                                                                                                                                                                                                       Feedback: The response provided is incorrect. The parameter mentioned, `grow`, does not exist in the context of Gradio's `Row` component as per the reference answer. The correct parameter for ensuring that elements in a row have the same height in Gradio is `equal_height`. The response does not align with the reference answer and is therefore not factual or accurate in the context of the question.  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[results_df['eval_score_GPT4'] < 3].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8246268656716418"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
