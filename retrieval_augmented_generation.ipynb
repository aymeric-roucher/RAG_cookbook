{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from datasets import Dataset\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize RAG workflow\n",
    "\n",
    "<img src=\"rag_workflow.png\" height=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "ds = datasets.load_dataset(\"A-Roucher/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup evaluation pipeline\n",
    "In this part, we build a synthetic dataset of questions and associated contexts.\n",
    "\n",
    "The idea is to randomly get elements from our knowledge base, and ask a LLM to generate questions based on these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PREGENERATED_EVALUATION_DATASET = True\n",
    "\n",
    "if LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    eval_dataset = datasets.load_dataset(\"A-Roucher/huggingface_doc_qa_eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare source documents\n",
    "\n",
    "We use Langchain's `RecursiveCharacterTextSplitter`, which makes efficient use of code language detection to make better splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "    langchain_docs = [\n",
    "        LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "        for doc in tqdm(ds)\n",
    "    ]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in langchain_docs:\n",
    "        docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup chains for question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    QA_generation_prompt = \"\"\"\n",
    "    Your task is to write a factoid question and an answer given a context.\n",
    "    Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "    Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "    This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Output:::\n",
    "    Factoid question: (your factoid question)\n",
    "    Answer: (your answer to the factoid question)\n",
    "\n",
    "    Now here is the context.\n",
    "\n",
    "    Context: {context}\\n\n",
    "    Output:::\"\"\"\n",
    "\n",
    "    question_relatedness_critique_prompt = \"\"\"\n",
    "    You will be given a context and a question.\n",
    "    Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "    Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating)\n",
    "    Total rating: (your rating)\n",
    "\n",
    "    Now here are the question and context.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Context: {context}\\n\n",
    "    Answer::: \"\"\"\n",
    "\n",
    "    question_relevance_critique_prompt = \"\"\"\n",
    "    You will be given a question.\n",
    "    Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "    Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "    Rating must be minimum, i.e. 1, if the questions refers to a particular setting, like 'in the context' or 'in the document'.\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating)\n",
    "    Total rating: (your rating)\n",
    "\n",
    "    Now here is the question.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Answer::: \"\"\"\n",
    "\n",
    "    question_standalone_critique_prompt = \"\"\"\n",
    "    You will be given a question.\n",
    "    Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "    Give your answer on a scale of 1 to 5, where 1 means that the question only makes sense in a specific context, and 5 means that the question makes sense by itself.\n",
    "    For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "    The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating)\n",
    "    Total rating: (your rating)\n",
    "\n",
    "    Now here is the question.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Answer::: \"\"\"\n",
    "\n",
    "    chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0.2)\n",
    "    QA_generation_prompt = ChatPromptTemplate.from_template(QA_generation_prompt)\n",
    "    QA_generation_chain = QA_generation_prompt | chat_model\n",
    "\n",
    "    question_relatedness_critique_prompt = ChatPromptTemplate.from_template(\n",
    "        question_relatedness_critique_prompt\n",
    "    )\n",
    "    question_relatedness_critique_chain = (\n",
    "        question_relatedness_critique_prompt | chat_model\n",
    "    )\n",
    "\n",
    "    question_relevance_critique_prompt = ChatPromptTemplate.from_template(\n",
    "        question_relevance_critique_prompt\n",
    "    )\n",
    "    question_relevance_critique_chain = question_relevance_critique_prompt | chat_model\n",
    "\n",
    "    question_standalone_critique_prompt = ChatPromptTemplate.from_template(\n",
    "        question_standalone_critique_prompt\n",
    "    )\n",
    "    question_standalone_critique_chain = (\n",
    "        question_standalone_critique_prompt | chat_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    N_GENERATIONS = 30\n",
    "\n",
    "    print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "    outputs = []\n",
    "    for context in tqdm(random.sample(langchain_docs, N_GENERATIONS)):\n",
    "        # Generate QA couple\n",
    "        output_QA_couple = QA_generation_chain.invoke(\n",
    "            {\"context\": context.page_content}\n",
    "        ).content\n",
    "        try:\n",
    "            question = output_QA_couple.split(\"Factoid question: \")[1].split(\n",
    "                \"Answer: \"\n",
    "            )[0]\n",
    "            answer = output_QA_couple.split(\"Answer: \")[1]\n",
    "            outputs.append(\n",
    "                {\n",
    "                    \"context\": context.page_content,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"source_doc\": context.metadata[\"source\"],\n",
    "                }\n",
    "            )\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    print(\"Generating critique for each QA couple...\")\n",
    "    for output in tqdm(outputs):\n",
    "        # Critique the generated QA couple\n",
    "        question_relatedness_evaluation = question_relatedness_critique_chain.invoke(\n",
    "            {\"context\": output[\"context\"], \"question\": output[\"question\"]}\n",
    "        ).content\n",
    "        question_relevance_evaluation = question_relevance_critique_chain.invoke(\n",
    "            {\"question\": output[\"question\"]}\n",
    "        ).content\n",
    "        question_standalone_evaluation = question_standalone_critique_chain.invoke(\n",
    "            {\"question\": output[\"question\"]}\n",
    "        ).content\n",
    "\n",
    "        try:\n",
    "            relatedness_score = int(\n",
    "                question_relatedness_evaluation.split(\"Total rating: \")[1][0]\n",
    "            )\n",
    "            relatedness_eval = question_relatedness_evaluation.split(\"Total rating: \")[\n",
    "                0\n",
    "            ].split(\"Evaluation: \")[1]\n",
    "            relevance_score = int(\n",
    "                question_relevance_evaluation.split(\"Total rating: \")[1][0]\n",
    "            )\n",
    "            relevance_eval = question_relevance_evaluation.split(\"Total rating: \")[\n",
    "                0\n",
    "            ].split(\"Evaluation: \")[1]\n",
    "            standalone_score = int(\n",
    "                question_standalone_evaluation.split(\"Total rating: \")[1][0]\n",
    "            )\n",
    "            standalone_eval = question_standalone_evaluation.split(\"Total rating: \")[\n",
    "                0\n",
    "            ].split(\"Evaluation: \")[1]\n",
    "            output.update(\n",
    "                {\n",
    "                    \"relatedness_score\": relatedness_score,\n",
    "                    \"relatedness_eval\": relatedness_eval,\n",
    "                    \"relevance_score\": relevance_score,\n",
    "                    \"relevance_eval\": relevance_eval,\n",
    "                    \"standalone_score\": standalone_score,\n",
    "                    \"standalone_eval\": standalone_eval,\n",
    "                }\n",
    "            )\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter down questions on the chosen criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    import pandas as pd\n",
    "\n",
    "    pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "    generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "    display(generated_questions.head())\n",
    "    generated_questions = generated_questions.loc[\n",
    "        (generated_questions[\"relatedness_score\"] >= 4)\n",
    "        & (generated_questions[\"relevance_score\"] >= 3)\n",
    "        & (generated_questions[\"standalone_score\"] >= 4)\n",
    "    ]\n",
    "    generated_questions.to_excel(\"qa_eval.xlsx\")\n",
    "    display(generated_questions.head())\n",
    "\n",
    "    eval_dataset = datasets.Dataset.from_pandas(\n",
    "        generated_questions, split=\"train\", preserve_index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_SETTINGS = {\n",
    "    \"retriever\": {\"chunk_size\": 1000, \"use_instruct_embeddings\": True},\n",
    "    \"reader\": {\n",
    "        \"model_id\": \"facebook/dpr-reader-single-nq-base\",\n",
    "        \"rerank_results\": True,\n",
    "    },\n",
    "    \"pipeline\": {\"FLARE\": False},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Retriever - embeddings\n",
    "Here we use Langchain vector databases since it offers a convenient FAISS index and allows us to keep document metadata throughout the processing.\n",
    "\n",
    "Options included:\n",
    "\n",
    "- Tune the size of chunks retrieved\n",
    "- Change the embedding model\n",
    "\n",
    "More could be considered:\n",
    "- Try another chunking method, like semantic chunking\n",
    "- Change the index used (here, FAISS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "In this part, we split the documents from our knowledge base into smaller chunks which will be the snippets that will support our answer.\n",
    "\n",
    "The goal is to have semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large in order to be centered around a key information.\n",
    "\n",
    "[This space](https://huggingface.co/spaces/A-Roucher/chunk_visualizer) lets you visualize how different chunking options affect the chunks you get, to help you tune this step.\n",
    "\n",
    "Options:\n",
    "- split respecting sentence boundaries\n",
    "- semantic splits\n",
    "\n",
    "\n",
    "Depending on your embedding model, chunks might be split at 512 tokens: so you may want to visualize the length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_documents(chunk_size: int, knowledge_base: List[LangchainDocument]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    return docs_processed\n",
    "\n",
    "\n",
    "docs_processed = split_documents(1000, RAW_KNOWLEDGE_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in docs_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lengths).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬇️🚨 The cell below takes 15min to run on A10G for chunk size 1000, and non-instruct embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import (\n",
    "    HuggingFaceInstructEmbeddings,\n",
    "    HuggingFaceEmbeddings,\n",
    "    HuggingFaceBgeEmbeddings,\n",
    ")\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    use_instruct_embeddings: Optional[bool] = False,\n",
    "):\n",
    "    # load embedding_model\n",
    "    if use_instruct_embeddings:\n",
    "        model_name = \"thenlper/gte-small\"\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\n",
    "                \"normalize_embeddings\": True\n",
    "            },  # set True to compute cosine similarity\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            multi_process=True,\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\n",
    "                \"normalize_embeddings\": True\n",
    "            },  # set True to compute cosine similarity\n",
    "        )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_instruct:{use_instruct_embeddings}\"\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=100,\n",
    "            add_start_index=True,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        )\n",
    "        docs_processed = []\n",
    "        for doc in langchain_docs:\n",
    "            docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_INDEX = load_embeddings(\n",
    "    RAW_KNOWLEDGE_BASE,\n",
    "    chunk_size=1000,\n",
    "    use_instruct_embeddings=False,\n",
    ")\n",
    "print(\"\\nStarting retrieval:\")\n",
    "docs = KNOWLEDGE_INDEX.similarity_search(query=\"How to create a pipeline object?\", k=5)\n",
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Reader - LLM\n",
    "Here we tried the following options to improve results\n",
    "- Turn on/off reranking\n",
    "- Change the reader model\n",
    "\n",
    "Many options could be considered here to further improve the results\n",
    "- Add few-shot prompting: this was cancelled becaue it would add a lot of noise in the prompt.\n",
    "- add prompt compression to keep only the relevant parts of the prompt.\n",
    "- further extensions:\n",
    "    - cite source\n",
    "    - make conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context, \n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "  </s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\n",
    "    \"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", max_new_tokens=3500\n",
    ")\n",
    "\n",
    "llm(\"Ok,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "API_URL = \"https://ytjpei7t003tedav.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.getenv('HF_TOKEN')}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_llm_answer(question):\n",
    "    payload = {\n",
    "        \"inputs\": question,\n",
    "        \"max_new_tokens\": 3000,\n",
    "    }\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "\n",
    "def apply_reranking(\n",
    "    question: str,\n",
    "    documents: List[LangchainDocument],\n",
    "    reranker: RAGPretrainedModel,\n",
    "    num_docs_final: Optional[int] = 5,\n",
    ") -> List[LangchainDocument]:\n",
    "    # keep only the top k documents\n",
    "    documents_sorted_in_order = reranker.rerank(question, documents, k=num_docs_final)\n",
    "\n",
    "    # Even and odd documents are ordered in opposite directions to put the most relevant documents at the beginning and end\n",
    "    documents_even = documents_sorted_in_order[::2]\n",
    "    documents_odd = documents_sorted_in_order[1::2]\n",
    "    documents = documents_even + documents_odd[::-1]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: BaseChatModel,\n",
    "    knowledge_index: FAISS,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(\n",
    "        query=question, k=num_retrieved_docs\n",
    "    )\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        apply_reranking(\n",
    "            question, relevant_docs, reranker, num_docs_final=num_docs_final\n",
    "        )\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"{str(i)}: \" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = prompt_template.format(context=context, question=question)\n",
    "\n",
    "    # Redact an answer\n",
    "    output = llm(final_prompt)\n",
    "    try:\n",
    "        answer = output[0][\"generated_text\"]\n",
    "    except:\n",
    "        # Retry with shorter contect\n",
    "        context = \"\\nExtracted documents:\\n\"\n",
    "        context += \"\".join(\n",
    "            [f\"{str(i)}: \" + doc for i, doc in enumerate(relevant_docs[:-1])]\n",
    "        )\n",
    "\n",
    "        final_prompt = prompt_template.format(context=context, question=question)\n",
    "\n",
    "        # Redact an answer\n",
    "        output = llm(final_prompt)\n",
    "        answer = output[0][\"generated_text\"]\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Test\n",
    "How does our RAG pipeline perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how to create a pipeline object?\"\n",
    "answer, relevant_docs = answer_with_rag(\n",
    "    question, get_llm_answer, KNOWLEDGE_INDEX, reranker=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Answer:\")\n",
    "print(f\"{answer}\")\n",
    "print(\"\\n\\nSource documents:\")\n",
    "for doc in relevant_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking the final system on your evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: Dataset,\n",
    "    llm: BaseChatModel,\n",
    "    knowledge_index: FAISS,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "):\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "    for example in tqdm(eval_dataset[\"train\"]):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(\n",
    "            question, llm, knowledge_index, reranker=reranker\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\":::::::\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"true_answer\": example[\"answer\"],\n",
    "                \"source_doc\": example[\"source_doc\"],\n",
    "                \"generated_answer\": answer,\n",
    "                \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "evaluator_name = \"GPT4\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(answer_path, \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "    except:\n",
    "        answers = []\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [\n",
    "            item.strip() for item in eval_result.content.split(\"[RESULT]\")\n",
    "        ]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_SETTINGS = {}\n",
    "\n",
    "for chunk_size in [500]:\n",
    "    for use_instruct_embeddings in [True]:\n",
    "        for rerank in [True, False]:\n",
    "            RAG_SETTINGS[\n",
    "                f\"chunk:{chunk_size}_instruct:{use_instruct_embeddings}_rerank:{rerank}_reader:mistral-7b\"\n",
    "            ] = {\n",
    "                \"retriever\": {\n",
    "                    \"chunk_size\": chunk_size,\n",
    "                    \"use_instruct_embeddings\": use_instruct_embeddings,\n",
    "                },\n",
    "                \"reader\": {\n",
    "                    \"rerank_results\": rerank,\n",
    "                },\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_llm_answer(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for settings_name, settings in RAG_SETTINGS.items():\n",
    "    print(f\"Running evaluation for {settings_name}:\")\n",
    "    output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "    print(\"Loading knowledge base embeddings...\")\n",
    "    knowledge_index = load_embeddings(\n",
    "        RAW_KNOWLEDGE_BASE,\n",
    "        chunk_size=settings[\"retriever\"][\"chunk_size\"],\n",
    "        use_instruct_embeddings=settings[\"retriever\"][\"use_instruct_embeddings\"],\n",
    "    )\n",
    "\n",
    "    print(\"Running RAG...\")\n",
    "    if settings[\"reader\"][\"rerank_results\"]:\n",
    "        reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "    else:\n",
    "        reranker = None\n",
    "    run_rag_tests(\n",
    "        eval_dataset=eval_dataset,\n",
    "        llm=get_llm_answer,\n",
    "        knowledge_index=knowledge_index,\n",
    "        output_file=output_file_name,\n",
    "        reranker=reranker,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    print(\"Running evaluation...\")\n",
    "    evaluate_answers(\n",
    "        output_file_name,\n",
    "        eval_chat_model,\n",
    "        evaluator_name,\n",
    "        evaluation_prompt_template,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "results = []\n",
    "for file in glob.glob(\"output/*.json\"):\n",
    "    print(file)\n",
    "    with open(file, \"r\") as f:\n",
    "        sub_df = pd.read_json(f)\n",
    "        sub_df[\"settings\"] = file.split(\"rag_\")[1].split(\".json\")[0]\n",
    "        results.append(sub_df)\n",
    "\n",
    "results_df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_result(x):\n",
    "    try:\n",
    "        return (int(x) - 1) / 4\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "results_df[\"normalized_score\"] = results_df[\"eval_score_GPT4\"].apply(interpret_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby(\"settings\")[\"normalized_score\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
