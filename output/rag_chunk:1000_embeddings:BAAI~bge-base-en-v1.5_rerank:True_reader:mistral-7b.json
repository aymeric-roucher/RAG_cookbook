[{"question": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n", "true_answer": "x86_64-unknown-linux-musl", "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md", "generated_answer": "The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture. (Answer according to context provided.)", "retrieved_docs": ["`tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-arm64-musl`\n\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-x64-gnu`\n\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-freebsd-x64`\n\nThis is the **x86_64-unknown-freebsd** binary for `tokenizers`", "`tokenizers-linux-arm64-gnu`\n\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-darwin-x64`\n\nThis is the **x86_64-apple-darwin** binary for `tokenizers`", "`tokenizers-linux-arm-gnueabihf`\n\nThis is the **armv7-unknown-linux-gnueabihf** binary for `tokenizers`"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, indicating that the `tokenizers-linux-x64-musl` binary is designed for the x86_64-unknown-linux-musl architecture."}, {"question": "What is the purpose of the BLIP-Diffusion model?\n", "true_answer": "The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md", "generated_answer": "The BLIP-Diffusion model, proposed in the paper \"BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing,\" enables zero-shot subject-driven generation and control-guided zero-shot generation. It allows for efficient fine-tuning for customized subjects with up to 20x speedup compared to previous methods like DreamBooth. BLIP-Diffusion combines well with existing techniques like Control", "retrieved_docs": [". Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).*", "The [`StableDiffusionImg2ImgPipeline`] uses the diffusion-denoising mechanism proposed in [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](https://huggingface.co/papers/2108.01073) by Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon.\n\nThe abstract from the paper is:", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# BLIP-Diffusion\n\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. \n\n\nThe abstract from the paper is:", "The original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization.\n\n`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/).\n\n<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n\n</Tip>\n\n\n## BlipDiffusionPipeline\n[[autodoc]] BlipDiffusionPipeline\n    - all\n    - __call__\n\n## BlipDiffusionControlNetPipeline\n[[autodoc]] BlipDiffusionControlNetPipeline\n    - all\n    - __call__", "BLIP is a model that is able to perform various multi-modal tasks including:\n- Visual Question Answering \n- Image-Text retrieval (Image-text matching)\n- Image Captioning\n\nThe abstract from the paper is the following:", "However, challenges still remain:\n\n- These systems need to work for large high-resolution original images.\n- Diffusion models often invent or re-interpret an instruction to perform the modifications in the image space. For a realistic image editing application, this is unacceptable.\n\n## Open questions\n\nWe acknowledge that our experiments are preliminary. We did not go deep into ablating the apparent factors in our experiments. Hence, here we enlist a few open questions that popped up during our experiments:", "## What about the general-purpose Stable Diffusion model?\n\nAs we showed with the Pokemon image generation task, it is possible to achieve a high level of optimization of the Stable Diffusion pipeline when using a relatively small amount of training resources. At the same time, it is well-known that training a general-purpose Stable Diffusion model is an [expensive task](https://www.mosaicml.com/blog/training-stable-diffusion-from-scratch-part-2). However, with enough budget and HW resources, it is possible to optimize the general-purpose model using the described approach and tune it to produce high-quality images. The only caveat we have is related to the token merging method that reduces the model capacity substantially. The rule of thumb here is the more complicated the dataset you have for the training, the less merging ratio you should use during the optimization."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It mentions the purpose of the BLIP-Diffusion model as enabling zero-shot subject-driven generation and control-guided zero-shot generation, which aligns with the reference answer's mention of controllable text-to-image generation and editing. The response also adds details about efficient fine-tuning and speedup compared to previous methods, which, while not mentioned in the reference answer, are accurate and provide context to the model's purpose. However, the response could be more concise to directly reflect the reference answer's emphasis on the model's design for controllable text-to-image generation and editing."}, {"question": "How can a user claim authorship of a paper on the Hugging Face Hub?\n", "true_answer": "By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/paper-pages.md", "generated_answer": "To claim authorship of a paper on the Hugging Face Hub, a user should follow these steps:\n\n1. Visit the Paper page of the paper they wish to claim authorship for.\n2. Click on their name in the corresponding Paper page to open their profile.\n3. Click on \"claim authorship\" to initiate the request.\n4. The user will be redirected to their paper settings where they can confirm the request.\n5. The admin team will", "retrieved_docs": ["If your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings-dark.png\"/>\n</div>\n\n\n## Frequently Asked Questions\n\n### Can I control which Paper pages show in my profile?\n\nYes! You can visit your Papers in [settings](https://huggingface.co/settings/papers), where you will see a list of verified papers. There, you can click the \"Show on profile\" checkbox to hide/show it in your profile.", "* Visit the Paper page.\n* Filter for other models or datasets on the Hub that cite the same paper.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\n</div>\n\n## Claiming authorship to a Paper\n\nThe Hub will attempt to automatically match paper to users based on their email. \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\n</div>", "### Do you support ACL anthology?\n\nWe're starting with Arxiv as it accounts for 95% of the paper URLs Hugging Face users have linked in their repos organically. We'll check how this evolve and potentially extend to other paper hosts in the future.\n\n### Can I have a Paper page even if I have no model/dataset/Space?\n\nYes. You can go to [the main Papers page](https://huggingface.co/papers), click search and write the name of the paper or the full Arxiv id. If the paper does not exist, you will get an option to index it. You can also just visit the page `hf.co/papers/xxxx.yyyyy` replacing with the arxiv id of the paper you wish to index.", "--\ntitle: \"Welcome PaddlePaddle to the Hugging Face Hub\" \nthumbnail: /blog/assets/126_paddlepaddle/thumbnail.jpg\nauthors:\n- user: PaddlePaddle\n  guest: true\n---\n\n# Welcome PaddlePaddle to the Hugging Face Hub\n\n\nWe are happy to share an open source collaboration between Hugging Face and [PaddlePaddle](https://www.paddlepaddle.org.cn/en) on a shared mission to advance and democratize AI through open source!", "Username: \n   Password:\n   ```\n\n3. Alternatively, if you prefer working from a Jupyter or Colaboratory notebook, login with `notebook_login`:\n\n   ```python\n   >>> from huggingface_hub import notebook_login\n   >>> notebook_login()\n   ```\n\n   `notebook_login` will launch a widget in your notebook from which you can enter your Hugging Face credentials.\n\n## Download files from the Hub\n\nIntegration allows users to download your hosted files directly from the Hub using your library.", "_To be able to push our model to the Hub, you need to register on the [Hugging Face Hub](https://huggingface.co/join)._\n\nWe will use the `notebook_login` util from the `huggingface_hub` package to log into our account. You can get your token in the settings at [Access Tokens](https://huggingface.co/settings/tokens).\n\n\n```python\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\n```\n\nSince we are now logged in let's get the `user_id`, which will be used to push the artifacts.\n\n\n```python\nfrom huggingface_hub import HfApi\n\nuser_id = HfApi().whoami()[\"name\"]\n\nprint(f\"user id '{user_id}' will be used during the example\")\n```\n\n\nThe [original BERT](https://arxiv.org/abs/1810.04805) was pretrained on [Wikipedia](https://huggingface.co/datasets/wikipedia) and [BookCorpus](https://huggingface.co/datasets/bookcorpus) datasets. Both datasets are available on the [Hugging Face Hub](https://huggingface.co/datasets) and can be loaded with `datasets`.", "The base URL for the HTTP endpoints above is `https://huggingface.co`.\n\nThose endpoints are not officially supported in `huggingface_hub` or `huggingface.js` yet but [this code snippet](https://github.com/huggingface/huggingface_hub/issues/1535#issuecomment-1614693412) (in Python) might help you getting started.\n\n### Download access report\n\nYou can download a report of all access requests for a gated datasets with the **download user access report** button. Click on it to download a json file with a list of users. For each entry, you have:\n- **user**: the user id. Example: *julien-c*.\n- **fullname**: name of the user on the Hub. Example: *Julien Chaumond*.\n- **status**: status of the request. Either `\"pending\"`, `\"accepted\"` or `\"rejected\"`.\n- **email**: email of the user.\n- **time**: datetime when the user initially made the request.\n\n<a id=\"modifying-the-prompt\"></a> <!-- backward compatible anchor -->\n\n### Customize requested information"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It outlines a series of steps that a user should follow to claim authorship of a paper on the Hugging Face Hub, which aligns with the reference answer. However, the response is incomplete as it ends abruptly at step 5 without explaining what happens after the admin team is involved. The reference answer implies that the admin team's role is to validate the request, which is not mentioned in the evaluated response. Despite this, the steps listed are in the correct order and provide a clear process up to the point where the response ends. Therefore, the response is not completely correct but is mostly correct and factual."}, {"question": "What is the purpose of the /healthcheck endpoint in the Datasets server API?\n", "true_answer": "Ensure the app is running", "source_doc": "huggingface/datasets-server/blob/main/services/api/README.md", "generated_answer": "The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the application is running properly. It is a simple endpoint that can be accessed to check the status of the server. If the server is running correctly, it will return a successful response. This endpoint can be useful for monitoring the health of the server and ensuring that it is available for use.", "retrieved_docs": ["## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /webhook: Add, update or remove a dataset\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset", "## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub_cache` field is null for deleted entries, or when the response is an error. The `num_rows` value is `0` if it could not be determined.\n\n```typescript\n{\n    dataset: string;\n    hub_cache: null | {\n        preview: boolean;\n        viewer: boolean;\n        partial: boolean;\n        num_rows: int;\n    };\n}\n```", "</Tip>\n\nIn this case, an error is raised to alert that the dataset has changed.\nTo ignore the error, one needs to specify `verification_mode=\"no_checks\"` in [`load_dataset`].\nAnytime you see a verification error, feel free to open a discussion or pull request in the corresponding dataset \"Community\" tab, so that the integrity checks for that dataset are updated.\n\n## Security\n\nThe dataset repositories on the Hub are scanned for malware, see more information [here](https://huggingface.co/docs/hub/security#malware-scanning).\n\nMoreover the datasets without a namespace (originally contributed on our GitHub repository) have all been reviewed by our maintainers.\nThe code of these datasets is considered **safe**.\nIt concerns datasets that are not under a namespace, e.g. \"squad\" or \"glue\", unlike the other datasets that are named \"username/dataset_name\" or \"org/dataset_name\".", "Check dataset validity\n\nBefore you download a dataset from the Hub, it is helpful to know if a specific dataset you're interested in is available. Datasets Server provides the `/is-valid` endpoint to check if a specific dataset works without any errors.\n\nThe API endpoint will return an error for datasets that cannot be loaded with the [\ud83e\udd17 Datasets](https://github.com/huggingface/datasets) library, for example, because the data hasn't been uploaded or the format is not supported.\n\n<Tip warning={true}>\n  The largest datasets are partially supported by Datasets Server. If they are{\" \"}\n  <a href=\"https://huggingface.co/docs/datasets/stream\">streamable</a>, Datasets\n  Server can extract the first 100 rows without downloading the whole dataset.\n  This is especially useful for previewing large datasets where downloading the\n  whole dataset may take hours! See the <code>preview</code> field in the\n  response of <code>/is-valid</code> to check if a dataset is partially\n  supported.\n</Tip>", "| Endpoint                    | Method | Description                                             | Query parameters                                                                                                                                                                                                  |\n| --------------------------- | ------ | ------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [/is-valid](./valid)        | GET    | Check whether a specific dataset is valid.              | `dataset`: name of the dataset                                                                                                                                                                                    |", "Datasets server API - search service\n\n> /search endpoint\n> /filter endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### Duckdb index full text search\n- `DUCKDB_INDEX_CACHE_DIRECTORY`: directory where the temporal duckdb index files are downloaded. Defaults to empty.\n- `DUCKDB_INDEX_TARGET_REVISION`: the git revision of the dataset where the index file is stored in the dataset repository.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /search: get a slice of a search result over a dataset split\n- /filter: filter rows of a dataset split", "Datasets server API - rows endpoint\n\n> /rows endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer, which states the purpose of the /healthcheck endpoint is to ensure the app is running. The response expands on this by explaining that it is a simple endpoint to check the server's status and that a successful response indicates the server is running correctly. It also correctly mentions the utility of the endpoint in monitoring server health and availability, which is implied in the reference answer's mention of ensuring the app is running."}, {"question": "What is the default context window size for Local Attention in the LongT5 model?\n", "true_answer": "127 tokens", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md", "generated_answer": "The default context window size for Local Attention in the LongT5 model is 127 tokens, as mentioned in the context under the section \"Longformer self attention\". The window size can be configured differently for each layer through the `config.attention_window` parameter, but the text states that it is assumed that the number of \"globally\" attending tokens is insignificant as compared to the number of \"locally\" attending tokens. This suggests that the default window", "retrieved_docs": ["## Longformer Self Attention\n\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`.\n\nNote that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\nattending tokens so that global attention is *symmetric*.", "The user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global_attention_mask`:\n\n- 0: the token attends \"locally\",\n- 1: the token attends \"globally\".\n\nFor more information please also refer to [`~LongformerModel.forward`] method.\n\nUsing Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to\n\\\\(\\mathcal{O}(n_s \\times w)\\\\), with \\\\(n_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize. It is assumed that the number of \"globally\" attending tokens is insignificant as compared to the number of\n\"locally\" attending tokens.\n\nFor more information, please refer to the official [paper](https://arxiv.org/pdf/2004.05150.pdf).\n\n\n## Training", "- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.", "#### Follow-up questions\n\n* The increasing size (throughout the layers) of the dilated windowed self-attention echoes findings in computer vision on increasing the receptive field of stacked CNN. How do these two findings relate? What are the transposable learnings?\n* Longformer\u2019s Encoder-Decoder architecture works well for tasks that do not require a long target length (e.g. summarization). However, how would it work for long-range seq2seq tasks which require a long target length (e.g. document translation, speech recognition, etc.) especially considering the cross-attention layer of encoder-decoder\u2019s models?\n* In practice, the sliding window self-attention relies on many indexing operations to ensure a symmetric query-key weights matrix. Those operations are very slow on TPUs which highlights the question of the applicability of such patterns on other hardware.\n\n### [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)", "## Local attention\n\n[Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\nrepresentation of the whole sentence.\n\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:\n\n<div class=\"flex justify-center\">\n    <img scale=\"50 %\" align=\"center\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png\"/>\n</div>", "The abstract from the paper is the following:\n\n*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.*", "### Local Self-Attention\n\n **Local self-attention** is the obvious solution to reducing the \\\\(\\mathcal{O}(n^2)\\\\) memory bottleneck, allowing us to model longer sequences with a reduced computational cost. \nIn local self-attention the input \\\\( \\mathbf{X} = \\mathbf{X}_{1:n} = \\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n} \\\\) \nis cut into \\\\(n_{c}\\\\) chunks: \\\\( \\mathbf{X} = \\left[\\mathbf{X}_{1:l_{c}}, \\ldots, \\mathbf{X}_{(n_{c} - 1) * l_{c} : n_{c} * l_{c}}\\right] \\\\) each \nof length `config.local_chunk_length`, *i.e.* \\\\(l_{c}\\\\), and subsequently global self-attention is applied on each chunk separately.\n\nLet's take our input sequence for \\\\(n=16, d_h=3\\\\) again for visualization:\n\n![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/input.png)\n\nAssuming \\\\(l_{c} = 4, n_{c} = 4\\\\), chunked attention can be illustrated as follows:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the default context window size for Local Attention in the LongT5 model as 127 tokens, which matches the reference answer. The additional information provided about the configuration of the window size and the comparison between \"globally\" and \"locally\" attending tokens, while not required, does not detract from the factual accuracy of the response regarding the default window size."}, {"question": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n", "true_answer": "from_pretrained()", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md", "generated_answer": "To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method of the corresponding task-specific pipeline class. For example, to load the `runwayml/stable-diffusion-v1-5` checkpoint for text-to-image, you can use `AutoPipelineForText2Image.from_pretrained()`. This method automatically detects the correct pipeline class to use, making it easier to load a check", "retrieved_docs": ["The `AutoPipeline` class is designed to simplify the variety of pipelines in \ud83e\udd17 Diffusers. It is a generic, *task-first* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name.\n\n<Tip>\n\nTake a look at the [AutoPipeline](../api/pipelines/auto_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting.\n\n</Tip>\n\nThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights.\n\n## Choose an AutoPipeline for your task\n\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:", "To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:", "You can also load a checkpoint with its specific pipeline class. The example above loaded a Stable Diffusion model; to get the same result, use the [`StableDiffusionPipeline`] class:\n\n```python\nfrom diffusers import StableDiffusionPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)\n```\n\nA checkpoint (such as [`CompVis/stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4) or [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)) may also be used for more than one task, like text-to-image or image-to-image. To differentiate what task you want to use the checkpoint for, you have to load it directly with its corresponding task-specific pipeline class:\n\n```python\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(repo_id)\n```\n\n### Local pipeline", "Therefore you have two ways to take advantage of this very beneficial feature:\n\n1. If you want to use a HF Transformers models you can do `model.gradient_checkpointing_enable()` or use `--gradient_checkpointing` in the HF Trainer, which will automatically enable this for you. `torch.utils.checkpoint` is used there.\n2. If you write your own model and you want to use DeepSpeed's activation checkpointing you can use the [API prescribed there](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html). You can also take the HF Transformers modeling code and replace `torch.utils.checkpoint` with the DeepSpeed's API. The latter is more flexible since it allows you to offload the forward activations to the CPU memory instead of recalculating them.\n\n\n### Optimizer and Scheduler", "You can also instantiate the checkpoint using the model architecture directly:\n\n{#if fw === 'pt'}\n```py\nfrom transformers import CamembertTokenizer, CamembertForMaskedLM\n\ntokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\nmodel = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n```\n\nHowever, we recommend using the [`Auto*` classes](https://huggingface.co/transformers/model_doc/auto.html?highlight=auto#auto-classes) instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the `Auto*` classes makes switching checkpoints simple:\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"camembert-base\")\n```\n{:else}\n```py\nfrom transformers import CamembertTokenizer, TFCamembertForMaskedLM", "As you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it's going to be used for. For example, here we are loading the `camembert-base` checkpoint in the `fill-mask` pipeline, which is completely fine. But if we were to load this checkpoint in the `text-classification` pipeline, the results would not make any sense because the head of `camembert-base` is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/tasks.png\" alt=\"The task selector on the web interface.\" width=\"80%\"/>\n</div>\n\nYou can also instantiate the checkpoint using the model architecture directly:", "--gradient_checkpointing \\\n\t--use_auth_token \\\n\t--fp16 \\\n\t--group_by_length \\\n\t--do_train --do_eval \\\n\t--push_to_hub''' > run.sh\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the `from_pretrained()` method as the way to load a checkpoint using `AutoPipeline`. It also provides an example of how to use this method with a specific checkpoint, which aligns with the reference answer. The response is accurate and factual, demonstrating a clear understanding of the task-specific pipeline class and its method for loading checkpoints."}, {"question": "What is the purpose of Diffusers library?\n", "true_answer": "To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md", "generated_answer": "The Diffusers library, as stated in the context, is a toolbox for generating images, audio, and even 3D structures of molecules using state-of-the-art pretrained diffusion models. It offers both simple inference solutions and the ability to train one's own diffusion models. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions. The purpose of Diffusers is to make it easier for users", "retrieved_docs": ["# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:", "\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\n\ud83e\udd17 Diffusers offers three core components:", "## Diffusers\n\nDiffusers is a generative AI library for creating images and videos from text or images with diffusion models. LoRA is an especially popular training method for diffusion models because you can very quickly train and share diffusion models to generate images in new styles. To make it easier to use and try multiple LoRA models, Diffusers uses the PEFT library to help manage different adapters for inference.\n\nFor example, load a base model and then load the [artificialguybr/3DRedmond-V1](https://huggingface.co/artificialguybr/3DRedmond-V1) adapter for inference with the [`load_lora_weights`](https://huggingface.co/docs/diffusers/v0.24.0/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) method. The `adapter_name` argument in the loading method is enabled by PEFT and allows you to set a name for the adapter so it is easier to reference.\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline", "For large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.\n**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:", "For large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.\n**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:", "Great, now you should have generally understood why \ud83e\udde8 Diffusers is designed the way it is \ud83e\udd17.\nWe try to apply these design principles consistently across the library. Nevertheless, there are some minor exceptions to the philosophy or some unlucky design choices. If you have feedback regarding the design, we would \u2764\ufe0f  to hear it [directly on GitHub](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=).\n\n## Design Philosophy in Details\n\nNow, let's look a bit into the nitty-gritty details of the design philosophy. Diffusers essentially consists of three major classes: [pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines), [models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models), and [schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).\nLet's walk through more detailed design decisions for each class.\n\n### Pipelines", "Great, now you should have generally understood why \ud83e\udde8 Diffusers is designed the way it is \ud83e\udd17.\nWe try to apply these design principles consistently across the library. Nevertheless, there are some minor exceptions to the philosophy or some unlucky design choices. If you have feedback regarding the design, we would \u2764\ufe0f  to hear it [directly on GitHub](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=).\n\n## Design Philosophy in Details\n\nNow, let's look a bit into the nitty-gritty details of the design philosophy. Diffusers essentially consists of three major classes: [pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines), [models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models), and [schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).\nLet's walk through more in-detail design decisions for each class.\n\n### Pipelines"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided captures the essence of the Diffusers library's purpose by mentioning its role as a toolbox for generating various types of content using pretrained diffusion models and its focus on usability, simplicity, and customizability. It aligns well with the reference answer, which emphasizes the library's modularity and its capabilities in both inference and training across multiple modalities. The response could be more concise and directly aligned with the reference answer's wording, but it is factually correct and communicates the main purpose of the Diffusers library effectively."}, {"question": "What method does the EulerAncestralDiscreteScheduler use for sampling?\n", "true_answer": "Ancestral sampling with Euler method steps.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler_ancestral.md", "generated_answer": "The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps, as stated in the extract provided. This is based on the original implementation of k-diffusion by Katherine Crowson.", "retrieved_docs": ["# EulerAncestralDiscreteScheduler\n\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteSchedulerOutput", "# EulerDiscreteScheduler\n\nThe Euler scheduler (Algorithm 2) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n\n## EulerDiscreteScheduler\n[[autodoc]] EulerDiscreteScheduler\n\n## EulerDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_discrete.EulerDiscreteSchedulerOutput", "```python\nfrom diffusers import LMSDiscreteScheduler\n\npipeline.scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator).images[0]\nimage\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_lms.png\" width=\"400\"/>\n    <br>\n</p>\n\n\n[`EulerDiscreteScheduler`] and [`EulerAncestralDiscreteScheduler`] can generate high quality results with as little as 30 steps.\n\n```python\nfrom diffusers import EulerDiscreteScheduler\n\npipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```", "diffusers.schedulers.scheduling_pndm.PNDMScheduler,\n    diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler,\n    diffusers.schedulers.scheduling_ddim.DDIMScheduler,\n]\n```", "diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete.KDPM2AncestralDiscreteScheduler,\n diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler]\n```", "<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_euler_discrete.png\" width=\"400\"/>\n    <br>\n</p>\n\n\nand:\n\n```python\nfrom diffusers import EulerAncestralDiscreteScheduler\n\npipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_euler_ancestral.png\" width=\"400\"/>\n    <br>\n</p>\n\n\n[`DPMSolverMultistepScheduler`] gives a reasonable speed/quality trade-off and can be run with as little as 20 steps.\n\n```python\nfrom diffusers import DPMSolverMultistepScheduler\n\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)", "Cool, lots of schedulers to look at. Feel free to have a look at their respective class definitions:\n\n- [`EulerDiscreteScheduler`],\n- [`LMSDiscreteScheduler`],\n- [`DDIMScheduler`],\n- [`DDPMScheduler`],\n- [`HeunDiscreteScheduler`],\n- [`DPMSolverMultistepScheduler`],\n- [`DEISMultistepScheduler`],\n- [`PNDMScheduler`],\n- [`EulerAncestralDiscreteScheduler`],\n- [`UniPCMultistepScheduler`],\n- [`KDPM2DiscreteScheduler`],\n- [`DPMSolverSinglestepScheduler`],\n- [`KDPM2AncestralDiscreteScheduler`].\n\nWe will now compare the input prompt with all other schedulers. To change the scheduler of the pipeline you can make use of the\nconvenient [`~ConfigMixin.config`] property in combination with the [`~ConfigMixin.from_config`] function.\n\n```python\npipeline.scheduler.config\n```\n\nreturns a dictionary of the configuration of the scheduler:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that the EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps. Additionally, it provides context by mentioning the original implementation by Katherine Crowson, which does not detract from the factual correctness of the answer."}, {"question": "What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n", "true_answer": "IDEFICS", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md", "generated_answer": "The name of the large multimodal model that can solve image-text tasks and is based on Flamingo is IDEFICS (I**mage-aware **D**ecoder **E**nhanced \u00e0 la **F**lamingo with **I**nterleaved **C**ross-attention**S**). It is an open-access reproduction of Flamingo, initially developed by DeepMind, and comes in two variants with 9", "retrieved_docs": ["Try out the [demo](https://huggingface.co/spaces/HuggingFaceM4/idefics_playground) and the [models](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct) on the Hub!\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/idefics/woodstock_ai.png\" width=\"600\" alt=\"Screenshot of IDEFICS generation for HF Woodstock of AI\"/>\n</p>\n\n## What is IDEFICS?\n\nIDEFICS is an 80 billion parameters multimodal model that accepts sequences of images and texts as input and generates coherent text as output. It can answer questions about images, describe visual content, create stories grounded in multiple images, etc.\n\nIDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source model across various image-text understanding benchmarks. It comes in two variants - 80 billion parameters and 9 billion parameters.", "Another approach to leveraging pre-trained language models for multi-modal tasks is to directly fuse visual information into the layers of a language model decoder using a cross-attention mechanism instead of using images as additional prefixes to the language model. Models such as [VisualGPT](https://arxiv.org/abs/2102.10407), [VC-GPT](https://arxiv.org/abs/2201.12723), and [Flamingo](https://arxiv.org/abs/2204.14198) use this pre-training strategy and are trained on image captioning and visual question-answering tasks. The main goal of such models is to balance the mixture of text generation capacity and visual information efficiently, which is highly important in the absence of large multi-modal datasets.", "## Introduction\n\nRecent years have seen rapid advancements in computer vision and natural language processing. Still, many real-world \nproblems are inherently multimodal - they involve several distinct forms of data, such as images and text. \nVisual-language models face the challenge of combining modalities so that they can open the door to a wide range of \napplications. Some of the image-to-text tasks that visual language models can tackle include image captioning, image-text \nretrieval, and visual question answering. Image captioning can aid the visually impaired, create useful product descriptions, \nidentify inappropriate content beyond text, and more. Image-text retrieval can be applied in multimodal search, as well \nas in applications such as autonomous driving. Visual question-answering can aid in education, enable multimodal chatbots, \nand assist in various domain-specific information retrieval applications.", "-->\n\n# Image tasks with IDEFICS\n\n[[open-in-colab]]\n\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach \nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. \nFor instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. \nThis approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can \nsolve image-text tasks with a large multimodal model called IDEFICS.", ". The resulting InstructBLIP models achieve state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and the larger Flamingo. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA IMG). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models.*", "# Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model\n\n\nWe are excited to release IDEFICS (**I**mage-aware **D**ecoder **E**nhanced \u00e0 la **F**lamingo with **I**nterleaved **C**ross-attention**S**), an open-access visual language model. IDEFICS is based on [Flamingo](https://huggingface.co/papers/2204.14198), a state-of-the-art visual language model initially developed by DeepMind, which has not been released publicly. Similarly to GPT-4, the model accepts arbitrary sequences of image and text inputs and produces text outputs. IDEFICS is built solely on publicly available data and models (LLaMA v1 and OpenCLIP) and comes in two variants\u2014the base version and the instructed version. Each variant is available at the 9 billion and 80 billion parameter sizes.", "The abstract from the paper is the following:\n\n*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\ndirection would be to use a single holistic universal model, as a \"foundation\", that targets all modalities\nat once -- a true vision and language foundation model should be good at vision tasks, language tasks, and\ncross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate\nimpressive performance on a wide range of 35 tasks spanning these target modalities.*\n\nThis model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava).\n\n## FlavaConfig"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the name of the model as IDEFICS, which aligns with the reference answer. The additional information provided about the model being an open-access reproduction of Flamingo and mentioning its variants does not detract from the accuracy of the response regarding the name of the model. Therefore, the response meets the criteria for a score of 5 as it is completely correct, accurate, and factual based on the reference answer."}, {"question": "What is the purpose of the `gradio.Blocks` API?\n", "true_answer": "The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/7.mdx", "generated_answer": "The purpose of the `gradio.Blocks` API is to offer a low-level approach for designing web apps with more flexible layouts and data flows in Gradio. It allows users to have full control over the data flows and layout of their applications, enabling them to build complex, multi-step applications with features such as grouping related demos as multiple tabs, changing the layout of the demo, having multi-step interfaces, and updating component properties or visibility based on user input", "retrieved_docs": ["#### Custom Demos with `gr.Blocks`\n\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction \u2014 still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n#### The Gradio Python & JavaScript Ecosystem", "### Custom Demos with `gr.Blocks`\n\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction \u2014 still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n### The Gradio Python & JavaScript Ecosystem", "```py\nwith gr.Blocks() as demo:\n    gr.Markdown(\"\u0633\u0644\u0627\u0645\", rtl=True)\ndemo.launch()\n```\n\n- The `get_api_info` method of `Blocks` now supports layout output components [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4871](https://github.com/gradio-app/gradio/pull/4871)\n\n- Added the support for the new command `gradio environment`to make it easier for people to file bug reports if we shipped an easy command to list the OS, gradio version, and versions of gradio/gradio-client dependencies. bu [@varshneydevansh](https://github.com/varshneydevansh) in [PR 4915](https://github.com/gradio-app/gradio/pull/4915).\n\n### Bug Fixes:", "```py\nwith gr.Blocks() as demo:\n    gr.Markdown(\"\u0633\u0644\u0627\u0645\", rtl=True)\ndemo.launch()\n```\n\n- The `get_api_info` method of `Blocks` now supports layout output components [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4871](https://github.com/gradio-app/gradio/pull/4871)\n\n- Added the support for the new command `gradio environment`to make it easier for people to file bug reports if we shipped an easy command to list the OS, gradio version, and versions of gradio/gradio-client dependencies. bu [@varshneydevansh](https://github.com/varshneydevansh) in [PR 4915](https://github.com/gradio-app/gradio/pull/4915).\n\n### Bug Fixes:", "Using Gradio Blocks Like Functions\n\nTags: TRANSLATION, HUB, SPACES\n\n**Prerequisite**: This Guide builds on the Blocks Introduction. Make sure to [read that guide first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).\n\n## Introduction\n\nDid you know that apart from being a full-stack machine learning demo, a Gradio Blocks app is also a regular-old python function!?\n\nThis means that if you have a gradio Blocks (or Interface) app called `demo`, you can use `demo` like you would any python function.\n\nSo doing something like `output = demo(\"Hello\", \"friend\")` will run the first event defined in `demo` on the inputs \"Hello\" and \"friend\" and store it\nin the variable `output`.\n\nIf I put you to sleep \ud83e\udd71, please bear with me! By using apps like functions, you can seamlessly compose Gradio apps.\nThe following section will show how.\n\n## Treating Blocks like functions\n\nLet's say we have the following demo that translates english text to german text.", "### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution in [PR 1819](https://github.com/gradio-app/gradio/pull/1819)\n\n## 3.0\n\n###### \ud83d\udd25 Gradio 3.0 is the biggest update to the library, ever.\n\n### New Features:\n\n###### 1. Blocks \ud83e\uddf1\n\nBlocks is a new, low-level API that allows you to have full control over the data flows and layout of your application. It allows you to build very complex, multi-step applications. For example, you might want to:\n\n- Group together related demos as multiple tabs in one web app\n- Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a Dropdown) or its visibility based on user input\n\nHere's a simple example that creates the demo below it:", "### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution in [PR 1819](https://github.com/gradio-app/gradio/pull/1819)\n\n## 3.0\n\n###### \ud83d\udd25 Gradio 3.0 is the biggest update to the library, ever.\n\n### New Features:\n\n###### 1. Blocks \ud83e\uddf1\n\nBlocks is a new, low-level API that allows you to have full control over the data flows and layout of your application. It allows you to build very complex, multi-step applications. For example, you might want to:\n\n- Group together related demos as multiple tabs in one web app\n- Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a Dropdown) or its visibility based on user input\n\nHere's a simple example that creates the demo below it:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by mentioning the full control over data flows and layout, and it expands on the reference answer by providing additional details such as the ability to group related demos, change layouts, and update component properties. These details are in line with the capabilities of the `gradio.Blocks` API and provide a comprehensive understanding of its purpose."}, {"question": "What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n", "true_answer": "The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md", "generated_answer": "The question asks about the purpose of a specific paper, \"Hierarchical Text-Conditional Image Generation with CLIP Latents.\" The paper proposes a two-stage model for text-conditional image generation using CLIP latents. The first stage involves generating a low-resolution image using a text encoder and a CLIP latent encoder. The second stage involves refining the low-resolution image using a high-resolution image decoder. The purpose of", "retrieved_docs": ["-->\n\n# CLIP\n\n## Overview\n\nThe CLIP model was proposed in [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. CLIP\n(Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be\ninstructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing\nfor the task, similarly to the zero-shot capabilities of GPT-2 and 3.\n\nThe abstract from the paper is the following:", ". Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference. Project Page: [this https URL](https://latent-consistency-models.github.io/)*", ". Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference. Project Page: [this https URL](https://latent-consistency-models.github.io/).*", "This model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/openai/CLIP).\n\n## Usage tips and example\n\nCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text\nfeatures. Both the text and visual features are then projected to a latent space with identical dimension. The dot\nproduct between the projected image and text features is then used as a similar score.", "-->\n\n# CLIPSeg\n\n## Overview\n\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo L\u00fcddecke\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero- and one-shot image segmentation.\n\nThe abstract from the paper is the following:", "However, I'm a little bit vary about making this public straightaway. Looking at the model card for the CLIP model we can look at the primary intended uses:\n\n> ### Primary intended uses\n>\n> We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n> [source](https://huggingface.co/openai/clip-vit-base-patch32)\n\nThis is fairly close to what we are interested in here. Particularly we might be interested in how well the model deals with the kinds of images in our dataset (illustrations from mostly 19th century books). The images in our dataset are (probably) fairly different from the training data. The fact that some of the images also contain text might help CLIP since it displays some [OCR ability](https://openai.com/blog/clip/).\n\nHowever, looking at the out-of-scope use cases in the model card:", "-->\n\n# Pix2Struct\n\n## Overview\n\nThe Pix2Struct model was proposed in [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.\n\nThe abstract from the paper is the following:"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response provided is somewhat correct and factual as it correctly identifies the two-stage model and its components, such as the text encoder, CLIP latent encoder, and high-resolution image decoder. However, the response is incomplete and does not fully explain the purpose of the two-stage model. It fails to mention the generation of a CLIP image embedding from a text caption and the conditioning of the image generation on this embedding, which are key elements of the purpose as described in the reference answer. Therefore, the response is only partially aligned with the reference answer."}, {"question": "What command is used to install the requirements for a research project using \ud83e\udd17 Transformers?\n", "true_answer": "pip install -r requirements.txt", "source_doc": "huggingface/transformers/blob/main/examples/research_projects/README.md", "generated_answer": "The command to install the requirements for a research project using \ud83e\udd17 Transformers is:\n\n```bash\npip install -r /examples/<your-example-folder>/requirements.txt\n```\n\nThis command should be run after cloning the repository and navigating to the specific example folder. It will install all the necessary dependencies required for training the model in that specific research project.", "retrieved_docs": ["## Installation\n\nTo install via [NPM](https://www.npmjs.com/package/@xenova/transformers), run:\n```bash\nnpm i @xenova/transformers\n```\n\nFor more information, including how to use it in vanilla JS (without any bundler) via a CDN or static hosting, refer to the [README](https://github.com/xenova/transformers.js/blob/main/README.md#installation).\n\n\n## Additional resources\n\n* Transformers.js [repository](https://github.com/xenova/transformers.js)\n* Transformers.js [docs](https://huggingface.co/docs/transformers.js)\n* Transformers.js [demo](https://xenova.github.io/transformers.js/)", "If any of the mentioned aspects above are **not** clear to you, now is a great time to talk to Patrick.\n\n### 2. Next prepare your environment\n\n1.  Fork the [repository](https://github.com/huggingface/transformers)\n    by clicking on the 'Fork' button on the repository's page. This\n    creates a copy of the code under your GitHub user account.\n\n2.  Clone your `transformers` fork to your local disk, and add the base\n    repository as a remote:\n\n    ```bash\n    git clone https://github.com/[your Github handle]/transformers.git\n    cd transformers\n    git remote add upstream https://github.com/huggingface/transformers.git\n    ```\n\n3.  Set up a development environment, for instance by running the\n    following command:\n\n    ```bash\n    python -m venv .env\n    source .env/bin/activate\n    pip install -e \".[dev]\"\n    ```\n\nand return to the parent directory\n\n```bash\ncd ..\n```", "Both official training and research examples consist of a directory that contains one or more training scripts, a requirements.txt file, and a README.md file. In order for the user to make use of the\ntraining examples, it is required to clone the repository:\n\n```bash\ngit clone https://github.com/huggingface/diffusers\n```\n\nas well as to install all additional dependencies required for training:\n\n```bash\npip install -r /examples/<your-example-folder>/requirements.txt\n```\n\nTherefore when adding an example, the `requirements.txt` file shall define all pip dependencies required for your training example so that once all those are installed, the user can run the example's training script. See, for example, the [DreamBooth `requirements.txt` file](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/requirements.txt).", "Both official training and research examples consist of a directory that contains one or more training scripts, a requirements.txt file, and a README.md file. In order for the user to make use of the\ntraining examples, it is required to clone the repository:\n\n```bash\ngit clone https://github.com/huggingface/diffusers\n```\n\nas well as to install all additional dependencies required for training:\n\n```bash\npip install -r /examples/<your-example-folder>/requirements.txt\n```\n\nTherefore when adding an example, the `requirements.txt` file shall define all pip dependencies required for your training example so that once all those are installed, the user can run the example's training script. See, for example, the [DreamBooth `requirements.txt` file](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/requirements.txt).", "## Add new model like command\n\nUsing the `transformers-cli add-new-model-like` command requires to have all the `dev` dependencies installed. Let's\nfirst clone the repository and install it in our environment:\n\n```shell script\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install -e \".[dev]\"\n```\n\nDepending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\nfailure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n(PyTorch, TensorFlow and/or Flax) then do:\n\n```bash\npip install -e \".[quality]\"\n```\n\nOnce the installation is done, you can use the CLI command `add-new-model-like` to generate your models:\n\n```shell script\ntransformers-cli add-new-model-like\n```\n\nThis will start a small questionnaire you have to fill.\n\n```\nWhat identifier would you like to use for the model type of this model?\n```", "Then switch your current clone of \ud83e\udd17 Transformers to a specific version, like v3.5.1 for example:\n\n```bash\ngit checkout tags/v3.5.1\n```\n\nAfter you've setup the correct library version, navigate to the example folder of your choice and install the example specific requirements:\n\n```bash\npip install -r requirements.txt\n```\n\n## Run a script\n\n<frameworkcontent>\n<pt>\nThe example script downloads and preprocesses a dataset from the \ud83e\udd17 [Datasets](https://huggingface.co/docs/datasets/) library. Then the script fine-tunes a dataset with the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) on an architecture that supports summarization. The following example shows how to fine-tune [T5-small](https://huggingface.co/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset. The T5 model requires an additional `source_prefix` argument due to how it was trained. This prompt lets T5 know this is a summarization task.", "To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice.\n\nIf you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides a command that is indeed used to install requirements from a requirements.txt file, which is correct. However, the response specifies a path to an example folder within the \ud83e\udd17 Transformers repository, which is not universally applicable and assumes the user is working with a specific example. The reference answer provides a more general command that would be correct in a wider range of scenarios. The response is mostly correct but includes unnecessary specificity that could be misleading if the user is not working within the examples directory of the \ud83e\udd17 Transformers repository."}, {"question": "What task does the `roberta-large-mnli` checkpoint perform?\n", "true_answer": "Text classification", "source_doc": "huggingface/course/blob/main/chapters/en/chapter1/10.mdx", "generated_answer": "The `roberta-large-mnli` checkpoint performs the task of natural language inference, specifically on the MNLI (Multi-Genre Natural Language Inference) dataset. This is indicated by the fact that the checkpoint is labeled as being for the MNLI task in the Hugging Face Hub, where it is available for download. The MNLI dataset consists of over 1 million sentence pairs with a labeled relationship between them (entailment, contradiction", "retrieved_docs": ["There are total **3 checkpoints** available in **\ud83e\udd17Hub** (at the point of writing this article): [`bigbird-roberta-base`](https://huggingface.co/google/bigbird-roberta-base), [`bigbird-roberta-large`](https://huggingface.co/google/bigbird-roberta-large), [`bigbird-base-trivia-itc`](https://huggingface.co/google/bigbird-base-trivia-itc). The first two checkpoints come from pretraining `BigBirdForPretraining` with `masked_lm loss`; while the last one corresponds to the checkpoint after finetuning `BigBirdForQuestionAnswering` on `trivia-qa` dataset.\n\nLet's have a look at minimal code you can write (in case you like to use your PyTorch trainer), to use \ud83e\udd17's BigBird model for fine-tuning your tasks.\n\n```python\n# let's consider our task to be question-answering as an example\n\nfrom transformers import BigBirdForQuestionAnswering, BigBirdTokenizer\nimport torch\n\ndevice = torch.device(\"cpu\")\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")", "| Checkpoint                                                      | Task          | UNet Model Size | Total Model Size | Training Data / h |\n|-----------------------------------------------------------------|---------------|-----------------|------------------|-------------------|\n| [audioldm2](https://huggingface.co/cvssp/audioldm2)             | Text-to-audio | 350M            | 1.1B             | 1150k             |\n| [audioldm2-large](https://huggingface.co/cvssp/audioldm2-large) | Text-to-audio | 750M            | 1.5B             | 1150k             |\n| [audioldm2-music](https://huggingface.co/cvssp/audioldm2-music) | Text-to-music | 350M            | 1.1B             | 665k              |\n\n### Constructing a prompt", "### Performance\n\nWe get the following results for [roberta-base](https://huggingface.co/roberta-base) and [roberta-large](https://huggingface.co/roberta-large)\nmixed precision training(fp16) on sst2 dataset under PyTorch and ONNX Runtime backends. A single Nvidia A100 card was used to run the\nexperiment for 3 epochs::\n\n| Model           | Backend      | Runtime(s) | Train samples(/s) |\n| --------------- |------------- | ---------- | ----------------- |\n| roberta-base    | PyTorch      | 752.3      | 268.6             |\n| roberta-base    | ONNX Runtime | 729.7      | 276.9             |\n| roberta-large   | PyTorch      | 3523.7     | 57.3              |\n| roberta-large   | ONNX Runtime | 2986.6     | 67.7              |\n\nWe observe the gain of ONNX Runtime compared to PyTorch as follow:\n\n| Model         | Latency | Throughput |\n| ------------- | ------- | ---------- |\n| roberta-base  | 2.99%   | 3.08%      |\n| roberta-large | 15.24%  | 17.98%     |", "Then, we set the training arguments:\n\n```python\nfrom transformers import TrainingArguments\n\nlr = 1e-4\nbatch_size = 8\nnum_epochs = 5\n\ntraining_args = TrainingArguments(\n    output_dir=\"roberta-large-lora-token-classification\",\n    learning_rate=lr,\n    lr_scheduler_type= \"constant\",\n    warmup_ratio= 0.1,\n    max_grad_norm= 0.3,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.001,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"wandb\",\n    fp16=False,\n    gradient_checkpointing=True,\n)\n```\n\nFinally, we define the RoBERTa trainer by providing the model, the training arguments and the tokenized datasets:", "to implement BigBird.\n- The first goal should be to successfully run a forward pass using the RoBERTa checkpoint `bigbr_base/model.ckpt-0.data-00000-of-00001` and `bigbr_base/model.ckpt-0.index`.", "!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingBanner\n    chapter={1}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood.\n\nFirst, though, let's test what you learned in this chapter!\n\n\n### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?", "At the time of writing, two Bark checkpoints are available, a [smaller](https://huggingface.co/suno/bark-small) and a [larger](https://huggingface.co/suno/bark) version.\n\n\n## Load the Model and its Processor\n\nThe pre-trained Bark small and large checkpoints can be loaded from the [pre-trained weights](https://huggingface.co/suno/bark) on the Hugging Face Hub. You can change the repo-id with the checkpoint size that you wish to use.\n\nWe'll default to the small checkpoint, to keep it fast. But you can try the large checkpoint by using `\"suno/bark\"` instead of `\"suno/bark-small\"`.\n\n```python\nfrom transformers import BarkModel\n\nmodel = BarkModel.from_pretrained(\"suno/bark-small\")\n```\n\nPlace the model to an accelerator device to get the most of the optimization techniques:\n\n```python\nimport torch\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\n```\n\nLoad the processor, which will take care of tokenization and optional speaker embeddings."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response accurately identifies the task performed by the `roberta-large-mnli` checkpoint as natural language inference on the MNLI dataset. It provides specific details about the dataset and the nature of the task, which aligns with the reference answer's indication of text classification, as natural language inference is a form of text classification. The response is factual and correct based on the reference answer and the context of the task."}, {"question": "What service is replacing the Paid tier of the Inference API at Hugging Face?\n", "true_answer": "Inference Endpoints", "source_doc": "huggingface/blog/blob/main/pricing-update.md", "generated_answer": "The service replacing the Paid tier of the Inference API at Hugging Face is the Hugging Face [Inference Endpoints](https://huggingface.co/pricing). This new service allows for the deployment of machine learning models directly from the Hugging Face hub to managed infrastructure on popular clouds with just a few clicks. It offers simple, secure, and scalable solutions, and is included in the Hugging Face PRO subscription, which also provides exclusive API endpoints for", "retrieved_docs": ["## Demos with the Hugging Face Inference API\n\nHugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production. Gradio integrates directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:\n\n```python\nimport gradio as gr\n\ndemo = gr.load(\"Helsinki-NLP/opus-mt-en-es\", src=\"models\")\n\ndemo.launch()\n```\n\nFor any Hugging Face model supported in the Inference API, Gradio automatically infers the expected input and output and make the underlying server calls, so you don't have to worry about defining the prediction function.", "We strive to fix this problem with the newly launched Hugging Face [Inference Endpoints](https://huggingface.co/inference-endpoints). In the spirit of making machine learning ever simpler without compromising on state-of-the-art quality, we've built a service that lets you deploy machine learning models directly from the [Hugging Face hub](https://huggingface.co) to managed infrastructure on your favorite cloud in just a few clicks. Simple, secure, and scalable: you can have it all.\n\nLet me show you how this works!\n\n### Deploying a model on Inference Endpoints", "It couldn't be simpler. Just log in to the Hugging Face [hub](https://huggingface.co/) and browse our [models](https://huggingface.co/models). Once you've found one that you like, you can try the Inference Widget directly on the page. Clicking on the \"Deploy\" button, you'll get auto-generated code to deploy the model on the free Inference API for evaluation, and a direct link to deploy it to production with Inference Endpoints or Spaces.\n\nPlease give it a try and let us know what you think. We'd love to read your feedback on the Hugging Face [forum](https://discuss.huggingface.co/).\n\nThank you for reading!", "## Is there programmatic access to the Inference API?\n\nYes, the `huggingface_hub` library has a client wrapper documented [here](https://huggingface.co/docs/huggingface_hub/how-to-inference).", "---\n\n## \ud83e\udd17 Accelerated Inference API\n\nThe [Accelerated Inference API](https://huggingface.co/inference-api) is our hosted service to run inference on any of the 10,000+ models publicly available on the \ud83e\udd17 Model Hub, or your own private models, via simple API calls. The API includes acceleration on CPU and GPU with [up to 100x speedup](https://huggingface.co/blog/accelerated-inference) compared to out of the box deployment of Transformers.\n\nTo integrate Few-Shot Learning predictions with `GPT-Neo` in your own apps, you can use the \ud83e\udd17 Accelerated Inference API with the code snippet below. You can find your API Token [here](https://huggingface.co/settings/token), if you don't have an account you can get started [here](https://huggingface.co/pricing).\n\n```python\nimport json\nimport requests\n\nAPI_TOKEN = \"\"", "These services can be called with the [`InferenceClient`] object. It acts as a replacement for the legacy\n[`InferenceApi`] client, adding specific support for tasks and handling inference on both\n[Inference API](https://huggingface.co/docs/api-inference/index) and [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index).\nLearn how to migrate to the new client in the [Legacy InferenceAPI client](#legacy-inferenceapi-client) section.\n\n<Tip>\n\n[`InferenceClient`] is a Python client making HTTP calls to our APIs. If you want to make the HTTP calls directly using\nyour preferred tool (curl, postman,...), please refer to the [Inference API](https://huggingface.co/docs/api-inference/index)\nor to the [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) documentation pages.", "Hugging Face PRO users now have access to exclusive API endpoints for a curated list of powerful models that benefit from ultra-fast inference powered by [text-generation-inference](https://github.com/huggingface/text-generation-inference). This is a benefit on top of the free inference API, which is available to all Hugging Face users to facilitate testing and prototyping on 200,000+ models. PRO users enjoy higher rate limits on these models, as well as exclusive access to some of the best models available today.\n\n## Contents"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the service replacing the Paid tier of the Inference API at Hugging Face as the \"Hugging Face Inference Endpoints.\" The additional information provided about the deployment of machine learning models and the inclusion in the Hugging Face PRO subscription is accurate and relevant, although not required by the reference answer. The response is factual and aligns with the reference answer."}, {"question": "What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n", "true_answer": "Grouped convolutions", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/squeezebert.md", "generated_answer": "SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers, as described in the paper \"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?\" by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. This architectural difference allows for faster computation and improved efficiency, as demonstrated in the paper's", "retrieved_docs": ["-->\n\n# SqueezeBERT\n\n## Overview\n\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers.\n\nThe abstract from the paper is the following:", "techniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\nset. The SqueezeBERT code will be released.*", "This model was contributed by [forresti](https://huggingface.co/forresti).\n\n## Usage tips\n\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n  rather than the left.\n- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\n  with a causal language modeling (CLM) objective are better in that regard.\n- For best results when finetuning on sequence classification tasks, it is recommended to start with the\n  *squeezebert/squeezebert-mnli-headless* checkpoint.\n\n## Resources", "1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\n1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.", "1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\n1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.", "| Format                  | Safe | Zero-copy | Lazy loading | No file size limit | Layout control | Flexibility | Bfloat16/Fp8\n| ----------------------- | --- | --- | --- | --- | --- | --- | --- |\n| pickle (PyTorch)        | \u2717 | \u2717 | \u2717 | \ud83d\uddf8 | \u2717 | \ud83d\uddf8 | \ud83d\uddf8 |\n| H5 (Tensorflow)         | \ud83d\uddf8 | \u2717 | \ud83d\uddf8 | \ud83d\uddf8 | ~ | ~ | \u2717 |\n| SavedModel (Tensorflow) | \ud83d\uddf8 | \u2717 | \u2717 | \ud83d\uddf8 | \ud83d\uddf8 | \u2717 | \ud83d\uddf8 |\n| MsgPack (flax)          | \ud83d\uddf8 | \ud83d\uddf8 | \u2717 | \ud83d\uddf8 | \u2717 | \u2717 | \ud83d\uddf8 |\n| Protobuf (ONNX)         | \ud83d\uddf8 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | \ud83d\uddf8 |\n| Cap'n'Proto             | \ud83d\uddf8 | \ud83d\uddf8 | ~ | \ud83d\uddf8 | \ud83d\uddf8 | ~ | \u2717 |\n| Arrow                   | ? | ? | ? | ? | ? | ? | \u2717 |\n| Numpy (npy,npz)         | \ud83d\uddf8 | ? | ? | \u2717 | \ud83d\uddf8 | \u2717 | \u2717 |\n| pdparams (Paddle)       | \u2717 | \u2717 | \u2717 | \ud83d\uddf8 | \u2717 | \ud83d\uddf8 | \ud83d\uddf8 |\n| SafeTensors             | \ud83d\uddf8 | \ud83d\uddf8 | \ud83d\uddf8 | \ud83d\uddf8 | \ud83d\uddf8 | \u2717 | \ud83d\uddf8 |", "| Format                  | Safe | Zero-copy | Lazy loading | No file size limit | Layout control | Flexibility | Bfloat16/Fp8\n| ----------------------- | --- | --- | --- | --- | --- | --- | --- |\n| pickle (PyTorch)        | \u2717 | \u2717 | \u2717 | \ud83d\uddf8 | \u2717 | \ud83d\uddf8 | \ud83d\uddf8 |\n| H5 (Tensorflow)         | \ud83d\uddf8 | \u2717 | \ud83d\uddf8 | \ud83d\uddf8 | ~ | ~ | \u2717 |\n| SavedModel (Tensorflow) | \ud83d\uddf8 | \u2717 | \u2717 | \ud83d\uddf8 | \ud83d\uddf8 | \u2717 | \ud83d\uddf8 |\n| MsgPack (flax)          | \ud83d\uddf8 | \ud83d\uddf8 | \u2717 | \ud83d\uddf8 | \u2717 | \u2717 | \ud83d\uddf8 |\n| Protobuf (ONNX)         | \ud83d\uddf8 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | \ud83d\uddf8 |\n| Cap'n'Proto             | \ud83d\uddf8 | \ud83d\uddf8 | ~ | \ud83d\uddf8 | \ud83d\uddf8 | ~ | \u2717 |\n| Arrow                   | ? | ? | ? | ? | ? | ? | \u2717 |\n| Numpy (npy,npz)         | \ud83d\uddf8 | ? | ? | \u2717 | \ud83d\uddf8 | \u2717 | \u2717 |\n| pdparams (Paddle)       | \u2717 | \u2717 | \u2717 | \ud83d\uddf8 | \u2717 | \ud83d\uddf8 | \ud83d\uddf8 |\n| SafeTensors             | \ud83d\uddf8 | \ud83d\uddf8 | \ud83d\uddf8 | \ud83d\uddf8 | \ud83d\uddf8 | \u2717 | \ud83d\uddf8 |"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies that SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. The information provided is accurate and factual, aligning with the reference answer. The response also includes additional context from the paper, which supports the correctness of the answer."}, {"question": "What type of license is the HuggingFace Team's software distributed under?\n", "true_answer": "Apache License, Version 2.0", "source_doc": "huggingface/simulate/blob/main/docs/source/tutorials/interaction.mdx", "generated_answer": "The HuggingFace Team's software is distributed under the Apache License, Version 2.0, as stated in the copyright notices provided in the extracted documents. The license allows for use of the software except in compliance with the license terms, and it is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. It is recommended to review the specific language governing permissions and limitations under the license, which can be found at http://www.", "retrieved_docs": ["!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Normalized Configurations", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ControlNet", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Philosophy", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Semantic Guidance"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the HuggingFace Team's software as being distributed under the Apache License, Version 2.0. This matches the reference answer, which also states the Apache License, Version 2.0. The additional details provided in the response about the terms of the license and the recommendation to review the specific language do not detract from the accuracy of the answer regarding the type of license. Therefore, the response is completely correct, accurate, and factual."}, {"question": "What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n", "true_answer": "Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md", "generated_answer": "The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:\n\n1. Splitting the embedding matrix into two smaller matrices: In ALBERT, the embedding matrix, which is large due to its context-independent nature, is split into two smaller matrices. This reduces the number of parameters in the model, leading to lower memory consumption and faster training.\n\n2. Using repeating layers split among groups: ALB", "retrieved_docs": ["-->\n\n# ALBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<a href=\"https://huggingface.co/models?filter=albert\">\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-albert-blueviolet\">\n</a>\n<a href=\"https://huggingface.co/spaces/docs-demos/albert-base-v2\">\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n</a>\n</div>\n\n## Overview\n\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nRadu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n\n- Splitting the embedding matrix into two smaller matrices.\n- Using repeating layers split among groups.\n\nThe abstract from the paper is the following:", "So, if you reduce the precision, you reduce the memory each model parameter takes in storage, therefore reducing the model size! This also means that you reduce... the actual precision of the computations, which can reduce the model's performance. However, we found out that on bigger models, this performance degradation is actually very [limited](https://huggingface.co/blog/overview-quantization-transformers).\n\nTo go back to our above example, our 30B parameters model in `float16` requires a bit less than 66G of RAM, in `8bit` it only requires half that, so 33G of RAM, and it `4bit` we reach even half of this, so around 16G of RAM, making it considerably more accessible.", "* Paper: [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054). The paper is very interesting, but it's very terse.\n* Here is a good [video discussion](https://www.youtube.com/watch?v=tC01FRB0M7w) of the paper with visuals\n* Paper: [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840). Just published - this one goes into the details of ZeRO Offload feature.\n* DeepSpeed [configuration and tutorials](https://www.deepspeed.ai/getting-started/)\n* In addition to the paper, I highly recommend to read the following detailed blog posts with diagrams:\n  - [DeepSpeed: Extreme-scale model training for everyone]( https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)", "- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\n  than the left.\n- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\n  similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\n  number of (repeating) layers.\n- Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters.\n- Layers are split in groups that share parameters (to save memory).", "1. model weights\n2. optimizer states\n3. gradients\n4. forward activations saved for gradient computation\n5. temporary buffers\n6. functionality-specific memory\n\nA typical model trained in mixed precision with AdamW requires 18 bytes per model parameter plus activation memory. For \ninference there are no optimizer states and gradients, so we can subtract those. And thus we end up with 6 bytes per \nmodel parameter for mixed precision inference, plus activation memory.\n\nLet's look at the details.\n\n**Model Weights:**\n\n- 4 bytes * number of parameters for fp32 training\n- 6 bytes * number of parameters for mixed precision training (maintains a model in fp32 and one in fp16 in memory)\n\n**Optimizer States:**", "| Method/tool                                                | Improves training speed | Optimizes memory utilization |\n|:-----------------------------------------------------------|:------------------------|:-----------------------------|\n| [Batch size choice](#batch-size-choice)                    | Yes                     | Yes                          |\n| [Gradient accumulation](#gradient-accumulation)            | No                      | Yes                          |\n| [Gradient checkpointing](#gradient-checkpointing)          | No                      | Yes                          |\n| [Mixed precision training](#mixed-precision-training)      | Yes                     | (No)                         |\n| [Optimizer choice](#optimizer-choice)                      | Yes                     | Yes                          |\n| [Data preloading](#data-preloading)                        | Yes                     | No                           |", "## Efficient training strategies\n\nEven training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer) in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB` just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn\u2019t train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you\u2019ll run out sooner or later.\n\nAnother option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies the first technique as \"splitting the embedding matrix into two smaller matrices,\" which aligns with the reference answer. However, the second technique is incomplete and does not provide the full information. The response starts to mention \"using repeating layers split among groups\" but does not finish the explanation. Based on the score rubric, the response is mostly correct, as it accurately describes the first technique but fails to fully articulate the second technique."}, {"question": "What are the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library?\n", "true_answer": "1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter5/1.mdx", "generated_answer": "The three main steps for fine-tuning a model with the \ufffd\ufffd\ufffdg Datasets library are:\n\n1. Load a dataset from the Hugging Face Hub using the `Dataset.load_from_hub()` function.\n2. Preprocess the data using `Dataset.map()` to apply transformations and tokenization.\n3. Load and compute metrics using `Dataset.map()` again with a `DatasetDict` containing the preprocessed data and", "retrieved_docs": ["Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={5}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nIn [Chapter 3](/course/chapter3) you got your first taste of the \ud83e\udd17 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n\n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics.\n\nBut this is just scratching the surface of what \ud83e\udd17 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\n* What do you do when your dataset is not on the Hub?\n* How can you slice and dice a dataset? (And what if you _really_ need to use Pandas?)\n* What do you do when your dataset is huge and will melt your laptop's RAM?\n* What the heck are \"memory mapping\" and Apache Arrow?\n* How can you create your own dataset and push it to the Hub?", "FrameworkSwitchCourse {fw} />\n\n# Fine-tuning, Check![[fine-tuning-check]]\n\n<CourseFloatingBanner\n    chapter={3}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThat was fun! In the first two chapters you learned about models and tokenizers, and now you know how to fine-tune them for your own data. To recap, in this chapter you:\n\n{#if fw === 'pt'}\n* Learned about datasets in the [Hub](https://huggingface.co/datasets)\n* Learned how to load and preprocess datasets, including using dynamic padding and collators\n* Implemented your own fine-tuning and evaluation of a model\n* Implemented a lower-level training loop\n* Used \ud83e\udd17 Accelerate to easily adapt your training loop so it works for multiple GPUs or TPUs\n\n{:else}\n* Learned about datasets in the [Hub](https://huggingface.co/datasets)\n* Learned how to load and preprocess datasets\n* Learned how to fine-tune and evaluate a model with Keras\n* Implemented a custom metric\n\n{/if}", "## Table of Contents\n\n1. [Introduction](#introduction)\n2. [Fine-tuning Whisper in a Google Colab](#fine-tuning-whisper-in-a-google-colab)\n    1. [Prepare Environment](#prepare-environment)\n    2. [Load Dataset](#load-dataset)\n    3. [Prepare Feature Extractor, Tokenizer and Data](#prepare-feature-extractor-tokenizer-and-data)\n    4. [Training and Evaluation](#training-and-evaluation)\n    5. [Building a Demo](#building-a-demo)\n3. [Closing Remarks](#closing-remarks)\n\n## Introduction", "With the dataset ready, you can now set up the model for fine-tuning. \n\n## Load a base model\n\nLoad the [\"microsoft/git-base\"](https://huggingface.co/microsoft/git-base) into a [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM) object.\n\n\n```python\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n```\n\n## Evaluate\n\nImage captioning models are typically evaluated with the [Rouge Score](https://huggingface.co/spaces/evaluate-metric/rouge) or [Word Error Rate](https://huggingface.co/spaces/evaluate-metric/wer). For this guide, you will use the Word Error Rate (WER). \n\nWe use the \ud83e\udd17 Evaluate library to do so. For potential limitations and other gotchas of the WER, refer to [this guide](https://huggingface.co/spaces/evaluate-metric/wer). \n\n\n```python\nfrom evaluate import load\nimport torch\n\nwer = load(\"wer\")", "This concludes the introduction to fine-tuning using the `Trainer` API. An example of doing this for most common NLP tasks will be given in [Chapter 7](/course/chapter7), but for now let's look at how to do the same thing in pure PyTorch.\n\n<Tip>\n\n\u270f\ufe0f **Try it out!** Fine-tune a model on the GLUE SST-2 dataset, using the data processing you did in section 2.\n\n</Tip>", "## Preprocess the dataset\n\nSince the dataset has two modalities (image and text), the pre-processing pipeline will preprocess images and the captions.\n\nTo do so, load the processor class associated with the model you are about to fine-tune. \n\n```python\nfrom transformers import AutoProcessor\n\ncheckpoint = \"microsoft/git-base\"\nprocessor = AutoProcessor.from_pretrained(checkpoint)\n```\n\nThe processor will internally pre-process the image (which includes resizing, and pixel scaling) and tokenize the caption. \n\n```python\ndef transforms(example_batch):\n    images = [x for x in example_batch[\"image\"]]\n    captions = [x for x in example_batch[\"text\"]]\n    inputs = processor(images=images, text=captions, padding=\"max_length\")\n    inputs.update({\"labels\": inputs[\"input_ids\"]})\n    return inputs\n\n\ntrain_ds.set_transform(transforms)\ntest_ds.set_transform(transforms)\n```\n\nWith the dataset ready, you can now set up the model for fine-tuning. \n\n## Load a base model", "You can follow the [timm recipe scripts](../scripts) for training a new model afresh.\n\n## Citation\n\n```BibTeX\n@article{DBLP:journals/corr/HuangLW16a,\n  author    = {Gao Huang and\n               Zhuang Liu and\n               Kilian Q. Weinberger},\n  title     = {Densely Connected Convolutional Networks},\n  journal   = {CoRR},\n  volume    = {abs/1608.06993},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1608.06993},\n  archivePrefix = {arXiv},\n  eprint    = {1608.06993},\n  timestamp = {Mon, 10 Sep 2018 15:49:32 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/HuangLW16a.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n```\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/rwightman/pytorch-image-models}}\n}\n```"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and aligns well with the reference answer. The first step correctly identifies the need to load a dataset from the Hugging Face Hub, although it specifies a method `Dataset.load_from_hub()` which is not mentioned in the reference answer. The second step is accurate in stating that data should be preprocessed using `Dataset.map()` to apply transformations and tokenization. The third step is somewhat vague and incomplete, as it mentions loading and computing metrics with `Dataset.map()` but does not clearly state that this is a separate step from preprocessing, nor does it clarify that the metrics are computed after the model has been fine-tuned. The response could be improved by clearly distinguishing the steps and providing a more precise description of the third step. However, it does not introduce any incorrect or non-factual information and stays on topic with the task of fine-tuning a model using the \ud83e\udd17 Datasets library."}, {"question": "What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n", "true_answer": "+800%", "source_doc": "huggingface/blog/blob/main/infinity-cpu-performance.md", "generated_answer": "According to the given context, Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers. (Source: https://huggingface.co/blog/infinity-performance-intel-xeon-ice-lake)", "retrieved_docs": ["The main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware.\n\nWith [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n\n\n## What is Hugging Face Infinity", "---\n\n## Conclusion\n\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput.\n\nHugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n\nThe flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. \n\nIf you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n\n\n## Resources", "At Hugging Face, we are on a mission to democratize state-of-the-art Machine Learning, and a critical part of our work is to make these state-of-the-art models as efficient as possible, to use less energy and memory at scale, and to be more affordable to run by companies of all sizes. \n\nOur collaboration with Intel through the \ud83e\udd17 [Hardware Partner Program](https://huggingface.co/hardware) enables us to make advanced efficiency and optimization techniques easily available to the community, through our new \ud83e\udd17 [Optimum open source library](https://github.com/huggingface/optimum) dedicated to production performance.\n\nFor companies looking to accelerate their Transformer models inference, our new \ud83e\udd17 [Infinity product offers a plug-and-play containerized solution](https://huggingface.co/infinity), achieving down to 1ms latency on GPU and 2ms on Intel Xeon Ice Lake CPUs.", "If you are interested in optimizing your models to run with maximum efficiency, check out the\u00a0[\ud83e\udd17 Optimum library](https://github.com/huggingface/optimum).\n\n---\n\nThanks for reading! If you have any questions, feel free to contact me, through\u00a0[Github](https://github.com/huggingface/transformers), or on the\u00a0[forum](https://discuss.huggingface.co/c/optimum/59). You can also connect with me on\u00a0[Twitter](https://twitter.com/_philschmid)\u00a0or\u00a0[LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).\n </html>", "Leveraging the latest features of the Hugging Face libraries, we achieve a reliable 10x speed up compared to an out-of-box deployment for a given model/hardware pair. As new releases of Transformers and Tokenizers typically ship every month, our API customers do not need to constantly adapt to new optimization opportunities, their models just keep running faster.\n\n## Compilation FTW: the hard to get 10x\nNow this is where it gets really tricky. In order to get the best possible performance we will need to modify the model and compile it targeting the specific hardware for inference. The choice of hardware itself will depend on both the model (size in memory) and the demand profile (request batching). Even when serving predictions from the same model, some API customers may benefit more from Accelerated CPU inference, and others from Accelerated GPU inference, each with different optimization techniques and libraries applied.", "## Conclusion\n\nAs you can see, training Hugging Face transformers on a cluster of Intel Xeon CPUs is a flexible, scalable, and cost-effective solution, especially if you're working with small or medium-sized models and datasets.\n\nHere are some additional resources to help you get started:\n\n* [Intel IPEX](https://github.com/intel/intel-extension-for-pytorch) on GitHub\n* Hugging Face documentation: \"[Efficient training on CPU](https://huggingface.co/docs/transformers/perf_train_cpu)\" and \"[Efficient training on many CPUs](https://huggingface.co/docs/transformers/perf_train_cpu_many)\".\n\nIf you have questions or feedback, we'd love to read them on the [Hugging Face forum](https://discuss.huggingface.co/).\n\nThanks for reading!", "After completing the training of BLOOM-176B, we at HuggingFace and BigScience were looking for ways to make this big model easier to run on less GPUs. Through our BigScience community we were made aware of research on Int8 inference that does not degrade predictive performance of large models and reduces the memory footprint of large models by a factor or 2x. Soon we started collaboring on this research which ended with a full integration into Hugging Face `transformers`. With this blog post, we offer LLM.int8() integration for all Hugging Face models which we explain in more detail below. If you want to read more about our research, you can read our paper, [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339).\n\nThis article focuses on giving a high-level overview of this quantization technology, outlining the difficulties in incorporating it into the `transformers` library, and drawing up the long-term goals of this partnership."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer by stating the maximum improvement in throughput as 800%, which is the same as the +800% indicated in the reference answer. Additionally, the response cites a source from Hugging Face's official blog, which adds to the factual accuracy of the claim."}, {"question": "What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n", "true_answer": "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl", "source_doc": "huggingface/blog/blob/main/spacy.md", "generated_answer": "The command to upload a spaCy pipeline to the Hugging Face Hub is `python -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]`. This command is part of the `spacy-huggingface-hub` library, which can be installed with `pip install spacy-huggingface-hub`. After packaging the pipeline with `spacy", "retrieved_docs": ["You can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`.\n\n```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n\nThe command will output two things:\n\n* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en_core_web_sm\n* And how to install the pipeline directly from the Hub!\n\n\n### From a Python script", "## Sharing your models\n\n### Using the spaCy CLI (recommended)\n\nThe `spacy-huggingface-hub` library extends `spaCy` native CLI so people can easily push their packaged models to the Hub.\n\nYou can install spacy-huggingface-hub from pip:\n\n```bash\npip install spacy-huggingface-hub\n```\n\nYou can then check if the command has been registered successfully\n\n```bash\npython -m spacy huggingface-hub --help\n```\n\nTo push with the CLI, you can use the `huggingface-hub push` command as seen below.\n\n```bash\npython -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n```", "Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n\n### Finding models\n\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\n\n\n### Widgets", "All spaCy canonical pipelines can now be found in the official spaCy [organization](https://huggingface.co/spacy), and any user can share their pipelines with a single command `python -m spacy huggingface-hub`. To read more about it, head to [https://huggingface.co/blog/spacy](https://huggingface.co/blog/spacy). You can try all canonical spaCy models directly in the Hub in the demo [Space](https://huggingface.co/spaces/spacy/pipeline-visualizer)!\n\n![spaCy NER example](assets/27_summer_at_huggingface/spacy_ner.jpeg)\n\nAnother exciting integration is Sentence Transformers. You can read more about it in the [blog announcement](https://huggingface.co/blog/sentence-transformers-in-the-hub): you can find over 200 [models](https://huggingface.co/models?library=sentence-transformers) in the Hub, easily share your models with the rest of the community and reuse models from the community.", "All `spaCy` models from the Hub can be directly installed using pip install.\n\n```bash\npip install https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl\n```\n\nTo find the link of interest, you can go to a repository with a `spaCy` model. When you open the repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!", "he Push to Hub API. Let's have a look at the push_to_hub API. You will need to be logged in with your Hugging Face account, which you can do by executing this first cell or typing huggingface-cli login in a terminal. Just enter your username and password and click login, which will store an authentication token in the cache of the machine you're using. Now, let's launch the fine-tuning of a BERT model on the GLUE COLA dataset. We won't go over the fine-tuning code because you can find it in any Transformers tutorial, or by looking at the videos linked below. What interests us here, is how we can leverage the Model Hub during training. This is done with the push_to_hub=True passed in your TrainingArguments . This will automatically upload your model to the Hub each time it is saved (so every epoch in our case), which allows you to resume training from a different machine if the current one gets interrupted", "And for larger-scale use cases, you can click \"Deploy > Accelerated Inference\" and see how to do this with Python.\n\n\n### Sharing your models\n\nBut probably the coolest feature is that now you can very easily share your models with the `spacy-huggingface-hub` [library](https://github.com/explosion/spacy-huggingface-hub), which extends the `spaCy` CLI with a new command, `huggingface-hub push`. \n\n```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n\nTry it out and share your models with the community!\n\n## Would you like to integrate your library to the Hub?"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided includes the correct command structure for uploading a spaCy pipeline to the Hugging Face Hub, which aligns with the reference answer. However, the response includes additional flags and parameters that are not present in the reference answer, which could imply a more general usage rather than a specific command example. Despite this, the core command is accurate and factual, indicating that the response is mostly correct."}, {"question": "What is the time and memory complexity of the Nystr\u00f6mformer's approximation of self-attention?\n", "true_answer": "O(n)", "source_doc": "huggingface/blog/blob/main/nystromformer.md", "generated_answer": "The Nystr\u00f6mformer's approximation of self-attention has a time and memory complexity of linear order, specifically O(n), where n is the length of the input sequence. This is in contrast to the standard self-attention mechanism, which has a time and memory complexity of quadratic order, specifically O(n^2). The linear complexity is achieved by using the Nystr\u00f6m method to approximate the self-attention matrix by sampling landmarks from queries and keys", "retrieved_docs": ["--\ntitle: \"Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\"\nthumbnail: /blog/assets/86_nystromformer/thumbnail.png\nauthors:\n- user: asi\n  guest: true\n---\n\n# Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n## Introduction\n\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences.", "### [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768)\n\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n\nThe goal is to reduce the complexity of the self-attention with respect to the sequence length \\\\(n\\\\)) from quadratic to linear. This paper makes the observation that the attention matrices are low rank (i.e. they don\u2019t contain \\\\(n \u00d7 n\\\\) worth of information) and explores the possibility of using high-dimensional data compression techniques to build more memory efficient transformers.", "The [Nystr\u00f6mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nystr\u00f6mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nystr\u00f6m method and how it can be adapted to approximate self-attention.\n\n\n## Nystr\u00f6m method for matrix approximation\n\nAt the heart of Nystr\u00f6mformer is the Nystr\u00f6m method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nystr\u00f6m method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:", "-->\n\n# Nystr\u00f6mformer\n\n## Overview\n\nThe Nystr\u00f6mformer model was proposed in [*Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention*](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn\nFung, Yin Li, and Vikas Singh.\n\nThe abstract from the paper is the following:", "## How can we adapt the Nystr\u00f6m method to approximate self-attention?\n\nInstead of sampling from \\\\(S\\\\), the authors propose to sample landmarks (or Nystr\u00f6m points) from queries and keys. We denote the query landmarks and key landmarks as \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) respectively. \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) can be used to construct three matrices corresponding to those in the Nystr\u00f6m approximation of \\\\(S\\\\). We define the following matrices:\n\n$$\\tilde{F} = softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) \\hspace{40pt} \\tilde{A} = softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+ \\hspace{40pt} \\tilde{B} = softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}})$$\n\nThe sizes of \\\\(\\tilde{F}\\\\), \\\\(\\tilde{A}\\\\), and \\\\(\\tilde{B}) are \\\\(n \\times m, m \\times m,\\\\) and \\\\(m \\times n\\\\) respectively. \nWe replace the three matrices in the Nystr\u00f6m approximation of \\\\(S\\\\) with the new matrices we have defined to obtain an alternative Nystr\u00f6m approximation:", "The four papers above offer different ways to deal with the quadratic memory complexity of the self-attention mechanism, usually by reducing it to linear complexity. Linformer and Longformer both rely on the observation that the self-attention matrix does not contain \\\\(n \u00d7 n\\\\) worth of information (the attention matrix is low-rank and sparse). Performer gives a principled method to approximate the softmax-attention kernel (and any kernelizable attention mechanisms beyond softmax). Compressive Transformer offers an orthogonal approach to model long range dependencies based on recurrence.", "$$\\text{Attention}(Q, K, V) \\sim \\phi(Q) * (\\phi(K) * V)$$\n\nTaking inspiration from machine learning papers from the early 2000s, the authors introduce **FAVOR+** (**F**ast **A**ttention **V**ia **O**rthogonal **R**andom positive (**+**) **F**eatures) a procedure to find unbiased or nearly-unbiased estimations of the self-attention matrix, with uniform convergence and low estimation variance.\n\n#### Main findings"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies that the Nystr\u00f6mformer's approximation of self-attention has a linear time and memory complexity, which aligns with the reference answer of O(n). The explanation provided is accurate and factual, detailing the contrast with the standard self-attention mechanism and explaining how the linear complexity is achieved through the Nystr\u00f6m method. The response goes beyond the reference answer by providing additional context, which is not penalized as the core information requested is correct and factual."}, {"question": "What is the goal of the Named Entity Recognition task in token classification?\n", "true_answer": "The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/tasks.md", "generated_answer": "The goal of the Named Entity Recognition task in token classification is to find and label the entities (such as persons, locations, or organizations) in a piece of text. This is achieved by attributing a label to each token in the input text, with one class for each entity and another class for tokens that have no entity.", "retrieved_docs": ["named entity recognition (WikiAnn) to low-resource tasks (Americas NLI, MasakhaNER).*", "Welcome to the Hugging Face tasks series! In this video we\u2019ll take a look at the token classification task.\nToken classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\nLet\u2019s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.\nAnother token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech.", "In particular, we can see the dataset contains labels for the three tasks we mentioned earlier: NER, POS, and chunking. A big difference from other datasets is that the input texts are not presented as sentences or documents, but lists of words (the last column is called `tokens`, but it contains words in the sense that these are pre-tokenized inputs that still need to go through the tokenizer for subword tokenization).\n\nLet's have a look at the first element of the training set:\n\n```py\nraw_datasets[\"train\"][0][\"tokens\"]\n```\n\n```python out\n['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n```\n\nSince we want to perform named entity recognition, we will look at the NER tags:\n\n```py\nraw_datasets[\"train\"][0][\"ner_tags\"]\n```\n\n```python out\n[3, 0, 7, 0, 0, 0, 7, 0, 0]\n```", "{/if}\n\nThe first application we'll explore is token classification. This generic task encompasses any problem that can be formulated as \"attributing a label to each token in a sentence,\" such as:\n\n- **Named entity recognition (NER)**: Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \"no entity.\"\n- **Part-of-speech tagging (POS)**: Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.).\n- **Chunking**: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually `B-`) to any tokens that are at the beginning of a chunk, another label (usually `I-`) to tokens that are inside a chunk, and a third label (usually `O`) to tokens that don't belong to any chunk.\n\n<Youtube id=\"wVHdVlPScxA\"/>", "To evaluate the performance of sahajBERT, we finetuned it on two downstream tasks in Bengali:\n\n-   Named entity recognition (NER) on the Bengali split of [WikiANN](https://aclanthology.org/P17-1178/). The goal of this task is to classify each token in the input text into one of the following categories: person, organization, location, or none of them.\n-   News Category Classification (NCC) on the Soham articles dataset from [IndicGLUE](https://aclanthology.org/2020.findings-emnlp.445/). The goal of this task is to predict the category to which belong the input text.\n\nWe evaluated it during training on the NER task to check that everything was going well; as you can see on the following plot, this was indeed the case!", "### Token classification\n\nIn any NLP task, text is preprocessed by separating the sequence of text into individual words or subwords. These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. \n\nTwo common types of token classification are:\n\n* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names.\n* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb).\n\n```py\n>>> from transformers import pipeline", "WNUT\u201917 is a very difficult task. Current state-of-the-art results on this dataset can be found [here](https://nlpprogress.com/english/named_entity_recognition.html)."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by stating the goal of the Named Entity Recognition task and expands on it by explaining the process of attributing labels to tokens, which adds depth to the understanding of the task without deviating from the correctness of the answer."}, {"question": "What is the resolution of images used by the CLIPSeg model?\n", "true_answer": "352 x 352 pixels", "source_doc": "huggingface/blog/blob/main/clipseg-zero-shot.md", "generated_answer": "The resolution of images used by the CLIPSeg model is 352 x 352 pixels, as stated in the provided context.", "retrieved_docs": ["Currently, CLIPSeg still has its limitations. For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg). Before we describe how to do that, let\u2019s first take a look at how CLIPSeg works.\n\n## CLIP: the magic model behind CLIPSeg", "## CLIPSeg: image segmentation with CLIP\n\n[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo L\u00fcddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn\u2019t only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP.", "asset = segments_client.upload_asset(f, \"clipseg_prediction.png\")\n\nattributes = {\n      'format_version': '0.1',\n      'annotations': [{\"id\": i, \"category_id\": i} for i in unique_inds if i != 0],\n      'segmentation_bitmap': { 'url': asset.url },\n  }\n\nsegments_client.add_label(sample.uuid, 'ground-truth', attributes)\n```\n\nIf you take a look at the [uploaded prediction on\nSegments.ai](https://segments.ai/admin-tobias/clipseg/samples/71a80d39-8cf3-4768-a097-e81e0b677517/ground-truth),\nyou can see that it\\'s not perfect. However, you can manually correct\nthe biggest mistakes, and then you can use the corrected dataset to\ntrain a better model than CLIPSeg.\n\n<figure class=\"image table text-center m-0 w-9/12\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Thumbnails of the final segmentation labels on Segments.ai.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/segments-thumbs.png\"></medium-zoom>\n</figure>\n\n## Conclusion", "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clipseg-overview.png\"></medium-zoom>\n  <figcaption><a href=\"https://arxiv.org/abs/2112.10003\">Source</a></figcaption>\n</figure>\n\nThe decoder is trained on the [PhraseCut dataset](https://arxiv.org/abs/2008.01187), which contains over 340,000 phrases with corresponding image segmentation masks. The authors also experimented with various augmentations to expand the size of the dataset. The goal here is not only to be able to segment the categories that are present in the dataset, but also to segment unseen categories. Experiments indeed show that the decoder can generalize to unseen categories.", "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png\"\nalt=\"drawing\" width=\"600\"/> \n\n<small> CLIPSeg overview. Taken from the <a href=\"https://arxiv.org/abs/2112.10003\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/timojl/clipseg).\n\n## Usage tips\n\n- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latter is identical to [`CLIPModel`].\n- [`CLIPSegForImageSegmentation`] can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text\n(provided to the model as `input_ids`) or an image (provided to the model as `conditional_pixel_values`). One can also provide custom\nconditional embeddings (provided to the model as `conditional_embeddings`).\n\n## Resources", "## Conclusion\n\nCLIPSeg is a zero-shot segmentation model that works with both text and image prompts. The model adds a decoder to CLIP and can segment almost anything. However, the output segmentation masks are still very low-res for now, so you\u2019ll probably still want to fine-tune a different segmentation model if accuracy is important. \n\nNote that there's more research on zero-shot segmentation currently being conducted, so you can expect more models to be added in the near future. One example is [GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit), which is already available in \ud83e\udd17 Transformers. To stay up to date with the latest news in segmentation research, you can follow us on Twitter: [@TobiasCornille](https://twitter.com/tobiascornille), [@NielsRogge](https://twitter.com/nielsrogge), and [@huggingface](https://twitter.com/huggingface).", "-->\n\n# CLIPSeg\n\n## Overview\n\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo L\u00fcddecke\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero- and one-shot image segmentation.\n\nThe abstract from the paper is the following:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, stating the resolution of images used by the CLIPSeg model as 352 x 352 pixels."}, {"question": "What can you use Gradio for?\n", "true_answer": "Create a demo for your machine learning model, share your machine learning model with others, and debug your model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/9.mdx", "generated_answer": "Answer: Gradio can be used to quickly build demos or web applications for any Python function, not just machine learning models or NLP. However, Gradio is primarily designed for NLP models and supplies developers with a library of pre-built components for various data types. (False statement is correct)\n\nExplanation:\n\nWhile Gradio can be used to build demos or web applications for any Python function, the documentation and examples provided by Gradio heavily focus on", "retrieved_docs": ["Quickstart\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/lcm-screenshot-3.gif\" style=\"padding-bottom: 10px\">\n\nIt just takes a few lines of Python to create a beautiful demo like the one above, so let's get started \ud83d\udcab\n\n## Installation\n\n**Prerequisite**: Gradio requires [Python 3.8 or higher](https://www.python.org/downloads/)\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install gradio\n```", "gradio\n\n## 4.11.0\n\n### Features", "gradio\n\n## 4.11.0\n\n### Features", ">> \"This is a test of the whisper speech recognition model.\"\n```\n\nThe Gradio client works with any hosted Gradio app, whether it be an image generator, a text summarizer, a stateful chatbot, a tax calculator, or anything else! The Gradio Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), but your app can be hosted anywhere, such as your own server.\n\n**Prerequisites**: To use the Gradio client, you do _not_ need to know the `gradio` library in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components.\n\n## Installation\n\nIf you already have a recent version of `gradio`, then the `gradio_client` is included as a dependency.\n\nOtherwise, the lightweight `gradio_client` package can be installed from pip (or pip3) and is tested to work with Python versions 3.9 or higher:\n\n```bash\n$ pip install gradio_client\n```\n\n## Connecting to a running Gradio App", "We will explore all of these concepts below.\n\n### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\n\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook.\n\n```py\nimport gradio as gr\n\n\ndef flip_text(x):\n    return x[::-1]\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n        \"\"\"\n    # Flip Text!\n    Start typing below to see the output.\n    \"\"\"\n    )\n    input = gr.Textbox(placeholder=\"Flip this text\")\n    output = gr.Textbox()\n\n    input.change(fn=flip_text, inputs=input, outputs=output)\n\ndemo.launch()\n```", "> [!TIP]\n > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:", "### 4. Gradio is designed primarily for NLP models\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"True\",\n\t\t\texplain: \"Gradio works with pretty much any data type, not just NLP.\"\n        },\n        {\n\t\t\ttext: \"False\",\n\t\t\texplain: \"Gradio supplies developers with a library of pre-built components for pretty much all data types.\",\n            correct: true\n        }\n\t]}\n/>\n\n### 5. Which of the following features are supported by Gradio?"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response correctly identifies that Gradio can be used to build demos or web applications for Python functions, which aligns with the reference answer's mention of creating demos for machine learning models. However, the response inaccurately states that Gradio is primarily designed for NLP models and supplies developers with a library of pre-built components for various data types. While Gradio does provide components that can be used for NLP, it is not limited to NLP and is designed for a wide range of applications, including but not limited to machine learning models. The statement that Gradio is primarily designed for NLP models is not supported by the reference answer or Gradio's actual scope of use. Therefore, the response is somewhat correct but includes an inaccuracy regarding Gradio's primary design focus."}, {"question": "What TensorFlow API function is used to load a saved tensor file?\n", "true_answer": "safetensors.tensorflow.load_file", "source_doc": "huggingface/safetensors/blob/main/docs/source/api/tensorflow.mdx", "generated_answer": "The TensorFlow API functions `safetensors.tensorflow.load_file` and `safetensors.tensorflow.load` can be used to load a saved tensor file in TensorFlow. These functions are provided by the SafeTensors library, which is a lightweight library for loading and saving TensorFlow models in a memory-efficient and safe way. The `load_file` function loads a tensor file from a file path, while the `load` function loads a", "retrieved_docs": ["Tensorflow API\n\n[[autodoc]] safetensors.tensorflow.load_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save_file\n[[autodoc]] safetensors.tensorflow.save", "return serving_fn\n```\n\nYou can first derive the [concrete function](https://www.tensorflow.org/guide/function)\nfrom the model's forward pass method (`call()`) so the model is nicely compiled\ninto a graph. After that, you can apply the following steps in order:\n\n1. Pass the inputs through the preprocessing operations.\n\n2. Pass the preprocessing inputs through the derived concrete function.\n\n3. Post-process the outputs and return them in a nicely formatted\n   dictionary.\n\nNow it's time to export the model!\n\n```py\nMODEL_DIR = tempfile.gettempdir()\nVERSION = 1\n\ntf.saved_model.save(\n    model,\n    os.path.join(MODEL_DIR, str(VERSION)),\n    signatures={\"serving_default\": model_exporter(model)},\n)\nos.environ[\"MODEL_DIR\"] = MODEL_DIR\n```\n\nAfter exporting, let's inspect the model signatures again:\n\n```bash\nsaved_model_cli show --dir {MODEL_DIR}/1 --tag_set serve --signature_def serving_default\n```", "## Make a prediction request\n\n\n```python\n# Generate sample data. \nimport base64\n\nimage_path = tf.keras.utils.get_file(\n    \"image.jpg\", \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n)\nbytes = tf.io.read_file(image_path)\nb64str = base64.b64encode(bytes.numpy()).decode(\"utf-8\")\n```\n\n\n```python\n# Model input signature key name.\npushed_model_location = os.path.join(GCS_BUCKET, LOCAL_MODEL_DIR)\nloaded = tf.saved_model.load(pushed_model_location)\nserving_input = list(\n    loaded.signatures[\"serving_default\"].structured_input_signature[1].keys()\n)[0]\nprint(\"Serving function input:\", serving_input)\n```\n\n\n```python\nfrom google.protobuf import json_format\nfrom google.protobuf.struct_pb2 import Value\n\n\ndef predict_image(image, endpoint, serving_input):\n    # The format of each instance should conform to the deployed model's prediction input schema.\n    instances_list = [{serving_input: {\"b64\": image}}]\n    instances = [json_format.ParseDict(s, Value()) for s in instances_list]", "- Safe: Can I use a file randomly downloaded and expect not to run arbitrary code ?\n- Zero-copy: Does reading the file require more memory than the original file ?\n- Lazy loading: Can I inspect the file without loading everything ? And loading only\nsome tensors in it without scanning the whole file (distributed setting) ?\n- Layout control: Lazy loading, is not necessarily enough since if the information about tensors is spread out in your file, then even if the information is lazily accessible you might have to access most of your file to read the available tensors (incurring many DISK -> RAM copies). Controlling the layout to keep fast access to single tensors is important.\n- No file size limit: Is there a limit to the file size ?\n- Flexibility: Can I save custom code in the format and be able to use it later with zero extra code ? (~ means we can store more than pure tensors, but no custom code)", "- Safe: Can I use a file randomly downloaded and expect not to run arbitrary code ?\n- Zero-copy: Does reading the file require more memory than the original file ?\n- Lazy loading: Can I inspect the file without loading everything ? And loading only\nsome tensors in it without scanning the whole file (distributed setting) ?\n- Layout control: Lazy loading, is not necessarily enough since if the information about tensors is spread out in your file, then even if the information is lazily accessible you might have to access most of your file to read the available tensors (incurring many DISK -> RAM copies). Controlling the layout to keep fast access to single tensors is important.\n- No file size limit: Is there a limit to the file size ?\n- Flexibility: Can I save custom code in the format and be able to use it later with zero extra code ? (~ means we can store more than pure tensors, but no custom code)", "# return the formated output\n        return self.serving_output(output)\n\n# Instantiate the model with the new serving method\nmodel = MyOwnModel.from_pretrained(\"bert-base-cased\")\n# save it with saved_model=True in order to have a SavedModel version along with the h5 weights.\nmodel.save_pretrained(\"my_model\", saved_model=True)\n```\n\nThe serving method has to be overridden by the new `input_signature` argument of the `tf.function` decorator. See the [official documentation](https://www.tensorflow.org/api_docs/python/tf/function#args_1) to know more about the `input_signature` argument. The `serving` method is used to define how will behave a SavedModel when deployed with TensorFlow Serving. Now the SavedModel looks like as expected, see the new `inputs_embeds` input:", "```bash\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs['string_input'] tensor_info:\n      dtype: DT_STRING\n      shape: (-1)\n      name: serving_default_string_input:0\nThe given SavedModel SignatureDef contains the following output(s):\n  outputs['confidence'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1)\n      name: StatefulPartitionedCall:0\n  outputs['label'] tensor_info:\n      dtype: DT_STRING\n      shape: (-1)\n      name: StatefulPartitionedCall:1\nMethod name is: tensorflow/serving/predict\n```\n\nYou can notice that the model's signature has now changed. Specifically,\nthe input type is now a string and the model returns two things: a\nconfidence score and the string label.\n\nProvided you've already installed TF Serving (covered in the Colab\nNotebook), you're now ready to deploy this model!\n\n# Deployment with TensorFlow Serving\n\nIt just takes a single command to do this:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies `safetensors.tensorflow.load_file` as a function used to load a saved tensor file in TensorFlow, which matches the reference answer. However, the response also includes additional information about the `safetensors.tensorflow.load` function and the SafeTensors library, which was not required by the instruction. The instruction asked for a specific TensorFlow API function, and the response provided that along with extra context. Based on the rubric, the response is mostly correct, accurate, and factual, but the additional information was unnecessary for the specific question asked."}, {"question": "Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n", "true_answer": "In the \"Logs\" tab of your Endpoint through the UI.", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx", "generated_answer": "The logs of your Endpoints in Hugging Face Endpoints can be accessed through the \"Logs\" tab in the UI of your Endpoint. This feature is provided by Hugging Face Endpoints and allows you to view both the build logs of your Image artifacts and the Container Logs during inference. If your Endpoint creation is in the \"Failed\" state, you can also check the Build Logs to see the reason for failure, such as wrong version of a dependency. (", "retrieved_docs": ["Access and read Logs\n\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the \u201cLogs\u201d tab of your Endpoint. \n\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\n\nThe Container Logs are only available when your Endpoint is in the \u201cRunning\u201d state. \n\n_Note: If your Endpoint creation is in the \u201cFailed\u201d state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\n\n**Build Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\n\n**Container Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png\" alt=\"container logs\" />", "Access and view Metrics\n\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the \u201cAnalytics\u201d tab of your Endpoints. \n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png\" alt=\"metric dashboard\" />\n\n## Access Metrics via API\n\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.", "```py\n>>> from huggingface_hub import get_inference_endpoint, list_inference_endpoints\n\n# Get one\n>>> get_inference_endpoint(\"my-endpoint-name\")\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='pending', url=None)\n\n# List all endpoints from an organization\n>>> list_inference_endpoints(namespace=\"huggingface\")\n[InferenceEndpoint(name='aws-starchat-beta', namespace='huggingface', repository='HuggingFaceH4/starchat-beta', status='paused', url=None), ...]\n\n# List all endpoints from all organizations the user belongs to\n>>> list_inference_endpoints(namespace=\"*\")\n[InferenceEndpoint(name='aws-starchat-beta', namespace='huggingface', repository='HuggingFaceH4/starchat-beta', status='paused', url=None), ...]\n```\n\n## Check deployment status", "## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /webhook: Add, update or remove a dataset\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset", "The base URL for the HTTP endpoints above is `https://huggingface.co`.\n\nThose endpoints are not officially supported in `huggingface_hub` or `huggingface.js` yet but [this code snippet](https://github.com/huggingface/huggingface_hub/issues/1535#issuecomment-1614693412) (in Python) might help you getting started.\n\n### Download access report\n\nYou can download a report of all access requests for a gated datasets with the **download user access report** button. Click on it to download a json file with a list of users. For each entry, you have:\n- **user**: the user id. Example: *julien-c*.\n- **fullname**: name of the user on the Hub. Example: *Julien Chaumond*.\n- **status**: status of the request. Either `\"pending\"`, `\"accepted\"` or `\"rejected\"`.\n- **email**: email of the user.\n- **time**: datetime when the user initially made the request.\n\n<a id=\"modifying-the-prompt\"></a> <!-- backward compatible anchor -->\n\n### Customize requested information", "## What is Hugging Face Inference Endpoints\n\n[Hugging Face Inference Endpoints](https://ui.endpoints.huggingface.co/) offers an easy and secure way to deploy Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, and offering advanced security. \n\nHere are some of the most important features for LLM deployment:", "Those API utilities are also exposed through the `huggingface-cli` CLI:\n\n```bash\nhuggingface-cli login\nhuggingface-cli logout\nhuggingface-cli whoami\nhuggingface-cli repo create\n```\n\nWith the `HfApi` class there are methods to query models, datasets, and metrics by specific tags (e.g. if you want to list models compatible with your library):\n- **Models**:\n  - `list_models()`\n  - `model_info()`\n  - `get_model_tags()`\n- **Datasets**:\n  - `list_datasets()`\n  - `dataset_info()`\n  - `get_dataset_tags()`\n- **Spaces**:\n  - `list_spaces()`\n  - `space_info()`\n\nThese lightly wrap around the API Endpoints. Documentation for valid parameters and descriptions can be found [here](https://huggingface.co/docs/hub/endpoints).\n  \n\n### Advanced programmatic repository management \n\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by stating that the logs can be accessed through the \"Logs\" tab in the UI of the Endpoint. Additionally, the response goes into further detail by explaining the types of logs available, such as build logs and container logs, and the utility of these logs, like troubleshooting a failed Endpoint creation. This additional information supports the correctness and factual nature of the response without deviating from the accuracy required by the task."}, {"question": "What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n", "true_answer": "Image Classification", "source_doc": "huggingface/blog/blob/main/autotrain-image-classification.md", "generated_answer": "The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification, as announced in the \"\ud83e\udd17 AutoTrain\" section of the provided context. This allows users to train state-of-the-art models for image classification using AutoTrain's \"no-code\" solution, with more tasks on the way.", "retrieved_docs": ["[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with **zero** configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer \ud83e\udd2f We\u2019ve been expanding the number of tasks that we support, and we\u2019re proud to announce that **you can now use AutoTrain for Computer Vision**! Image Classification is the latest task we\u2019ve added, with more on the way. But what does this mean for you?", "## \ud83e\udd17 AutoTrain\n\n[AutoTrain](https://huggingface.co/autotrain) provides a \u201cno-code\u201d solution to train state-of-the-art Machine Learning models for tasks like text classification, text summarization, named entity recognition, and more. For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage.\n\nAutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate \ud83e\udd17 Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details.\n\n## The technical philosophy", "Here is a list of things we\u2019ll cover:\n\n- [Supported vision tasks and Pipelines](#support-for-pipelines)\n- [Training your own vision models](#training-your-own-models)\n- [Integration with `timm`](#\ud83e\udd17-\ud83e\udd1d-timm)\n- [Diffusers](#\ud83e\udde8-diffusers)\n- [Support for third-party libraries](#support-for-third-party-libraries)\n- [Deployment](#deployment)\n- and much more!\n\n## Enabling the community: One task at a time \ud83d\udc41\n\nThe Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:\n\n- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- Depth estimation\n- Image-to-image synthesis\n- Unconditional image generation\n- Zero-shot image classification", "--\ntitle: The State of Computer Vision at Hugging Face \ud83e\udd17\nthumbnail: /blog/assets/cv_state/thumbnail.png\nauthors:\n- user: sayakpaul\n---\n\n# The State of Computer Vision at Hugging Face \ud83e\udd17\n\n\nAt Hugging Face, we pride ourselves on democratizing the field of artificial intelligence together with the community. As a part of that mission, we began focusing our efforts on computer vision over the last year. What started as a [PR for having Vision Transformers (ViT) in \ud83e\udd17 Transformers](https://github.com/huggingface/transformers/pull/10950) has now grown into something much bigger \u2013 8 core vision tasks, over 3000 models, and over 100 datasets on the Hugging Face Hub.\n\nA lot of exciting things have happened since ViTs joined the Hub. In this blog post, we\u2019ll summarize what went down and what\u2019s coming to support the continuous progress of Computer Vision from the \ud83e\udd17 ecosystem.\n\nHere is a list of things we\u2019ll cover:", "At Hugging Face, we\u2019ve been quietly working to pave the way for this inclusive future. Our suite of tools, including services like Spaces, AutoTrain, and Inference Endpoints, are designed to make the world of machine learning accessible to everyone.\n\nTo showcase just how accessible this democratized future is, this tutorial will show you how to use [Spaces](https://huggingface.co/Spaces), [AutoTrain](https://huggingface.co/autotrain) and [ChatUI](https://huggingface.co/inference-endpoints) to build the chat app. All in just three simple steps, sans a single line of code. For context I\u2019m also not an ML engineer, but a member of the Hugging Face GTM team. If I can do this then you can too! Let's dive in!\n\n## Introduction to Spaces", "In this blog post, we will discuss the past, present, and future of text-to-video models. We will start by reviewing the differences between the text-to-video and text-to-image tasks, and discuss the unique challenges of unconditional and text-conditioned video generation. Additionally, we will cover the most recent developments in text-to-video models, exploring how these methods work and what they are capable of. Finally, we will talk about what we are working on at Hugging Face to facilitate the integration and use of these models and share some cool demos and resources both on and outside of the Hugging Face Hub.\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/140_text-to-video/make-a-video.png\" alt=\"samples\"><br>\n    <em>Examples of videos generated from various text description inputs, image taken from <a href=https://arxiv.org/abs/2209.14792>Make-a-Video</a>.</em>\n</p>", "The [Hugging Face Hub](https://huggingface.co/) is a central repository where people can share and access machine learning models, datasets and demos. The Hub hosts over 190,000 machine learning models, 33,000 datasets and over 100,000 machine learning applications and demos. These models cover a wide range of tasks from pre-trained language models, text, image and audio classification models, object detection models, and a wide range of generative models. \n\nThe models, datasets and demos hosted on the Hub span a wide range of domains and languages, with regular community efforts to expand the scope of what is available via the Hub. This blog post intends to offer people working in or with the galleries, libraries, archives and museums (GLAM) sector to understand how they can use &mdash; and contribute to &mdash; the Hugging Face Hub.\n\nYou can read the whole post or jump to the most relevant sections!"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies \"Image Classification\" as the latest task added to Hugging Face AutoTrain for Computer Vision, which aligns with the reference answer. The additional context provided in the response does not detract from the accuracy of the answer to the question asked."}, {"question": "What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n", "true_answer": "model", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md", "generated_answer": "The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository, as stated in section 1 of the context provided. If you want to create a dataset repository or a Space repository, you can specify the `repo_type` parameter with the accepted values of \"dataset\" or \"space\", respectively.", "retrieved_docs": ["### `create_repo`\n\nThe `create_repo` method creates a repository on the Hub. Use the `name` parameter to provide a name for your repository:\n\n```python\n>>> from huggingface_hub import create_repo\n>>> create_repo(repo_id=\"test-model\")\n'https://huggingface.co/lysandre/test-model'\n```\n\nWhen you check your Hugging Face account, you should now see a `test-model` repository under your namespace.\n\n### `upload_file`\n\nThe `upload_file` method uploads files to the Hub. This method requires the following:\n\n- A path to the file to upload.\n- The final path in the repository.\n- The repository you wish to push the files to.\n\nFor example:\n\n```python\n>>> from huggingface_hub import upload_file\n>>> upload_file(\n...    path_or_fileobj=\"/home/lysandre/dummy-test/README.md\", \n...    path_in_repo=\"README.md\", \n...    repo_id=\"lysandre/test-model\"\n... )\n'https://huggingface.co/lysandre/test-model/blob/main/README.md'\n```", "By default, [`create_repo`] creates a model repository. But you can use the `repo_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-dataset\", repo_type=\"dataset\")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```\n\nWhen you create a repository, you can set your repository visibility with the `private` parameter.\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-private\", private=True)\n```\n\nIf you want to change the repository visibility at a later time, you can use the [`update_repo_visibility`] function.\n\n### Delete a repository\n\nDelete a repository with [`delete_repo`]. Make sure you want to delete a repository because this is an irreversible process!\n\nSpecify the `repo_id` of the repository you want to delete:\n\n```py\n>>> delete_repo(repo_id=\"lysandre/my-corrupted-dataset\", repo_type=\"dataset\")\n```", "```python\n# if the repository doesn't exist remotely on the Hugging Face Hub, it will be created when we set create_remote to True\nrepo_id = \"skops-user/my-awesome-model\"\nhub_utils.push(\n    repo_id=repo_id,\n    source=local_repo,\n    token=token,\n    commit_message=\"pushing files to the repo from the example!\",\n    create_remote=True,\n)\n```\n\nOnce we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements.\n\n```python\ndownload_repo = \"downloaded-model\"\nhub_utils.download(repo_id=repo_id, dst=download_repo)\n```\n\nThe inference widget is enabled to make predictions in the repository.\n\n![Hosted Inference Widget](assets/94_skops/skops_widget.png)\n\nIf the requirements of your project have changed, you can use `update_env` to update the environment.", "```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(\"dummy-model\")\n```\n\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n\n```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(\"dummy-model\", organization=\"huggingface\")\n```\n\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization.\nOther arguments which may be useful are:\n\n- `private`, in order to specify if the repository should be visible from others or not.\n- `token`, if you would like to override the token stored in your cache by a given token.\n- `repo_type`, if you would like to create a `dataset` or a `space` instead of a model. Accepted values are `\"dataset\"` and `\"space\"`.\n\nOnce the repository is created, we should add files to it! Jump to the next section to see the three ways this can be handled.", "## Creating a repository\n\nUsing the Hub's web interface you can easily create repositories, add files (even large ones!), explore models, visualize diffs, and much more. There are three kinds of repositories on the Hub, and in this guide you'll be creating a **model repository** for demonstration purposes. For information on creating and managing models, datasets, and Spaces, refer to their respective documentation.\n\n1. To create a new repository, visit [huggingface.co/new](http://huggingface.co/new):\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new_repo.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new_repo-dark.png\"/>\n</div>\n\n2. Specify the owner of the repository: this can be either you or any of the organizations you\u2019re affiliated with.", "```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(repo_id=\"super-cool-model\")\n```\n\n### \ud30c\uc77c \uc5c5\ub85c\ub4dc\n\n\ud30c\uc77c \ud558\ub098\uc758 \uacbd\uc6b0:\n\n```py\nfrom huggingface_hub import upload_file\n\nupload_file(\n    path_or_fileobj=\"/home/lysandre/dummy-test/README.md\",\n    path_in_repo=\"README.md\",\n    repo_id=\"lysandre/test-model\",\n)\n```\n\n\ub808\ud3ec\uc9c0\ud1a0\ub9ac \uc804\uccb4\uc758 \uacbd\uc6b0:\n\n```py\nfrom huggingface_hub import upload_folder\n\nupload_folder(\n    folder_path=\"/path/to/local/space\",\n    repo_id=\"username/my-cool-space\",\n    repo_type=\"space\",\n)\n```\n\n\uc790\uc138\ud55c \ub0b4\uc6a9\uc740 [\uc5c5\ub85c\ub4dc \uac00\uc774\ub4dc](https://huggingface.co/docs/huggingface_hub/ko/guides/upload)\ub97c \ucc38\uc870\ud558\uc138\uc694.\n\n## Hugging Face Hub\uc640 \ud568\uaed8 \uc131\uc7a5\ud558\uae30\n\n\uc800\ud76c\ub294 \uba4b\uc9c4 \uc624\ud508\uc18c\uc2a4 ML \ub77c\uc774\ube0c\ub7ec\ub9ac\ub4e4\uacfc \ud611\ub825\ud558\uc5ec, \ubaa8\ub378 \ud638\uc2a4\ud305\uacfc \ubc84\uc804 \uad00\ub9ac\ub97c \ubb34\ub8cc\ub85c \uc81c\uacf5\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ubbf8 \ud1b5\ud569\ub41c \ub77c\uc774\ube0c\ub7ec\ub9ac\ub4e4\uc740 [\uc5ec\uae30](https://huggingface.co/docs/hub/libraries)\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc774\ub807\uac8c \ud558\uba74 \ub2e4\uc74c\uacfc \uac19\uc740 \uc7a5\uc810\uc774 \uc788\uc2b5\ub2c8\ub2e4:", "## Repository: the historical git-based approach\n\nAt first, `huggingface_hub` was mostly built around the [`Repository`] class. It provides\nPython wrappers for common `git` commands such as `\"git add\"`, `\"git commit\"`, `\"git push\"`,\n`\"git tag\"`, `\"git checkout\"`, etc.\n\nThe library also helps with setting credentials and tracking large files, which are often\nused in machine learning repositories. Additionally, the library allows you to execute its\nmethods in the background, making it useful for uploading data during training."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that the default repository type is a model repository. Additionally, the response provides extra information about how to specify other types of repositories, which is factually correct and relevant to the instruction but not necessary for a full score."}, {"question": "How many splits does the \"duorc\" dataset have?\n", "true_answer": "Six", "source_doc": "huggingface/datasets-server/blob/main/docs/source/splits.mdx", "generated_answer": "The \"duorc\" dataset has 6 splits, as mentioned in the context provided. The splits are listed in the JSON response obtained by making a GET request to the Hugging Face API, and they are named \"train\", \"validation\", and \"test\" for both the \"ParaphraseRC\" and \"SelfRC\" configurations. The exact number of splits can be confirmed by checking the JSON response or by referring to the dataset's documentation.", "retrieved_docs": ["```json\n{\n    \"dataset_info\": {\n        \"description\": \"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\n\",\n        \"citation\": \"@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = {2018}\\n}\\n\",\n        \"homepage\": \"https://duorc.github.io/\",\n        \"license\": \"https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\",\n        \"features\": {\n            \"plot_id\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"plot\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"title\": {", "},\n        \"splits\": {\n            \"train\": {\n                \"name\": \"train\",\n                \"num_bytes\": 239852729,\n                \"num_examples\": 60721,\n                \"dataset_name\": \"duorc\"\n            },\n            \"validation\": {\n                \"name\": \"validation\",\n                \"num_bytes\": 51662519,\n                \"num_examples\": 12961,\n                \"dataset_name\": \"duorc\"\n            },\n            \"test\": {\n                \"name\": \"test\",\n                \"num_bytes\": 49142710,\n                \"num_examples\": 12559,\n                \"dataset_name\": \"duorc\"\n            }\n        },\n        \"download_checksums\": {\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_train.json\": {\n                \"num_bytes\": 24388192,\n                \"checksum\": null\n            },\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_dev.json\": {\n                \"num_bytes\": 5051240,\n                \"checksum\": null", "### Data Splits\n\nDescribe and name the splits in the dataset if there are more than one.\n\nDescribe any criteria for splitting the data, if used. If there are differences between the splits (e.g. if the training annotations are machine-generated and the dev and test ones are created by humans, or if different numbers of annotators contributed to each example), describe them here.\n\nProvide the sizes of each split. As appropriate, provide any descriptive statistics for the features, such as average length.  For example:\n\n|                         | train | validation | test |\n|-------------------------|------:|-----------:|-----:|\n| Input Sentences         |       |            |      |\n| Average Sentence Length |       |            |      |\n\n## Dataset Creation\n\n### Curation Rationale\n\nWhat need motivated the creation of this dataset? What are some of the reasons underlying the major choices involved in putting it together?\n\n### Source Data", "This dataset is large enough to stress the RAM of most laptops, yet we've been able to load and access it without breaking a sweat! Let's now combine the examples from the FreeLaw and PubMed Abstracts datasets with the `interleave_datasets()` function:\n\n```py\nfrom itertools import islice\nfrom datasets import interleave_datasets\n\ncombined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])\nlist(islice(combined_dataset, 2))\n```", "},\n    {\n      \"dataset\": \"duorc\",\n      \"config\": \"SelfRC\",\n      \"split\": \"test\",\n      \"url\": \"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/test/0000.parquet\",\n      \"filename\": \"0000.parquet\",\n      \"size\": 3035735\n    },\n    {\n      \"dataset\": \"duorc\",\n      \"config\": \"SelfRC\",\n      \"split\": \"train\",\n      \"url\": \"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/train/0000.parquet\",\n      \"filename\": \"0000.parquet\",\n      \"size\": 14851719\n    },\n    {\n      \"dataset\": \"duorc\",\n      \"config\": \"SelfRC\",\n      \"split\": \"validation\",\n      \"url\": \"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/validation/0000.parquet\",\n      \"filename\": \"0000.parquet\",\n      \"size\": 3114389\n    }\n  ]\n}\n```", "The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n\n```json\n{\n  \"splits\": [\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"test\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"train\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"validation\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"test\" }\n  ],\n  \"pending\": [],\n  \"failed\": []\n}\n```", "![Saving a checkpoint](assets/27_summer_at_huggingface/save_checkpoint.png)\n\n### New in Datasets\n\nYou can find 1400 public datasets in [https://huggingface.co/datasets](https://huggingface.co/datasets) thanks to the awesome contributions from all our community. \ud83d\udcaf\n\nThe support for `datasets` keeps growing: it can be used in JAX, process parquet files, use remote files, and has wider support for other domains such as Automatic Speech Recognition and Image Classification.\n\nUsers can also directly host and share their datasets to the community simply by uploading their data files in a repository on the Dataset Hub.\n\n![Untitled](assets/27_summer_at_huggingface/streaming.png)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the number of splits in the \"duorc\" dataset as 6, which aligns with the reference answer. The additional details provided about the names of the splits and the methods to confirm the number of splits demonstrate a thorough understanding and factual accuracy."}, {"question": "What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n", "true_answer": "FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.", "source_doc": "huggingface/peft/blob/main/docs/source/accelerate/fsdp.md", "generated_answer": "Fully Sharded Data Parallel (FSDP) is a data parallel method used in distributed training to improve memory efficiency and allow for larger models and batch sizes. Unlike traditional data parallel methods like DistributedDataParallel (DDP), FSDP shards a model's parameters, gradients, and optimizer states across available GPUs (workers or ranks), reducing memory usage because the model is replicated on each GPU. This improves GPU memory efficiency and allows for training of much", "retrieved_docs": ["-->\n\n# Fully Sharded Data Parallel\n\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or *rank*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class.\n\nBefore you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer.\n\n```bash\npip install accelerate\n```\n\n## FSDP configuration", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Fully Sharded Data Parallel\n\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n\n<Tip warning={true}>\n\nCurrently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates.\n\n</Tip>", "Fully Sharded Data Parallelism (FSDP) is a paradigm in which the optimizer states, gradients and parameters are sharded across devices. During the forward pass, each FSDP unit performs an _all-gather operation_ to get the complete weights, computation is performed followed by discarding the shards from other devices. After the forward pass, the loss is computed followed by the backward pass. In the backward pass, each FSDP unit performs an all-gather operation to get the complete weights, with computation performed to get the local gradients. These local gradients are averaged and sharded across the devices via a _reduce-scatter operation_ so that each device can update the parameters of its shard. For more information on what PyTorch FSDP is, please refer to this blog post: [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp).\n\n![FSDP Workflow](./assets/62_pytorch_fsdp/FSDP_workflow.png)", "Distributed training is the key to enable training such large ML models. There have been major recent advances in the field of **Distributed Training at Scale**. Few the most notable advances are given below:\n1. Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]\n    1. Stage 1: Shards optimizer states across data parallel workers/GPUs\n    2. Stage 2: Shards optimizer states + gradients across data parallel workers/GPUs\n    3. Stage 3: Shards optimizer states + gradients + model parameters across data parallel workers/GPUs\n    4. CPU Offload: Offloads the gradients + optimizer states to CPU building on top of ZERO Stage 2 [3]\n2. Tensor Parallelism [4]: Form of model parallelism wherein sharding parameters of individual layers with huge number of parameters across accelerators/GPUs is done in a clever manner to achieve parallel computation while avoiding expensive communication synchronization overheads.", "In this post we will look at Data Parallelism using ZeRO and more specifically the latest PyTorch feature **[FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.  **[DeepSpeed](https://github.com/microsoft/deepspeed)** and **[FairScale](https://github.com/facebookresearch/fairscale/)** have implemented the core ideas of the ZERO paper. These have already been integrated in `transformers` Trainer and accompanied by great blog [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale) [10]. PyTorch recently upstreamed the Fairscale FSDP into PyTorch Distributed with additional optimizations.\n\n# Accelerate \ud83d\ude80:  Leverage PyTorch FSDP without any code\u00a0changes\n\nWe will look at the task of Causal Language Modelling using GPT-2 Large (762M) and XL (1.5B) model variants.", "(Source:\u00a0[link](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/))\n\nThe above workflow gives an overview of what happens behind the scenes when FSDP is activated. Let's first understand how DDP works and how FSDP improves it. In DDP, each worker/accelerator/GPU has a replica of the entire model parameters, gradients and optimizer states. Each worker gets a different batch of data, it goes through the forwards pass, a loss is computed followed by the backward pass to generate gradients. Now, an all-reduce operation is performed wherein each worker gets the gradients from the remaining workers and averaging is done. In this way, each worker now has the same global gradients which are used by the optimizer to update the model parameters. We can see that having full replicas consume a lot of redundant memory on each GPU, which limits the batch size as well as the size of the models.", "### Addressing Challenge 2\nIt is addressed via choosing `SHARDED_STATE_DICT` state dict type when creating FSDP config. `SHARDED_STATE_DICT` saves shard per GPU separately which makes it quick to save or resume training from intermediate checkpoint. When `FULL_STATE_DICT` is used, first process (rank 0) gathers the whole model on CPU and then saving it in a standard format. \n\nLet\u2019s create the accelerate config via below command:\n```\naccelerate config --config_file \"fsdp_config.yaml\"\n```\n\n![fsdp_config](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/160_ram_efficient_fsdp/fsdp_config.jpg)"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies FSDP as a data parallel method used in distributed training to improve memory efficiency and enable larger models and batch sizes. It accurately describes the sharding of model parameters, gradients, and optimizer states across GPUs, which aligns with the reference answer's mention of sharding across data parallel processes. The response is factual and provides a clear explanation of how FSDP differs from traditional data parallel methods like DDP. However, it does not mention the capability of FSDP to handle models up to 1T parameters, which is a specific fact included in the reference answer. Therefore, the response is mostly correct, accurate, and factual, but not completely, as it lacks this specific detail.\n\nFeedback: The response is mostly correct, accurate, and factual, but it omits the specific capability of FSDP to handle models up to 1T parameters, which is mentioned in the reference answer."}, {"question": "What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n", "true_answer": "`.safetensors`", "source_doc": "huggingface/safetensors/blob/main/docs/source/convert-weights.md", "generated_answer": "The more secure file format for saving and storing PyTorch model weights is `.safetensors`. This format is recommended over saving weights as `.bin` files with Python's `pickle` utility, as it provides a more secure way to store weights. The easiest way to convert PyTorch weights to `.safetensors` is to use the Convert Space provided by Hugging Face, which downloads the pickled weights, converts them, and opens", "retrieved_docs": ["Convert weights to safetensors\n\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n\nThe easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n\n<Tip warning={true}>\n\nFor larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights.", "```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```", "### Yet another format ?\n\nThe main rationale for this crate is to remove the need to use\n`pickle` on `PyTorch` which is used by default.\nThere are other formats out there used by machine learning and more general\nformats.\n\n\nLet's take a look at alternatives and why this format is deemed interesting.\nThis is my very personal and probably biased view:", "### Yet another format ?\n\nThe main rationale for this crate is to remove the need to use\n`pickle` on `PyTorch` which is used by default.\nThere are other formats out there used by machine learning and more general\nformats.\n\n\nLet's take a look at alternatives and why this format is deemed interesting.\nThis is my very personal and probably biased view:", "## Sharding state dicts\n\nTraditionally, PyTorch models are saved in a whole file containing a map from parameter name to weight. This map is often called a `state_dict`. Here is an excerpt from the [PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html) on saving on loading:\n\n```python\n# Save the model weights\ntorch.save(my_model.state_dict(), 'model_weights.pth')\n\n# Reload them\nnew_model = ModelClass()\nnew_model.load_state_dict(torch.load('model_weights.pth'))\n```\n\nThis works pretty well for models with less than 1 billion parameters, but for larger models, this is very taxing in RAM. The BLOOM model has 176 billions parameters; even with the weights saved in bfloat16 to save space, it still represents 352GB as a whole. While the super computer that trained this model might have this amount of memory available, requiring this for inference is unrealistic.", "{#if fw === 'pt'}\nThe *pytorch_model.bin* file is known as the *state dictionary*; it contains all your model's weights. The two files go hand in hand; the configuration is necessary to know your model's architecture, while the model weights are your model's parameters.\n\n{:else}\nThe *tf_model.h5* file is known as the *state dictionary*; it contains all your model's weights. The two files go hand in hand; the configuration is necessary to know your model's architecture, while the model weights are your model's parameters.\n\n{/if}\n\n## Using a Transformer model for inference[[using-a-transformer-model-for-inference]]\n\nNow that you know how to load and save a model, let's try using it to make some predictions. Transformer models can only process numbers \u2014 numbers that the tokenizer generates. But before we discuss tokenizers, let's explore what inputs the model accepts.", "```python\nfrom torch import nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = nn.Linear(100, 100)\n        self.b = self.a\n\n    def forward(self, x):\n        return self.b(self.a(x))\n\n\nmodel = Model()\nprint(model.state_dict())\n# odict_keys(['a.weight', 'a.bias', 'b.weight', 'b.bias'])\ntorch.save(model.state_dict(), \"model.bin\")\n# This file is now 41k instead of ~80k, because A and B are the same weight hence only 1 is saved on disk with both `a` and `b` pointing to the same buffer\n```\n\n## Why are shared tensors not saved in `safetensors` ?\n\nMultiple reasons for that:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies `.safetensors` as a more secure file format for saving and storing PyTorch model weights compared to `.bin` files. The information provided is accurate and factual, aligning with the reference answer. However, the response includes additional information about using Hugging Face's Convert Space, which was not required by the instruction but does not detract from the correctness of the answer."}, {"question": "What type of security certification does Hugging Face have?\n", "true_answer": "SOC2 Type 2 certified", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/security.md", "generated_answer": "Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses. (Source: Extract 1)", "retrieved_docs": ["Security\n\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.\n\nHugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). \n\nHugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.", "[Explore existing organizations](https://huggingface.co/organizations), create a new organization [here](https://huggingface.co/organizations/new), and then visit the [**Organizations documentation**](./organizations) to learn more.\n\n## Security\n\nThe Hugging Face Hub supports security and access control features to give you the peace of mind that your code, models, and data are safe. Visit the [**Security**](./security) section in these docs to learn about:\n\n- User Access Tokens\n- Access Control for Organizations\n- Signing commits with GPG\n- Malware scanning\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">", "## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security\n\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\n## Inference Endpoint Security level\n\nWe currently offer three types of endpoints, in order or increasing security level:", "<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\nFor any other security questions, please feel free to send us an email at security@huggingface.co.\n\n## Contents\n\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)", "Single Sign-On (SSO)\n\nThe Hugging Face Hub gives you the ability to implement mandatory Single Sign-On (SSO) for your organization.\n\nWe support both SAML 2.0 and OpenID Connect (OIDC) protocols.\n\n<Tip warning={true}>\nThis feature is part of the <a href=\"https://huggingface.co/enterprise\" target=\"_blank\">Enterprise Hub</a>.\n</Tip>\n\n## How does it work?\n\nWhen Single Sign-On is enabled, the members of your organization must authenticate through your Identity Provider (IdP) to access any content under the organization's namespace. Public content will still be available to users who are not members of the organization.\n\n**We use email addresses to identify SSO users. Make sure that your organizational email address (e.g. your company email) has been added to [your user account](https://huggingface.co/settings/account).**\n\nWhen users log in, they will be prompted to complete the Single Sign-On authentication flow with a banner similar to the following:", "\ud83c\udf81 Merchandise and assets. \n\n\u2728 Being officially recognized as a Hugging Face\u2019s Ambassador\n\n<br />\n\n**Eligibility Requirements for Students**\n\n- Validate your student status \n- Have taken at least one machine learning/data science course (online courses are considered as well)\n- Be enrolled in an accredited college or university\n- Be a user of the Hugging Face Hub and/or the Hugging Face\u2019s libraries\n- Acknowledge the [Code of Conduct](https://huggingface2.notion.site/huggingface2/Hugging-Face-Code-of-Conduct-45eeeafa9ef44c5e888a2952619fdfa8). Community is at the center of the Hugging Face ecosystem. Because of that, we strictly adhere to our [Code of conduct](https://huggingface2.notion.site/huggingface2/Hugging-Face-Code-of-Conduct-45eeeafa9ef44c5e888a2952619fdfa8). If any ambassador infringes it or behaves inadequately, they will be excluded from the Program.", "We are happy to announce that Hugging Face has been selected by the [CNIL](https://www.cnil.fr/en/home) (French Data Protection Authority) to benefit from its [Enhanced Support program](https://www.cnil.fr/en/enhanced-support-cnil-selects-3-digital-companies-strong-potential)!\nThis new program picked three companies with \u201cstrong potential for economic development\u201d out of over 40 candidates, who will receive support in understanding and implementing their duties with respect to data protection - \na daunting and necessary endeavor in the context of the rapidly evolving field of Artificial Intelligence."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies Hugging Face as having SOC2 Type 2 certification, which aligns with the reference answer. The additional information provided about the certification's implications for security practices and customer assurance is accurate and relevant, though not required by the instruction. The response is factual and provides a clear answer to the question posed in the instruction."}, {"question": "What do RAG models combine to generate outputs?\n", "true_answer": "Pretrained dense retrieval (DPR) and sequence-to-sequence models.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md", "generated_answer": "RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs. They retrieve relevant context documents from an external dataset during execution and use them in conjunction with the original input to produce an output. This combination is called retrieval-augmented generation (RAG). The retriever and seq2seq modules are initialized from pretrained models and fine-tuned jointly, allowing both retrieval and generation to adapt", "retrieved_docs": ["This model was contributed by [ola13](https://huggingface.co/ola13).\n\n## Usage tips\n\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. \nRAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq \nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt \nto downstream tasks.\n\n## RagConfig\n\n[[autodoc]] RagConfig\n\n## RagTokenizer\n\n[[autodoc]] RagTokenizer\n\n## Rag specific outputs\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput\n\n## RagRetriever\n\n[[autodoc]] RagRetriever\n\n<frameworkcontent>\n<pt>\n\n## RagModel\n\n[[autodoc]] RagModel\n    - forward\n\n## RagSequenceForGeneration\n\n[[autodoc]] RagSequenceForGeneration\n    - forward\n    - generate\n\n## RagTokenForGeneration\n\n[[autodoc]] RagTokenForGeneration\n    - forward\n    - generate", "--model_type rag_sequence \\ # RAG model type (rag_token or rag_sequence)\n        --evaluation_set output/biencoder-nq-dev.questions \\ # an input dataset for evaluation\n        --gold_data_path poutput/biencoder-nq-dev.pages \\ # a dataset containing ground truth answers for samples from the evaluation_set\n        --predictions_path output/retrieval_preds.tsv  \\ # name of file where predictions will be stored\n        --eval_mode retrieval \\ # indicates whether we're performing retrieval evaluation or e2e evaluation\n        --k 1 # parameter k for the precision@k metric", "Intro\n\nAuthors: @patrickvonplaten and @lhoestq\n\nAimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn't be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time.\n\nA RAG model encapsulates two core components: a question encoder and a generator.\nDuring a forward pass, we encode the input with the question encoder and pass it\nto the retriever to extract relevant context documents. The documents are then prepended to the input.\nSuch contextualized inputs are passed to the generator.\n\nRead more about RAG  at https://arxiv.org/abs/2005.11401.\n\n# Note\n\n\u26a0\ufe0f This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\n\n# Finetuning", "python examples/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8 \\\n    --profile \\\n    --do_train \\\n    --do_predict \\\n    --n_val -1 \\\n    --train_batch_size 8 \\\n    --eval_batch_size 1 \\\n    --max_source_length 128 \\\n    --max_target_length 25 \\\n    --val_max_target_length 25 \\\n    --test_max_target_length 25 \\\n    --label_smoothing 0.1 \\\n    --dropout 0.1 \\\n    --attention_dropout 0.1 \\\n    --weight_decay 0.001 \\\n    --adam_epsilon 1e-08 \\\n    --max_grad_norm 0.1 \\\n    --lr_scheduler polynomial \\\n    --learning_rate 3e-05 \\\n    --num_train_epochs 100 \\\n    --warmup_steps 500 \\\n    --gradient_accumulation_steps 1 \\\n    --distributed_retriever ray \\\n    --num_retrieval_workers 4\n\n# Stop the Ray cluster.\nray stop\n```\n\n## What\u2019s next?", "_An overview of RAG. The model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. The  GIF is taken from [Facebook's original blog post](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models)._\n\n\n\nRecently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.com/) to introduce the [RAG](https://twitter.com/huggingface/status/1310597560906780680) model as part of its Transformers library.", "If you plan to try RAG+Ray integration out, please feel free to share your experiences on the [Ray Discourse](https://discuss.ray.io/) or join the [Ray community Slack](https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform) for further discussion -- we\u2019d love to hear from you!\n\n\n> Also published at https://medium.com/distributed-computing-with-ray/retrieval-augmented-generation-with-huggingface-transformers-and-ray-b09b56161b1e", "## Overview\n\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\noutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\nboth retrieval and generation to adapt to downstream tasks.\n\nIt is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, Douwe Kiela.\n\nThe abstract from the paper is the following:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response accurately identifies that RAG models combine pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs, which is in line with the reference answer. The additional details provided about the process of retrieving relevant context documents and the joint fine-tuning of the retriever and seq2seq modules are correct and factual, but they are not necessary to fulfill the instruction. The core of the question is addressed correctly and factually."}, {"question": "What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n", "true_answer": "Beautiful Soup", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md", "generated_answer": "The question asks about the library used by MarkupLMFeatureExtractor to extract data from HTML and XML files. The answer can be found in the context provided. According to the documentation, MarkupLMFeatureExtractor uses Beautiful Soup, a Python library for pulling data out of HTML and XML files, under the hood. Therefore, the answer to the question is Beautiful Soup.", "retrieved_docs": ["## Usage: MarkupLMProcessor\n\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks.\n\n```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\n\nfeature_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\nprocessor = MarkupLMProcessor(feature_extractor, tokenizer)\n```", "In short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\nOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.\n\n[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].", "```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> node_labels = [1, 2, 2, 1]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, node_labels=node_labels, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq', 'labels'])\n```\n\n**Use case 4: web page question answering (inference), parse_html=True**\n\nFor question answering tasks on web pages, you can provide a question to the processor. By default, the\nprocessor will use the feature extractor to get all nodes and xpaths, and create [CLS] question tokens [SEP] word tokens [SEP].\n\n```python\n>>> from transformers import MarkupLMProcessor", "```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> question = \"What's his name?\"\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, questions=question, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n## Resources\n\n- [Demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n\n## MarkupLMConfig\n\n[[autodoc]] MarkupLMConfig\n    - all\n\n## MarkupLMFeatureExtractor", "-->\n\n# MarkupLM\n\n## Overview\n\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm).\n\nThe model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)", "## MarkupLMConfig\n\n[[autodoc]] MarkupLMConfig\n    - all\n\n## MarkupLMFeatureExtractor\n\n[[autodoc]] MarkupLMFeatureExtractor\n    - __call__\n\n## MarkupLMTokenizer\n\n[[autodoc]] MarkupLMTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## MarkupLMTokenizerFast\n\n[[autodoc]] MarkupLMTokenizerFast\n    - all\n\n## MarkupLMProcessor\n\n[[autodoc]] MarkupLMProcessor\n    - __call__\n\n## MarkupLMModel\n\n[[autodoc]] MarkupLMModel\n    - forward\n\n## MarkupLMForSequenceClassification\n\n[[autodoc]] MarkupLMForSequenceClassification\n    - forward\n\n## MarkupLMForTokenClassification\n\n[[autodoc]] MarkupLMForTokenClassification\n    - forward\n\n## MarkupLMForQuestionAnswering\n\n[[autodoc]] MarkupLMForQuestionAnswering\n    - forward", "```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>My name is Niels.</p>\n...  </body>\n...  </html>\"\"\"\n\n>>> question = \"What's his name?\"\n>>> encoding = processor(html_string, questions=question, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 5: web page question answering (inference), parse_html=False**\n\nFor question answering tasks (such as WebSRC), you can provide a question to the processor. If you have extracted\nall nodes and xpaths yourself, you can provide them directly to the processor. Make sure to set `parse_html` to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that MarkupLMFeatureExtractor uses Beautiful Soup to extract data from HTML and XML files. The response adds additional context by mentioning that this information is according to the documentation, which supports the accuracy of the answer."}, {"question": "What is the file size limit for syncing to HF Spaces without using Git-LFS?\n", "true_answer": "10MB", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md", "generated_answer": "The file size limit for syncing to HF Spaces without using Git-LFS is not specified in the given context. The text mentions that for files larger than 10MB, Spaces requires Git-LFS, and if Git-LFS is not used, one may need to review files and check history using a tool like BFG Repo-Cleaner. Therefore, it can be inferred that files smaller than 10MB can be synced to", "retrieved_docs": ["9 directories, 34 files\n```\n\nNice, we reduced the *n-gram* by more than half to less than 2GB now. In\nthe final step, let's upload all files.\n\n```python\nrepo.push_to_hub(commit_message=\"Upload lm-boosted decoder\")\n```\n\n**Output:**\n```bash\n    Git LFS: (1 of 1 files) 1.85 GB / 1.85 GB\n    Counting objects: 9, done.\n    Delta compression using up to 2 threads.\n    Compressing objects: 100% (9/9), done.\n    Writing objects: 100% (9/9), 1.23 MiB | 1.92 MiB/s, done.\n    Total 9 (delta 3), reused 0 (delta 0)\n    To https://huggingface.co/hf-test/xls-r-300m-sv\n       27d0c57..5a191e2  main -> main\n```\n\nThat's it. Now you should be able to use the *5gram* for LM-boosted\ndecoding as shown in Section 1.\n\nAs can be seen on [`xls-r-300m-sv`'s model\ncard](https://huggingface.co/hf-test/xls-r-300m-sv#inference-with-lm)\nour *5gram* LM-boosted decoder yields a WER of 18.85% on Common Voice's 7\ntest set which is a relative performance of *ca.* 30% \ud83d\udd25.", "Managing Spaces with Github Actions\n\nYou can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\n\nFirst, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n\n```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main\n```\n\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:", "2. Clone your fork repository:\n\n```\ngit clone git@hf.co:me/myfork\n```\n\n3. Fetch non-LFS files:\n\n```\ncd myfork\ngit lfs install --skip-smudge --local # affects only this clone\ngit remote add upstream git@hf.co:friend/upstream\ngit fetch upstream\n```\n\n4. Fetch large files. This can take some time depending on your download bandwidth:\n\n```\ngit lfs fetch --all upstream # this can take time depending on your download bandwidth\n```\n\n4.a. If you want to completely override the fork history (which should only have an initial commit), run:\n\n```\ngit reset --hard upstream/main\n```\n\n4.b. If you want to rebase instead of overriding, run the following command and resolve any conflicts:\n\n```\ngit rebase upstream/main\n```\n\n5. Prepare your LFS files to push:\n\n```\ngit lfs install --force --local # this reinstalls the LFS hooks\nhuggingface-cli lfs-enable-largefiles . # needed if some files are bigger than 5GB\n```\n\n6. And finally push:", "Finally, create an Action that automatically checks the file size of any new pull request:\n\n\n```yaml\nname: Check file size\non:               # or directly `on: [push]` to run the action on every push on any branch\n  pull_request:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check large files\n        uses: ActionsDesk/lfs-warning@v2.0\n        with:\n          filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n```", "```bash\ngit push\n```\n\n```bash\nUploading LFS objects: 100% (1/1), 433 MB | 1.3 MB/s, done.\nEnumerating objects: 11, done.\nCounting objects: 100% (11/11), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (9/9), done.\nWriting objects: 100% (9/9), 288.27 KiB | 6.27 MiB/s, done.\nTotal 9 (delta 1), reused 0 (delta 0), pack-reused 0\nTo https://huggingface.co/lysandre/dummy\n   891b41d..b08aab1  main -> main\n```\n\n{#if fw === 'pt'}\nIf we take a look at the model repository when this is finished, we can see all the recently added files:\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/full_model.png\" alt=\"The 'Files and versions' tab now contains all the recently uploaded files.\" width=\"80%\"/>\n</div>\n\nThe UI allows you to explore the model files and commits and to see the diff introduced by each commit:", "Text file extensions are not tracked by Git LFS by default, and if they're greater than 10MB, they will not be committed and uploaded. Take a look at the `.gitattributes` file in your repository for a complete list of tracked file extensions. For this tutorial, you can use the following sample `.csv` files since they're small: <a href=\"https://huggingface.co/datasets/stevhliu/demo/raw/main/train.csv\" download>train.csv</a>, <a href=\"https://huggingface.co/datasets/stevhliu/demo/raw/main/test.csv\" download>test.csv</a>.\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/upload_files.png\"/>\n</div>\n\n2. Drag and drop your dataset files and add a brief descriptive commit message.\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/commit_files.png\"/>\n</div>", "improve the service, but one must always remember that a git repository is not meant to work as a database with a lot of\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\nfresh start using `huggingface_hub`'s [`super_squash_history`](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.HfApi.super_squash_history). Be aware that this is a non-revertible operation.\n- **Number of operations per commit**: Once again, there is no hard limit here. When a commit is uploaded on the Hub, each\ngit operation (addition or delete) is checked by the server. When a hundred LFS files are committed at once,\neach file is checked individually to ensure it's been correctly uploaded. When pushing data through HTTP,\na timeout of 60s is set on the request, meaning that if the process takes more time, an error is raised. However, it can"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies that there is a file size limit for syncing to HF Spaces without using Git-LFS and correctly infers that files smaller than 10MB can be synced without Git-LFS. However, the response does not explicitly state the 10MB limit as the reference answer does. Instead, it provides additional context regarding the use of tools like BFG Repo-Cleaner for larger files. While the additional information is related, it does not directly answer the question posed. Based on the score rubric, the response is mostly correct but lacks the conciseness and directness of the reference answer."}, {"question": "What is the title of the paper introducing the ByT5 model?\n", "true_answer": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md", "generated_answer": "The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models\" (arXiv:2105.13626).", "retrieved_docs": ["-->\n\n# ByT5\n\n## Overview\n\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel.\n\nThe abstract from the paper is the following:", "- **byT5**: byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences. Refer\n  to the documentation of byT5 which can be found [here](byt5).\n\n- **UL2**: UL2 is a T5 like model pretrained on various denoising objectives\n\n- **Flan-T5**: Flan is a pretraining methods that is based on prompting. The Flan-T5 are T5 models trained on the Flan collection of \n    datasets which include: `taskmaster2`, `djaym7/wiki_dialog`, `deepmind/code_contests`, `lambada`, `gsm8k`, `aqua_rat`, `esnli`, `quasc` and `qed`.\n\n- **FLan-UL2** : the UL2 model finetuned using the \"Flan\" prompt tuning and dataset collection.\n\n- **UMT5**: UmT5 is a multilingual T5 model trained on an improved and refreshed mC4 multilingual corpus,  29 trillion characters across 107 language, using a new sampling method, UniMax. Refer to\n the documentation of mT5 which can be found [here](umt5).\n\n## Training", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.", "This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/byt5).\n\n<Tip>\n\nByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference. They\nonly differ in how inputs should be prepared for the model, see the code examples below.\n\n</Tip>\n\nSince ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n\n\n## Usage example\n\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\n\n```python\n>>> from transformers import T5ForConditionalGeneration\n>>> import torch\n\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")", "<p>An overview of the ViT model structure as introduced in <a href=\"https://arxiv.org/abs/2010.11929\" rel=\"noopener\" target=\"_blank\">Google Research\u2019s original 2021 paper</a></p>\n</div>", "-->\n\n# UL2\n\n## Overview\n\nThe T5 model was presented in [Unifying Language Learning Paradigms](https://arxiv.org/pdf/2205.05131v1.pdf) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler.\n\nThe abstract from the paper is the following:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly and includes additional information such as the arXiv identifier, which is not necessary but does not detract from the correctness of the response."}, {"question": "What is the dimension of the feature vector for the base BERT model?\n", "true_answer": "768", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter1/05_encoders.md", "generated_answer": "The dimension of the feature vector for the base BERT model is 768, as stated in the first extract provided. This vector holds the \"meaning\" of a word in its context, thanks to the self-attention mechanism. The vector is output by the BERT model for each word in a given text, and takes into account the words surrounding it to provide a contextualized value. This vector can be used for various tasks, such as classification, by passing it through", "retrieved_docs": ["Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn't the representation of only the \"to\" word. It also takes into account the words around it, which we call the \u201ccontext\u201d.As in, it looks to the left context, the word on the left of the one we're studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \"meaning\" of that word in the text. How it does this is thanks to the self-attention mechanism", "The following table shows a complete list of all investigated model\nvariants including the number of randomly initialized weights, *i.e.*\n\\\"random\\\", and the number of weights initialized from the respective\npre-trained checkpoints, *i.e.* \\\"leveraged\\\". All models are based on a\n12-layer architecture with 768-dim hidden size embeddings, corresponding\nto the `bert-base-cased`, `bert-base-uncased`, `roberta-base`, and\n`gpt2` checkpoints in the \ud83e\udd17Transformers model hub.\n\n  |Model          |random   |leveraged   |total\n  |-------------- |:------- |----------  |-------\n  |Rnd2Rnd        |221M     |0           |221M\n  |Rnd2BERT       |112M     |109M        |221M\n  |BERT2Rnd       |112M     |109M        |221M\n  |Rnd2GPT2       |114M     |125M        |238M\n  |BERT2BERT      |26M      |195M        |221M\n  |BERTShare      |26M      |109M        |135M\n  |RoBERTaShare   |26M      |126M        |152M\n  |BERT2GPT2      |26M      |234M        |260M\n  |RoBERTa2GPT2   |26M      |250M        |276M", "Cool! Next, let\\'s get a sense of the length of input data and labels.\n\nAs models compute length in *token-length*, we will make use of the\n`bert-base-uncased` tokenizer to compute the article and summary length.\n\nFirst, we load the tokenizer.\n\n```python\nfrom transformers import BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n```\n\nNext, we make use of `.map()` to compute the length of the article and\nits summary. Since we know that the maximum length that\n`bert-base-uncased` can process amounts to 512, we are also interested\nin the percentage of input samples being longer than the maximum length.\nSimilarly, we compute the percentage of summaries that are longer than\n64, and 128 respectively.\n\nWe can define the `.map()` function as follows.", "The result is a matrix of shape (number of examples, hidden size). The hidden size is the number of dimensions in the hidden representation. For BERT, the hidden size is 768. The hidden representation is a vector of numbers that represents the text that can be used for many different tasks. In this case, we will use it for classification with [XGBoost](https://github.com/dmlc/xgboost) afterwards.\n\n```python\nimport numpy as np\nimport tqdm\n# Function that transforms a list of texts to their representation\n# learned by the transformer.\ndef text_to_tensor(\n   list_text_X_train: list,\n   transformer_model: AutoModelForSequenceClassification,\n   tokenizer: AutoTokenizer,\n   device: str,\n) -> np.ndarray:\n   # Tokenize each text in the list one by one\n   tokenized_text_X_train_split = []\n   tokenized_text_X_train_split = [\n       tokenizer.encode(text_x_train, return_tensors=\"pt\")\n       for text_x_train in list_text_X_train\n   ]", "---\n \\\\( {}^1 \\\\) For a simpler explanation, the layer norm layer which is normally applied to \\\\( \\mathbf{\\overline{Z}} \\\\) before being processed by the feed forward layers is omitted for now.\n\n \\\\( {}^2 \\\\) In `bert-base-uncased`, *e.g.* the intermediate dimension \\\\( d_{f} \\\\) is with 3072 four times larger than the output dimension \\\\( d_{h} \\\\).\n\n \\\\( {}^3 \\\\) As a reminder, the output `config.num_attention_heads` is assumed to be 1 for the sake of clarity and illustration in this notebook, so that the output of the self-attention layers can be assumed to be of size `config.hidden_size`.\n\nMore information on chunked linear / feed forward layers can also be found [here](https://huggingface.co/transformers/glossary.html#feed-forward-chunking) on the \ud83e\udd17Transformers docs.\n\n\n### Benchmark\n\nLet's test how much memory can be saved by using chunked feed forward layers.", "Regarding the summary length, we can see that a length of 128 already\nincludes all of the summary labels. 128 is easily within the limits of\n`bert-base-cased`, so we decide to limit the generation to 128.\n\nAgain, we will make use of the `.map()` function - this time to\ntransform each training batch into a batch of model inputs.\n\n`\"article\"` and `\"highlights\"` are tokenized and prepared as the\nEncoder\\'s `\"input_ids\"` and Decoder\\'s `\"decoder_input_ids\"`\nrespectively.\n\n`\"labels\"` are shifted automatically to the left for language modeling\ntraining.\n\nLastly, it is very important to remember to ignore the loss of the\npadded labels. In \ud83e\udd17Transformers this can be done by setting the label to\n-100. Great, let\\'s write down our mapping function then.\n\n```python\nencoder_max_length=512\ndecoder_max_length=128", "(dense): Linear(in_features=3072, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n            )\n          )\n        )\n        (cls): BertOnlyMLMHead(\n          (predictions): BertLMPredictionHead(\n            (transform): BertPredictionHeadTransform(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            )\n            (decoder): Linear(in_features=768, out_features=30522, bias=True)\n          )\n        )\n      )\n    )\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the dimension of the feature vector for the base BERT model as 768, which is in line with the reference answer. The additional explanation provided about the vector's role and its use in various tasks does not detract from the accuracy of the answer to the question asked. Therefore, the response is completely correct, accurate, and factual."}, {"question": "What special identifier does the WordPiece Model use for continuing subwords?\n", "true_answer": "##", "source_doc": "huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx", "generated_answer": "The WordPiece Model uses the special identifier \"##\" for continuing subwords. (Source: Context provided)", "retrieved_docs": ["<a id='wordpiece'></a>\n\n### WordPiece\n\nWordPiece is the subword tokenization algorithm used for [BERT](model_doc/bert), [DistilBERT](model_doc/distilbert), and [Electra](model_doc/electra). The algorithm was outlined in [Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.", "<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"WordPiece is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.\",\n\t\t\texplain: \"That's the case indeed!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.\",\n\t\t\texplain: \"No, that's the approach taken by a different tokenization algorithm.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizers learn merge rules by merging the pair of tokens that is the most frequent.\",\n\t\t\texplain: \"No, that's the strategy applied by another tokenization algorithm.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"A WordPiece tokenizer learns a merge rule by merging the pair of tokens that maximizes a score that privileges frequent pairs with less frequent individual parts.\",\n\t\t\texplain: \"That's correct!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model.\",", "explain: \"No, that's how another tokenization algorithm works.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text.\",\n\t\t\texplain: \"Yes, this is how WordPiece proceeds for the encoding.\",\n\t\t\tcorrect: true\n\t\t},\n\t]}\n/>", "Now that we've seen a little of how some different tokenizers process text, we can start to explore the underlying algorithms themselves. We'll begin with a quick look at the broadly widely applicable SentencePiece; then, over the next three sections, we'll examine how the three main algorithms used for subword tokenization work.\n\n## SentencePiece[[sentencepiece]]\n\n[SentencePiece](https://github.com/google/sentencepiece) is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, `\u2581`. Used in conjunction with the Unigram algorithm (see [section 7](/course/chapter7/7)), it doesn't even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese).", "| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don\u2019t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word).  |\n| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |", "</Tip>\n\n## Training algorithm[[training-algorithm]]\n\n<Tip warning={true}>\n\n\u26a0\ufe0f Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate.\n\n</Tip>\n\nLike BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\n\n```\nw ##o ##r ##d\n```\n\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.", "The other main feature of SentencePiece is *reversible tokenization*: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the `_`s with spaces -- this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible.\n\n## Algorithm overview[[algorithm-overview]]\n\nIn the following sections, we'll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here's a quick overview of how they each work. Don't hesitate to come back to this table after reading each of the next sections if it doesn't make sense to you yet."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It correctly identifies \"##\" as the special identifier used by the WordPiece Model for continuing subwords, which matches the reference answer."}, {"question": "What is the purpose of the \ud83e\udde8 Diffusers tutorials?\n", "true_answer": "To provide a gentle introduction to diffusion models and help understand the library fundamentals.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/tutorials/tutorial_overview.md", "generated_answer": "The purpose of the \ud83e\udde8 Diffusers tutorials, as mentioned in the context, is to provide a basic crash course for learning how to use the library's most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model. These tutorials aim to serve as an example on how to use Diffusers for training and also strive to serve as good educational materials. The philosophy behind these tutorials is to keep them simple, serve", "retrieved_docs": ["--\ntitle: What's new in Diffusers? \ud83c\udfa8\nthumbnail: /blog/assets/102_diffusers_2nd_month/inpainting.png\nauthors:\n- user: osanseviero\n---\n\n# What's new in Diffusers? \ud83c\udfa8\n\n\nA month and a half ago we released `diffusers`, a library that provides a modular toolbox for diffusion models across modalities. A couple of weeks later, we released support for Stable Diffusion, a high quality text-to-image model, with a free demo for anyone to try out. Apart from burning lots of GPUs, in the last three weeks the team has decided to add one or two new features to the library that we hope the community enjoys! This blog post gives a high-level overview of the new features in `diffusers` version 0.3! Remember to give a \u2b50 to the [GitHub repository](https://github.com/huggingface/diffusers).", "**Note**: If you are looking for **official** examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines).\n\nOur examples aspire to be **self-contained**, **easy-to-tweak**, **beginner-friendly** and for **one-purpose-only**.\nMore specifically, this means:", "# Diffusers examples with ONNXRuntime optimizations\n\n**This research project is not actively maintained by the diffusers team. For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.**\n\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!", "| **Documentation**                                                   | **What can I learn?**                                                                                                                                                                           |\n|---------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Tutorial](https://huggingface.co/docs/diffusers/tutorials/tutorial_overview)                                                            | A basic crash course for learning how to use the library's most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model.  |", "Training examples of the Diffusers library should adhere to the following philosophy:\n- All the code necessary to run the examples should be found in a single Python file.\n- One should be able to run the example from the command line with `python <your-example>.py --args`.\n- Examples should be kept simple and serve as **an example** on how to use Diffusers for training. The purpose of example scripts is **not** to create state-of-the-art diffusion models, but rather to reproduce known training schemes without adding too much custom logic. As a byproduct of this point, our examples also strive to serve as good educational materials.", "Training examples of the Diffusers library should adhere to the following philosophy:\n- All the code necessary to run the examples should be found in a single Python file.\n- One should be able to run the example from the command line with `python <your-example>.py --args`.\n- Examples should be kept simple and serve as **an example** on how to use Diffusers for training. The purpose of example scripts is **not** to create state-of-the-art diffusion models, but rather to reproduce known training schemes without adding too much custom logic. As a byproduct of this point, our examples also strive to serve as good educational materials.", "# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It aligns well with the reference answer by mentioning the purpose of the \ud83e\udde8 Diffusers tutorials, which is to offer a basic course for understanding and using the library's features. The response goes into detail about the use of models and schedulers, as well as the aim to serve as educational material, which is in line with the reference answer's mention of a gentle introduction and understanding library fundamentals. However, the response could be more concise to match the reference answer's brevity."}, {"question": "What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n", "true_answer": "\"manual\"", "source_doc": "gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/using-flagging.md", "generated_answer": "The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `\"manual\"`. This means that users will see a button to flag, and samples are only flagged when the button is clicked. The other possible values for this parameter are `\"auto\"` and `\"never\"`. With `\"auto\"`, every sample will be flagged automatically, and with `\"never\"`, users will not see a button to flag, and no", "retrieved_docs": ["\u5728 `gradio.Interface` \u4e2d\u6709[\u56db\u4e2a\u53c2\u6570](https://gradio.app/docs/#interface-header)\u63a7\u5236\u6807\u8bb0\u7684\u5de5\u4f5c\u65b9\u5f0f\u3002\u6211\u4eec\u5c06\u8be6\u7ec6\u4ecb\u7ecd\u5b83\u4eec\u3002\n\n- `allow_flagging`\uff1a\u6b64\u53c2\u6570\u53ef\u4ee5\u8bbe\u7f6e\u4e3a `\"manual\"`\uff08\u9ed8\u8ba4\u503c\uff09\uff0c`\"auto\"` \u6216 `\"never\"`\u3002\n  - `manual`\uff1a\u7528\u6237\u5c06\u770b\u5230\u4e00\u4e2a\u6807\u8bb0\u6309\u94ae\uff0c\u53ea\u6709\u5728\u70b9\u51fb\u6309\u94ae\u65f6\u6837\u672c\u624d\u4f1a\u88ab\u6807\u8bb0\u3002\n  - `auto`\uff1a\u7528\u6237\u5c06\u4e0d\u4f1a\u770b\u5230\u4e00\u4e2a\u6807\u8bb0\u6309\u94ae\uff0c\u4f46\u6bcf\u4e2a\u6837\u672c\u90fd\u4f1a\u81ea\u52a8\u88ab\u6807\u8bb0\u3002\n  - `never`\uff1a\u7528\u6237\u5c06\u4e0d\u4f1a\u770b\u5230\u4e00\u4e2a\u6807\u8bb0\u6309\u94ae\uff0c\u5e76\u4e14\u4e0d\u4f1a\u6807\u8bb0\u4efb\u4f55\u6837\u672c\u3002\n- `flagging_options`\uff1a\u6b64\u53c2\u6570\u53ef\u4ee5\u662f `None`\uff08\u9ed8\u8ba4\u503c\uff09\u6216\u5b57\u7b26\u4e32\u5217\u8868\u3002\n  - \u5982\u679c\u662f `None`\uff0c\u5219\u7528\u6237\u53ea\u9700\u70b9\u51fb**\u6807\u8bb0**\u6309\u94ae\uff0c\u4e0d\u4f1a\u663e\u793a\u5176\u4ed6\u9009\u9879\u3002\n  - \u5982\u679c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u5219\u7528\u6237\u4f1a\u770b\u5230\u591a\u4e2a\u6309\u94ae\uff0c\u5bf9\u5e94\u4e8e\u63d0\u4f9b\u7684\u6bcf\u4e2a\u5b57\u7b26\u4e32\u3002\u4f8b\u5982\uff0c\u5982\u679c\u6b64\u53c2\u6570\u7684\u503c\u4e3a`[\" \u9519\u8bef \", \" \u6a21\u7cca \"]`\uff0c\u5219\u4f1a\u663e\u793a\u6807\u8bb0\u4e3a**\u6807\u8bb0\u4e3a\u9519\u8bef**\u548c**\u6807\u8bb0\u4e3a\u6a21\u7cca**\u7684\u6309\u94ae\u3002\u8fd9\u4ec5\u9002\u7528\u4e8e `allow_flagging` \u4e3a `\"manual\"` \u7684\u60c5\u51b5\u3002\n  - \u6240\u9009\u9009\u9879\u5c06\u4e0e\u8f93\u5165\u548c\u8f93\u51fa\u4e00\u8d77\u8bb0\u5f55\u3002\n- `flagging_dir`\uff1a\u6b64\u53c2\u6570\u63a5\u53d7\u4e00\u4e2a\u5b57\u7b26\u4e32\u3002\n  - \u5b83\u8868\u793a\u6807\u8bb0\u6570\u636e\u5b58\u50a8\u7684\u76ee\u5f55\u540d\u79f0\u3002\n- `flagging_callback`\uff1a\u6b64\u53c2\u6570\u63a5\u53d7 `FlaggingCallback` \u7c7b\u7684\u5b50\u7c7b\u7684\u5b9e\u4f8b\n  - \u4f7f\u7528\u6b64\u53c2\u6570\u5141\u8bb8\u60a8\u7f16\u5199\u5728\u70b9\u51fb\u6807\u8bb0\u6309\u94ae\u65f6\u8fd0\u884c\u7684\u81ea\u5b9a\u4e49\u4ee3\u7801\n  - \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5b83\u8bbe\u7f6e\u4e3a `gr.CSVLogger` \u7684\u4e00\u4e2a\u5b9e\u4f8b\n  - \u4e00\u4e2a\u793a\u4f8b\u662f\u5c06\u5176\u8bbe\u7f6e\u4e3a `gr.HuggingFaceDatasetSaver` \u7684\u4e00\u4e2a\u5b9e\u4f8b\uff0c\u8fd9\u6837\u60a8\u53ef\u4ee5\u5c06\u4efb\u4f55\u6807\u8bb0\u7684\u6570\u636e\u5bfc\u5165\u5230 HuggingFace \u6570\u636e\u96c6\u4e2d\uff08\u53c2\u89c1\u4e0b\u6587\uff09\u3002\n\n## \u6807\u8bb0\u7684\u6570\u636e\u4f1a\u53d1\u751f\u4ec0\u4e48\uff1f\n\n\u5728 `flagging_dir` \u53c2\u6570\u63d0\u4f9b\u7684\u76ee\u5f55\u4e2d\uff0c\u5c06\u8bb0\u5f55\u6807\u8bb0\u7684\u6570\u636e\u7684 CSV \u6587\u4ef6\u3002\n\n\u4ee5\u4e0b\u662f\u4e00\u4e2a\u793a\u4f8b\uff1a\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e86\u5d4c\u5165\u5176\u4e2d\u7684\u8ba1\u7b97\u5668\u754c\u9762\uff1a\n\n```python\nimport gradio as gr", "## The **Flag** button in `gradio.Interface`\n\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.\n\nThere are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.", "![flagging callback hf](/assets/guides/flagging-callback-hf.png)\n\n\u6211\u4eec\u521b\u5efa\u4e86 `gradio.HuggingFaceDatasetSaver` \u7c7b\uff0c\u4f46\u53ea\u8981\u5b83\u7ee7\u627f\u81ea[\u6b64\u6587\u4ef6](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py)\u4e2d\u5b9a\u4e49\u7684 `FlaggingCallback`\uff0c\u60a8\u53ef\u4ee5\u4f20\u9012\u81ea\u5df1\u7684\u81ea\u5b9a\u4e49\u7c7b\u3002\u5982\u679c\u60a8\u521b\u5efa\u4e86\u4e00\u4e2a\u5f88\u68d2\u7684\u56de\u8c03\uff0c\u8bf7\u5c06\u5176\u8d21\u732e\u7ed9\u8be5\u5b58\u50a8\u5e93\uff01\n\n## \u4f7f\u7528 Blocks \u8fdb\u884c\u6807\u8bb0\n\n\u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528 `gradio.Blocks`\uff0c\u53c8\u8be5\u600e\u4e48\u529e\u5462\uff1f\u4e00\u65b9\u9762\uff0c\u4f7f\u7528 Blocks \u60a8\u62e5\u6709\u66f4\u591a\u7684\u7075\u6d3b\u6027\n--\u60a8\u53ef\u4ee5\u7f16\u5199\u4efb\u4f55\u60a8\u60f3\u5728\u6309\u94ae\u88ab\u70b9\u51fb\u65f6\u8fd0\u884c\u7684 Python \u4ee3\u7801\uff0c\n\u5e76\u4f7f\u7528 Blocks \u4e2d\u7684\u5185\u7f6e\u4e8b\u4ef6\u5206\u914d\u5b83\u3002\n\n\u540c\u65f6\uff0c\u60a8\u53ef\u80fd\u5e0c\u671b\u4f7f\u7528\u73b0\u6709\u7684 `FlaggingCallback` \u6765\u907f\u514d\u7f16\u5199\u989d\u5916\u7684\u4ee3\u7801\u3002\n\u8fd9\u9700\u8981\u4e24\u4e2a\u6b65\u9aa4\uff1a\n\n1. \u60a8\u5fc5\u987b\u5728\u4ee3\u7801\u4e2d\u7684\u67d0\u4e2a\u4f4d\u7f6e\u8fd0\u884c\u60a8\u7684\u56de\u8c03\u7684 `.setup()` \u65b9\u6cd5\n   \u5728\u7b2c\u4e00\u6b21\u6807\u8bb0\u6570\u636e\u4e4b\u524d\n2. \u5f53\u70b9\u51fb\u6807\u8bb0\u6309\u94ae\u65f6\uff0c\u60a8\u89e6\u53d1\u56de\u8c03\u7684 `.flag()` \u65b9\u6cd5\uff0c\n   \u786e\u4fdd\u6b63\u786e\u6536\u96c6\u53c2\u6570\u5e76\u7981\u7528\u901a\u5e38\u7684\u9884\u5904\u7406\u3002\n\n\u4e0b\u9762\u662f\u4e00\u4e2a\u4f7f\u7528\u9ed8\u8ba4\u7684 `CSVLogger` \u6807\u8bb0\u56fe\u50cf\u6000\u65e7\u6ee4\u955c Blocks \u6f14\u793a\u7684\u793a\u4f8b\uff1a\ndata using the default `CSVLogger`:\n\n$code_blocks_flag\n$demo_blocks_flag\n\n## \u9690\u79c1\n\n\u91cd\u8981\u63d0\u793a\uff1a\u8bf7\u786e\u4fdd\u7528\u6237\u4e86\u89e3\u4ed6\u4eec\u63d0\u4ea4\u7684\u6570\u636e\u4f55\u65f6\u88ab\u4fdd\u5b58\u4ee5\u53ca\u60a8\u8ba1\u5212\u5982\u4f55\u5904\u7406\u5b83\u3002\u5f53\u60a8\u4f7f\u7528 `allow_flagging=auto`\uff08\u5f53\u901a\u8fc7\u6f14\u793a\u63d0\u4ea4\u7684\u6240\u6709\u6570\u636e\u90fd\u88ab\u6807\u8bb0\u65f6\uff09\uff0c\u8fd9\u4e00\u70b9\u5c24\u4e3a\u91cd\u8981\n\n### \u8fd9\u5c31\u662f\u5168\u90e8\uff01\u795d\u60a8\u5efa\u8bbe\u6109\u5feb :)", "iface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\n    flagging_callback=hf_writer\n)\n\niface.launch()\n```\n\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow_flagging=\"manual\"`\nbecause on Hugging Face Spaces, `allow_flagging` is set to `\"never\"` by default. Here's our demo:\n\n<gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app>", "- `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.\n  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n  - `never`: users will not see a button to flag, and no sample will be flagged.\n- `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n  - If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.\n  - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n  - The chosen option is then logged along with the input and output.", "If you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV.\n\nIf we go back to the calculator example, the following code will create the interface embedded below it.\n\n```python\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\n)\n\niface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flagging-options/\"></gradio-app>\n\nWhen users click the flag button, the csv file will now include a column indicating the selected option.\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```", "- [#5496](https://github.com/gradio-app/gradio/pull/5496) [`82ec4d26`](https://github.com/gradio-app/gradio/commit/82ec4d2622a43c31b248b78e9410e2ac918f6035) - Allow interface with components to be run inside blocks.  Thanks [@abidlabs](https://github.com/abidlabs)!"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that the default setting for the `allow_flagging` parameter in Gradio's `Interface` is `\"manual\"`. Additionally, the response gives extra information about what the \"manual\" setting means and what the other possible values are, which supports the correctness and factual nature of the answer."}, {"question": "Where can the full code for the Stable Diffusion demo be found?\n", "true_answer": "https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "source_doc": "gradio-app/gradio/blob/main/demo/stable-diffusion/DESCRIPTION.md", "generated_answer": "The full code for the Stable Diffusion demo can be found at the following link: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. Alternatively, the original codebase for the demo can be found at [Xiang-cd/DiffEdit-stable-diffusion](https://github.com/Xiang-cd/DiffEdit-stable-diffusion) and a demo can be tried out at [blog.", "retrieved_docs": ["ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "The original codebase can be found at [Xiang-cd/DiffEdit-stable-diffusion](https://github.com/Xiang-cd/DiffEdit-stable-diffusion), and you can try it out in this [demo](https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html).\n\nThis pipeline was contributed by [clarencechen](https://github.com/clarencechen). \u2764\ufe0f\n\n## Tips", "### Stable Diffusion v1.5\n\n[Stable Diffusion v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5) is a latent diffusion model initialized from [Stable Diffusion v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4), and finetuned for 595K steps on 512x512 images from the LAION-Aesthetics V2 dataset. You can use this model like:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n\t\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\"\n).to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(31)\nimage = pipeline(\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", generator=generator).images[0]\nimage\n```\n\n### Stable Diffusion XL", "The original codebase can be found at [LambdaLabsML/lambda-diffusers](https://github.com/LambdaLabsML/lambda-diffusers#stable-diffusion-image-variations) and additional official checkpoints for image variation can be found at [lambdalabs/sd-image-variations-diffusers](https://huggingface.co/lambdalabs/sd-image-variations-diffusers).\n\n<Tip>\n\nMake sure to check out the Stable Diffusion [Tips](./overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently!\n\n</Tip>\n\n## StableDiffusionImageVariationPipeline\n\n[[autodoc]] StableDiffusionImageVariationPipeline\n\t- all\n\t- __call__\n\t- enable_attention_slicing\n\t- disable_attention_slicing\n\t- enable_xformers_memory_efficient_attention\n\t- disable_xformers_memory_efficient_attention\n\n## StableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput", "Gradio Demo: stable-diffusion\n### Note: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n        \n\n\n```\n!pip install -q gradio diffusers transformers nvidia-ml-py3 ftfy torch\n```\n\n\n```\nimport gradio as gr\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport os\n\nauth_token = os.getenv(\"auth_token\")\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cpu\"\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id, use_auth_token=auth_token, revision=\"fp16\", torch_dtype=torch.float16\n)\npipe = pipe.to(device)", "And that's it!\n\nOnce you have created your tool, open a pull request to the `gradio_tools` repo! We welcome all contributions.\n\n## Example tool - Stable Diffusion\n\nHere is the code for the StableDiffusion tool as an example:\n\n```python\nfrom gradio_tool import GradioTool\nimport os\n\nclass StableDiffusionTool(GradioTool):\n    \"\"\"Tool for calling stable diffusion from llm\"\"\"\n\n    def __init__(\n        self,\n        name=\"StableDiffusion\",\n        description=(\n            \"An image generator. Use this to generate images based on \"\n            \"text input. Input should be a description of what the image should \"\n            \"look like. The output will be a path to an image file.\"\n        ),\n        src=\"gradio-client-demos/stable-diffusion\",\n        hf_token=None,\n    ) -> None:\n        super().__init__(name, description, src, hf_token)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(query, \"\", 9, fn_index=1)", "And much more! Check out the repo.\n\n### Reusable Seeds\n\nOne of the most impressive early demos of Stable Diffusion was the reuse of seeds to tweak images. The idea is to use the seed of an image of interest to generate a new image, with a different prompt. This yields some cool results! Check out the [Colab](https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb) \n\n\n## Thanks for reading!\n\nI hope you enjoy reading this! Remember to give a Star in our [GitHub Repository](https://github.com/huggingface/diffusers) and join the [Hugging Face Discord Server](https://hf.co/join/discord), where we have a category of channels just for Diffusion models. Over there the latest news in the library are shared! \n\nFeel free to open issues with feature requests and bug reports! Everything that has been achieved couldn't have been done without such an amazing community."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provides the correct link to the full code for the Stable Diffusion demo as per the reference answer, which is https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. Additionally, the response offers an alternative source for the original codebase and a demo, which is not required but adds value for someone interested in the topic. The response is therefore completely correct, accurate, and factual."}, {"question": "What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n", "true_answer": "Fourier transform", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md", "generated_answer": "The FNet model replaces the self-attention layer in a BERT model with a fourier transform that returns only the real parts of the transform. (Source: Extract 1)", "retrieved_docs": ["-->\n\n# FNet\n\n## Overview\n\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:", "*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all", "*Pre-trained language models like BERT and its variants have recently achieved impressive performance in various\nnatural language understanding tasks. However, BERT heavily relies on the global self-attention block and thus suffers\nlarge memory footprint and computation cost. Although all its attention heads query on the whole input sequence for\ngenerating the attention map from a global perspective, we observe some heads only need to learn local dependencies,\nwhich means the existence of computation redundancy. We therefore propose a novel span-based dynamic convolution to\nreplace these self-attention heads to directly model local dependencies. The novel convolution heads, together with the\nrest self-attention heads, form a new mixed attention block that is more efficient at both global and local context\nlearning. We equip BERT with this mixed attention design and build a ConvBERT model. Experiments have shown that", "The abstract from the paper is the following:\n\n*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.*", "$$\n  f_{\\theta{\\text{p,c}}}: \\mathbf{\\overline{X}}_{1:n} \\to c.\n $$\n\nIt has been shown that adding a pooling- and classification layer,\ndefined as \\\\(\\theta_{\\text{p,c}}\\\\), on top of a pre-trained BERT model\n\\\\(\\theta_{\\text{BERT}}\\\\) and subsequently fine-tuning the complete model\n\\\\(\\{\\theta_{\\text{p,c}}, \\theta_{\\text{BERT}}\\}\\\\) can yield\nstate-of-the-art performances on a variety of NLU tasks, *cf.* to [BERT\nby Devlin et al.](https://arxiv.org/abs/1810.04805).\n\nLet\\'s visualize BERT.\n\n![texte du\nlien](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/bert.png)\n\nThe BERT model is shown in grey. The model stacks multiple *BERT\nblocks*, each of which is composed of *bi-directional* self-attention\nlayers (shown in the lower part of the red box) and two feed-forward\nlayers (short in the upper part of the red box).", "2.  Second, BERT\\'s *bi-directional* self-attention layers have to be\n    changed to *uni-directional* self-attention layers to comply with\n    auto-regressive generation. Because both the bi-directional and the\n    uni-directional self-attention layer are based on the same *key*,\n    *query* and *value* projection weights, the decoder\\'s\n    self-attention layer weights can be initialized with BERT\\'s\n    self-attention layer weights. *E.g.* the query, key and value weight\n    parameters of the decoder\\'s uni-directional self-attention layer\n    are initialized with those of BERT\\'s bi-directional self-attention", "techniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\nset. The SqueezeBERT code will be released.*"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the Fourier transform as the replacement for the self-attention layer in the FNet model, which aligns with the reference answer. Additionally, the response provides an accurate detail that FNet returns only the real parts of the transform, which is a factual statement about the FNet model's implementation. This additional detail demonstrates a complete and accurate understanding of the transformation used in FNet."}, {"question": "What type of test should typically accompany a bug fix in Gradio's testing strategy?\n", "true_answer": "Dynamic code test", "source_doc": "gradio-app/gradio/blob/main/test-strategy.md", "generated_answer": "According to the provided testing strategy document in Gradio's context, the type of test that should typically accompany a bug fix is a test. The document states that bug fixes should be accompanied by tests wherever is reasonably possible, which falls under the first objective of ensuring that the Gradio library functions as expected. This helps to prevent regressions and ensures that the fix is effective in resolving the issue.", "retrieved_docs": ["We should not focus on code coverage but on test coverage following the below criteria:\n\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n\n## Types of testing", "gradio_test\n\n## 0.3.3\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d), [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:\n  - @gradio/statustracker@0.4.3\n  - @gradio/atoms@0.4.1\n\n## 0.3.2\n\n### Patch Changes\n\n- Updated dependencies [[`4d1cbbc`](https://github.com/gradio-app/gradio/commit/4d1cbbcf30833ef1de2d2d2710c7492a379a9a00)]:\n  - @gradio/atoms@0.4.0\n  - @gradio/statustracker@0.4.2\n\n## 0.3.1\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.3.1\n  - @gradio/statustracker@0.4.1\n\n## 0.3.0\n\n### Features\n\n- [#6532](https://github.com/gradio-app/gradio/pull/6532) [`96290d304`](https://github.com/gradio-app/gradio/commit/96290d304a61064b52c10a54b2feeb09ca007542) - tweak deps. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.2.3\n\n### Patch Changes", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.3.0-beta.6\n\n### Features", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests.  Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.1.0-beta.6\n\n### Features", "Test Strategy\n\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to.\n\nThis document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).\n\n## Objectives\n\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n\n1. Ensure that the Gradio library functions as we expect it to.\n2. Enable the maintenance team to quickly identify both the presence and source of defects.\n3. Prevent regressions, i.e. if we fix something it should stay fixed.\n4. Improve the quality of the codebase in order to ease maintenance efforts.\n5. Reduce the amount of manual testing required.\n\n## Scope", "### Testing and Infrastructure Changes\n\n1. Website fixes and refactoring by [@aliabd](https://github.com/aliabd) in [PR 2280](https://github.com/gradio-app/gradio/pull/2280)\n2. Don't deploy to spaces on release by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2313](https://github.com/gradio-app/gradio/pull/2313)\n\n### Full Changelog:", "### Testing and Infrastructure Changes\n\n1. Website fixes and refactoring by [@aliabd](https://github.com/aliabd) in [PR 2280](https://github.com/gradio-app/gradio/pull/2280)\n2. Don't deploy to spaces on release by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2313](https://github.com/gradio-app/gradio/pull/2313)\n\n### Full Changelog:"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response does not specify the type of test that should accompany a bug fix in Gradio's testing strategy, which according to the reference answer is a \"Dynamic code test.\" The response is generic and does not provide the specific type of test required. It only mentions that a test should accompany a bug fix, which is a broad statement and does not align with the precision of the reference answer. Based on the score rubric, the response is somewhat correct as it acknowledges the need for a test but fails to specify the type, making it only partially accurate."}, {"question": "How can you force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate?\n", "true_answer": "By passing `fp16=True` to the Accelerator init.", "source_doc": "huggingface/blog/blob/main/accelerate-library.md", "generated_answer": "To force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate, you can set the `mixed_precision` flag to `\"fp16\"` or `\"bf16\"` in the `Accelerator` initialization, as shown in the following example:\n\n```python\nfrom accelerate import Accelerator\n\naccelerator = Accelerator(mixed_precision=\"fp16\")\n``", "retrieved_docs": ["For example, to speedup training with mixed precision using the bf16 format, add the `--mixed_precision` parameter to the training command:\n\n```bash\naccelerate launch train_unconditional.py \\\n  --mixed_precision=\"bf16\"\n```\n\nSome basic and important parameters to specify include:\n\n- `--dataset_name`: the name of the dataset on the Hub or a local path to the dataset to train on\n- `--output_dir`: where to save the trained model\n- `--push_to_hub`: whether to push the trained model to the Hub\n- `--checkpointing_steps`: frequency of saving a checkpoint as the model trains; this is useful if training is interrupted, you can continue training from that checkpoint by adding `--resume_from_checkpoint` to your training command\n\nBring your dataset, and let the training script handle everything else!\n\n## Training script", "For example, to speedup training with mixed precision using the fp16 format, add the `--mixed_precision` parameter to the training command:\n\n```bash\naccelerate launch train_text_to_image.py \\\n  --mixed_precision=\"fp16\"\n```\n\nSome basic and important parameters include:\n\n- `--pretrained_model_name_or_path`: the name of the model on the Hub or a local path to the pretrained model\n- `--dataset_name`: the name of the dataset on the Hub or a local path to the dataset to train on\n- `--image_column`: the name of the image column in the dataset to train on\n- `--caption_column`: the name of the text column in the dataset to train on\n- `--output_dir`: where to save the trained model\n- `--push_to_hub`: whether to push the trained model to the Hub\n- `--checkpointing_steps`: frequency of saving a checkpoint as the model trains; this is useful if for some reason training is interrupted, you can continue training from that checkpoint by adding `--resume_from_checkpoint` to your training command", "```bash\naccelerate launch --mixed_precision=\"fp16\" train_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --dataset_name=$DATASET_ID \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=256 \\\n    --random_flip \\\n    --train_batch_size=4 \\\n    --gradient_accumulation_steps=4 \\\n    --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 \\\n    --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 \\\n    --max_grad_norm=1 \\\n    --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --mixed_precision=fp16 \\\n    --seed=42 \\\n    --push_to_hub\n```\n\nAfter training is finished, you can use your new InstructPix2Pix for inference:\n\n```py\nimport PIL\nimport requests\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline\nfrom diffusers.utils import load_image", "To enable mixed precision training, set the `fp16` flag to `True`:\n\n```py\ntraining_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n```\n\nIf you prefer to use \ud83e\udd17 Accelerate, find the \ud83e\udd17 Accelerate example [further in this guide](#using-accelerate). \n\n### BF16\n\nIf you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While \nbf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have \nis `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which \nis about the same as fp32 - because both have 8-bits used for the numerical range.\n\nYou can enable BF16 in the \ud83e\udd17 Trainer with:\n\n```python\ntraining_args = TrainingArguments(bf16=True, **default_args)\n```\n\n### TF32", "```bash \naccelerate launch --mixed_precision=\"fp16\" --multi_gpu train_instruct_pix2pix.py \\\n --pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5 \\\n --dataset_name=sayakpaul/instructpix2pix-1000-samples \\\n --use_ema \\\n --enable_xformers_memory_efficient_attention \\\n --resolution=512 --random_flip \\\n --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n --max_train_steps=15000 \\\n --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n --learning_rate=5e-05 --lr_warmup_steps=0 \\\n --conditioning_dropout_prob=0.05 \\\n --mixed_precision=fp16 \\\n --seed=42 \\\n --push_to_hub\n```\n\n ## Inference\n\n Once training is complete, we can perform inference:\n\n ```python\nimport PIL\nimport requests\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline", "Now, we can launch training:\n\n```bash\naccelerate launch --mixed_precision=\"fp16\" train_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --dataset_name=$DATASET_ID \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=256 --random_flip \\\n    --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 --max_grad_norm=1 --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --mixed_precision=fp16 \\\n    --seed=42 \\\n    --push_to_hub\n```\n\nAdditionally, we support performing validation inference to monitor training progress\nwith Weights and Biases. You can enable this feature with `report_to=\"wandb\"`:", "accelerate launch --mixed_precision=\"fp16\" --multi_gpu  train_text_to_image.py --ort \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$dataset_name \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\ \n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --output_dir=\"sd-pokemon-model\" \n```"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It accurately describes how to enable mixed precision training by setting the `mixed_precision` flag to either `\"fp16\"` or `\"bf16\"` during the initialization of the Accelerator in the \ud83e\udd17 Accelerate library. However, the reference answer specifies using `fp16=True`, which is not mentioned in the response. The response could be considered less direct than the reference answer, but it still conveys the correct method to achieve the task, albeit with a slightly different parameter. Therefore, the response does not fully align with the reference answer but remains factually correct in the context of how the Accelerate library works."}, {"question": "What is the range of parameters for the LLaMA models?\n", "true_answer": "7B to 65B parameters", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/llama.md", "generated_answer": "Answer: The LLaMA models range from 7B to 65B parameters, as stated in the context provided. (Source: Extract 1)", "retrieved_docs": ["-->\n\n# LLaMA\n\n## Overview\n\nThe LLaMA model was proposed in [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. It is a collection of foundation language models ranging from 7B to 65B parameters.\n\nThe abstract from the paper is the following:", "## The LLaMA model\n\nWhen doing RLHF, it is important to start with a capable model: the RLHF step is only a fine-tuning step to align the model with how we want to interact with it and how we expect it to respond.  Therefore, we choose to use the recently introduced and performant [LLaMA models](https://arxiv.org/abs/2302.13971). The LLaMA models are the latest large language models developed by Meta AI. They come in sizes ranging from 7B to 65B parameters and were trained on between 1T and 1.4T tokens, making them very capable. We use the 7B model as the base for all the following steps!\nTo access the model, use the [form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform) from Meta AI.\n\n## Stack Exchange dataset", "Check out all Code Llama model checkpoints [here](https://huggingface.co/models?search=code_llama) and the officially released ones in the [codellama org](https://huggingface.co/codellama).\n\nThis model was contributed by [ArthurZucker](https://huggingface.co/ArthurZ). The original code of the authors can be found [here](https://github.com/facebookresearch/llama).\n\n## Usage tips and examples\n\n<Tip warning={true}>\n\nThe `Llama2` family models, on which Code Llama is based, were trained using `bfloat16`, but the original inference uses `float16`. Let's look at the different precisions:", "## Appendix: Off-the-shelf Model List\n| Model Name  | Bits | Ranks |\n| ----------- | ---- | ----- |\n| LLAMA-2-7b  | 4    | 64    |\n| LLAMA-2-13b | 4    | 64    |\n| LLAMA-2-70b | 4    | 64    |\n| Mistral     | 4    | 64    |\n| Mistral     | 4    | 32    |\n| BART-large  | 4    | 8     |\n| BART-large  | 4    | 16    |\n| BART-large  | 4    | 32    |\n| BART-large  | 2    | 8     |", "*we\u2019re currently running evaluation of the Llama 2 70B (non chatty version). This table will be updated with the results.\n\n\n## Demo\n\nYou can easily try the Big Llama 2 Model (70 billion parameters!) in [this Space](https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI) or in the playground embedded below:\n\n<script type=\"module\" src=\"https://gradio.s3-us-west-2.amazonaws.com/3.37.0/gradio.js\"> </script>\n<gradio-app theme_mode=\"light\" space=\"ysharma/Explore_llamav2_with_TGI\"></gradio-app>\n\nUnder the hood, this playground uses Hugging Face's [Text Generation Inference](https://github.com/huggingface/text-generation-inference), the same technology that powers [HuggingChat](https://huggingface.co/chat/), and which we'll share more in the following sections.", "with 7b model: \n\n| model_id                           | Average | ARC   | Hellaswag | MMLU  | TruthfulQA |\n|------------------------------------|---------|-------|-----------|-------|------------|\n| meta-llama/llama-2-7b-hf           | **54.32**   | 53.07 | 78.59     | 46.87 | 38.76      |\n| meta-llama/llama-2-7b-hf-bnb-4bit  | **53.4**    | 53.07 | 77.74     | 43.8  | 38.98      |\n| TheBloke/Llama-2-7B-GPTQ           | **53.23**   | 52.05 | 77.59     | 43.99 | 39.32      |\n\nwith 13b model:", "The first model family in this series was the [LLaMA](https://huggingface.co/papers/2302.13971) family, released by Meta AI. The explicit objective of the researchers was to train a set of models of various sizes with the best possible performances for a given computing budget. For one of the first times, the research team explicitly decided to consider not only the training budget but also the inference cost (for a given performance objective, how much does it cost to run inference with the model). In this perspective, they decided to train smaller models on even more data and for more steps than was usually done, thereby reaching higher performances at a smaller model size (the trade-off being training compute efficiency). The biggest model in the Llama 1 family is a 65B parameters model trained on 1.4T tokens, while the smaller models (resp. 6 and 13B parameters) were trained on 1T tokens"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating the range of parameters for the LLaMA models as being from 7B to 65B. The inclusion of a source reference does not detract from the accuracy of the information given."}, {"question": "What is the purpose of tokenizers in the NLP pipeline?\n", "true_answer": "To translate text into data that can be processed by the model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter2/4.mdx", "generated_answer": "The purpose of tokenizers in the NLP pipeline is to convert raw text inputs into numerical data that can be processed by NLP models, as models can only process numbers. Tokenizers serve this purpose by breaking down text into smaller meaningful units, such as words or subwords, and assigning numerical values to each unit. This process is called tokenization. The goal is to find the most meaningful and smallest representation of the text that makes the most sense to the model. Tokenizers, especially those", "retrieved_docs": ["{/if}\n\n<Youtube id=\"VFp38yj8h3A\"/>\n\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. \n\nIn NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\n\n```\nJim Henson was a puppeteer\n```\n\nHowever, models can only process numbers, so we need to find a way to convert the raw text to numbers. That's what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation \u2014 that is, the one that makes the most sense to the model \u2014 and, if possible, the smallest representation.\n\nLet's take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization.", "{/if}\n\nIn this section we will take a closer look at the capabilities of the tokenizers in \ud83e\udd17 Transformers. Up to now we have only used them to tokenize inputs or decode IDs back into text, but tokenizers -- especially those backed by the \ud83e\udd17 Tokenizers library -- can do a lot more. To illustrate these additional features, we will explore how to reproduce the results of the `token-classification` (that we called `ner`) and `question-answering` pipelines that we first encountered in [Chapter 1](/course/chapter1).\n\n<Youtube id=\"g8quOxoqhHQ\"/>", "### 2. What part of the preprocessing for token classification differs from the other preprocessing pipelines?", "### Highlights of this release\n- We can now encode both pre-tokenized inputs, and raw strings. This is especially usefull when\nprocessing datasets that are already pre-tokenized like for NER (Name Entity Recognition), and helps\nwhile applying labels to each word.\n- Full tokenizer serialization. It is now easy to save a tokenizer to a single JSON file, to later\nload it back with just one line of code. That's what sharing a Tokenizer means now: 1 line of code.\n- With the serialization comes the compatibility with `Pickle`! The Tokenizer, all of its components,\nEncodings, everything can be pickled!\n- Training a tokenizer is now even faster (up to 5-10x) than before!\n- Compatibility with `multiprocessing`, even when using the `fork` start method. Since this library\nmakes heavy use of the multithreading capacities of our computers to allows a very fast tokenization,\nthis led to problems (deadlocks) when used with `multiprocessing`. This version now allows to", "In this chapter, you will learn how to train a brand new tokenizer on a corpus of texts, so it can then be used to pretrain a language model. This will all be done with the help of the [\ud83e\udd17 Tokenizers](https://github.com/huggingface/tokenizers) library, which provides the \"fast\" tokenizers in the [\ud83e\udd17 Transformers](https://github.com/huggingface/transformers) library. We'll take a close look at the features that this library provides, and explore how the fast tokenizers differ from the \"slow\" versions.\n\nTopics we will cover include:\n\n* How to train a new tokenizer similar to the one used by a given checkpoint on a new corpus of texts\n* The special features of fast tokenizers\n* The differences between the three main subword tokenization algorithms used in NLP today\n* How to build a tokenizer from scratch with the \ud83e\udd17 Tokenizers library and train it on some data", "#### Tokenization\n\n```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer\n>>> dataset = load_dataset(\"mc4\", \"en\", streaming=True, split=\"train\", trust_remote_code=True)\n>>> tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n>>> def encode(examples):\n...     return tokenizer(examples['text'], truncation=True, padding='max_length')\n>>> dataset = dataset.map(encode, batched=True, remove_columns=[\"text\", \"timestamp\", \"url\"])\n>>> next(iter(dataset))\n{'input_ids': [101, 8466, 1018, 1010, 4029, 2475, 2062, 18558, 3100, 2061, ...,1106, 3739, 102],\n'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 1, 1]}\n```\n\n<Tip>\n\nSee other examples of batch processing in the [batched map processing](./process#batch-processing) documentation. They work the same for iterable datasets.\n\n</Tip>\n\n### Filter", "|                                      |                                   | `tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY, max_length=42)` |"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It explains the purpose of tokenizers in the NLP pipeline by stating that they convert raw text into numerical data, which is a process necessary for NLP models to process the information. The response also correctly identifies that tokenization involves breaking down text into smaller units and assigning numerical values to them. However, the response goes into more detail than the reference answer, which succinctly states the purpose of tokenizers as translating text into data for the model. The additional detail does not make the response incorrect but rather more elaborate."}, {"question": "What is the purpose of the Safety Checker in the Diffusers library?\n", "true_answer": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md", "generated_answer": "The Safety Checker in the Diffusers library is a component that checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. Its purpose is to ensure that diffusion models like Stable Diffusion, which have the ability to produce harmful and NSFW content, are used responsibly and ethically. Model creators can choose to incorporate this component into their models if they want. The Safety Checker helps prevent the", "retrieved_docs": ["- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repository\u2019s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.", "### Safety checker\n\nDiffusion models like Stable Diffusion can generate harmful content, which is why \ud83e\udde8 Diffusers has a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) to check generated outputs against known hardcoded NSFW content. If you'd like to disable the safety checker for whatever reason, pass `None` to the `safety_checker` argument:\n\n```python\nfrom diffusers import DiffusionPipeline", "```python\nfrom diffusers import DiffusionPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, safety_checker=None, use_safetensors=True)\n\"\"\"\nYou have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide by the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend keeping the safety filter enabled in all public-facing circumstances, disabling it only for use cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n\"\"\"\n```\n\n### Reuse components across pipelines", "```py\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", revision=\"refs/pr/22\", use_safetensors=True\n)\n```\n\n## Why use safetensors?\n\nThere are several reasons for using safetensors:", "## Ethics and safety\n\nGenerative models are cool, but they also have the ability to produce harmful and NSFW content. To help users interact with these models responsibly and ethically, we\u2019ve added a [`safety_checker`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) component that flags inappropriate content generated during inference. Model creators can choose to incorporate this component into their models if they want.", "## Tips\n\nUse the `safety_concept` property of [`StableDiffusionPipelineSafe`] to check and edit the current safety concept:\n\n```python\n>>> from diffusers import StableDiffusionPipelineSafe\n\n>>> pipeline = StableDiffusionPipelineSafe.from_pretrained(\"AIML-TUDA/stable-diffusion-safe\")\n>>> pipeline.safety_concept\n'an image showing hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity, drug use, theft, vandalism, weapons, child abuse, brutality, cruelty'\n```\nFor each image generation the active concept is also contained in [`StableDiffusionSafePipelineOutput`].\n\nThere are 4 configurations (`SafetyConfig.WEAK`, `SafetyConfig.MEDIUM`, `SafetyConfig.STRONG`, and `SafetyConfig.MAX`) that can be applied:\n\n```python\n>>> from diffusers import StableDiffusionPipelineSafe\n>>> from diffusers.pipelines.stable_diffusion_safe import SafetyConfig", "Great, now you should have generally understood why \ud83e\udde8 Diffusers is designed the way it is \ud83e\udd17.\nWe try to apply these design principles consistently across the library. Nevertheless, there are some minor exceptions to the philosophy or some unlucky design choices. If you have feedback regarding the design, we would \u2764\ufe0f  to hear it [directly on GitHub](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=).\n\n## Design Philosophy in Details\n\nNow, let's look a bit into the nitty-gritty details of the design philosophy. Diffusers essentially consists of three major classes: [pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines), [models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models), and [schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).\nLet's walk through more detailed design decisions for each class.\n\n### Pipelines"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and aligns well with the reference answer. It accurately describes the Safety Checker as a component that checks the class probability of harmful concepts in the embedding space against an image after it has been generated. The response also correctly identifies the purpose of the Safety Checker, which is to ensure responsible and ethical use of diffusion models like Stable Diffusion that can produce harmful and NSFW content. Additionally, it mentions the optional nature of incorporating this component into models by creators. However, the response is slightly less concise than the reference answer and ends abruptly, which may indicate an incomplete explanation. Despite this, the response maintains factual accuracy and correctness in describing the Safety Checker's purpose."}, {"question": "What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n", "true_answer": "HfApi", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md", "generated_answer": "The `HfApi` class in the `huggingface_hub` library allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub. (Source: Extracted documents, question provided)", "retrieved_docs": ["!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Interact with Discussions and Pull Requests \n\nThe `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub.\nVisit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\n\n## Retrieve Discussions and Pull Requests from the Hub\n\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(repo_id=\"bigscience/bloom\"):\n...     print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is_pull_request}\")", "```python\n>>> from huggingface_hub import get_repo_discussions\n>>> discussions_list = list(get_repo_discussions(repo_id=\"bert-base-uncased\"))\n```\n\nThe [`Discussion`] object returned by [`HfApi.get_repo_discussions`] contains high-level overview of the\nDiscussion or Pull Request. You can also get more detailed information using [`HfApi.get_discussion_details`]:\n\n```python\n>>> from huggingface_hub import get_discussion_details", "## API and client library interaction with the Hub\n\nInteracting with the Hugging Face Hub via an [API](https://huggingface.co/docs/hub/api) or the [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index) Python library is possible. This includes creating new repositories, uploading data programmatically and creating and modifying metadata for datasets. This can be powerful for research workflows where new data or annotations continue to be created. The client library also makes uploading large datasets much more accessible. \n\n## Community \n\nThe Hugging Face Hub is already home to a large community of researchers, developers, artists, and others interested in using and contributing to an ecosystem of open-source machine learning. Making your datasets accessible to this community increases their visibility, opens them up to new types of users and places your datasets within the context of a larger ecosystem of models, datasets and libraries.", "--\ntitle: Introducing Pull Requests and Discussions \ud83e\udd73\nthumbnail: /blog/assets/76_community_update/thumbnail.png\n---\n\n# Introducing Pull Requests and Discussions \ud83e\udd73\n\n![Pull requests and discussions on the hub](assets/76_community_update/community-update.png)\n\nWe are thrilled to announce the release of our latest collaborative features: pull requests and discussions on the Hugging Face Hub!\n\nPull requests and discussions are available today under the [community tab](https://huggingface.co/gpt2/discussions) for all repository types: models, datasets, and Spaces. Any member of the community can create and participate in discussions and pull requests, facilitating collaborations not only within teams, but also with everyone else in the community!\n\nIt's the biggest update ever done to the Hub, and we can't wait to see the community members start collaborating with it \ud83e\udd29.", "---\n\n## Welcome to the huggingface_hub library\n\nThe `huggingface_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators. Discover pre-trained models and datasets for your projects or play with the thousands of machine learning apps hosted on the Hub. You can also create and share your own models, datasets and demos with the community. The `huggingface_hub` library provides a simple way to do all these things with Python.\n\n## Key features", "![Discussions on the Hugging Face Hub](assets/76_community_update/new-discussion.png)\n\n[Discussions](https://huggingface.co/gpt2/discussions?type=discussion) allow community members ask and answer questions as well as share their ideas and suggestions directly with the repository owners and the community. Anyone can create and participate in discussions in the community tab of a repository.\n\n## Pull requests\n\n![Pull requests on the Hugging Face Hub](assets/76_community_update/new-pr.png)\n\n[Pull requests](https://huggingface.co/gpt2/discussions?type=pull_request) allow community members open, comment, merge, or close pull requests directly from the website. The easiest way to open a pull request is to use the \"Collaborate\" button in the \"Files and versions\" tab. It will let you do single file contributions very easily.", "Next Steps\n\nThese next sections highlight features and additional information that you may find useful to make the most out of the Git repositories on the Hugging Face Hub.\n\n## How to programmatically manage repositories\n\nHugging Face supports accessing repos with Python via the [`huggingface_hub` library](https://huggingface.co/docs/huggingface_hub/index). The operations that we've explored, such as downloading repositories and uploading files, are available through the library, as well as other useful functions!\n\nIf you prefer to use git directly, please read the sections below.\n\n## Learning more about Git\n\nA good place to visit if you want to continue learning about Git is [this Git tutorial](https://learngitbranching.js.org/). For even more background on Git, you can take a look at [GitHub's Git Guides](https://github.com/git-guides). \n\n## How to use branches"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the `HfApi` class as the one used to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub. It matches the reference answer, which is scored as a 5, indicating that it is completely correct, accurate, and factual."}, {"question": "What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n", "true_answer": "Skops", "source_doc": "huggingface/blog/blob/main/skops.md", "generated_answer": "The name of the new library introduced by Hugging Face for hosting scikit-learn models is Skops. This information can be found in the context provided, specifically in the section titled \"Introducing Skops\". The question asks for the name of the library, and the answer is provided in the section title and the first sentence of the section.", "retrieved_docs": ["---\n\n## Welcome to the huggingface_hub library\n\nThe `huggingface_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators. Discover pre-trained models and datasets for your projects or play with the thousands of machine learning apps hosted on the Hub. You can also create and share your own models, datasets and demos with the community. The `huggingface_hub` library provides a simple way to do all these things with Python.\n\n## Key features", "Hugging Face is a collaborative Machine Learning platform in which the community has shared over 150,000 models, 25,000 datasets, and 30,000 ML apps. The Hub has model and dataset versioning tools, including model cards and client-side libraries to automate the versioning process. However, only including a model card with hyperparameters is not enough to provide the best reproducibility; this is where notebooks can help. Alongside these models, datasets, and demos, the Hub hosts over 7,000 notebooks. These notebooks often document the development process of a model or a dataset and can provide guidance and tutorials showing how others can use these resources. We\u2019re therefore excited about our improved support for notebook hosting on the Hub. \n\n## What have we changed?", "## API and client library interaction with the Hub\n\nInteracting with the Hugging Face Hub via an [API](https://huggingface.co/docs/hub/api) or the [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index) Python library is possible. This includes creating new repositories, uploading data programmatically and creating and modifying metadata for datasets. This can be powerful for research workflows where new data or annotations continue to be created. The client library also makes uploading large datasets much more accessible. \n\n## Community \n\nThe Hugging Face Hub is already home to a large community of researchers, developers, artists, and others interested in using and contributing to an ecosystem of open-source machine learning. Making your datasets accessible to this community increases their visibility, opens them up to new types of users and places your datasets within the context of a larger ecosystem of models, datasets and libraries.", "3. **Open-source libraries**: the Hugging Face [libraries](https://huggingface.co/docs) let you download pre-trained models with a single line of code, and you can start experimenting with your data in minutes. From training to deployment to hardware optimization, customers can rely on a consistent set of community-driven tools that work the same everywhere, from their laptops to their production environment.", "The [Hugging Face Hub](https://huggingface.co/) is a central repository where people can share and access machine learning models, datasets and demos. The Hub hosts over 190,000 machine learning models, 33,000 datasets and over 100,000 machine learning applications and demos. These models cover a wide range of tasks from pre-trained language models, text, image and audio classification models, object detection models, and a wide range of generative models. \n\nThe models, datasets and demos hosted on the Hub span a wide range of domains and languages, with regular community efforts to expand the scope of what is available via the Hub. This blog post intends to offer people working in or with the galleries, libraries, archives and museums (GLAM) sector to understand how they can use &mdash; and contribute to &mdash; the Hugging Face Hub.\n\nYou can read the whole post or jump to the most relevant sections!", "## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.", "--\ntitle: \"Introducing Skops\"\nthumbnail: /blog/assets/94_skops/introducing_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n\n# Introducing Skops\n\n\n##\u00a0Introducing Skops\n\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others.\n\nLet's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production.\n\n```python\n# let's import the libraries first\nimport sklearn\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by correctly identifying \"Skops\" as the name of the new library introduced by Hugging Face for hosting scikit-learn models."}, {"question": "What is the purpose of Textual Inversion?\n", "true_answer": "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/loaders/textual_inversion.md", "generated_answer": "Textual Inversion is a method to personalize text2image models like Stable Diffusion on one's own images using just 3-5 examples. It creates new embeddings that represent the images provided with a specific word in the prompt, allowing the diffusion model weights to remain the same during training. The purpose of Textual Inversion is to teach new concepts, such as styles or objects, to a diffusion model through personalization. It cannot be used on its own like DreamBo", "retrieved_docs": ["## Textual Inversion\n\nTextual Inversion lets you personalize a Stable Diffusion model on your own images with just 3-5 samples. With this tool, you can train a model on a concept, and then share the concept with the rest of the community! \n\n![image info](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/textual_inversion.jpeg)\n\nIn just a couple of days, the community shared over 200 concepts! Check them out!", "Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare Datasets", "## Textual inversion\n\n[Textual inversion](https://textual-inversion.github.io/) is very similar to DreamBooth and it can also personalize a diffusion model to generate certain concepts (styles, objects) from just a few images. This method works by training and finding new embeddings that represent the images you provide with a special word in the prompt. As a result, the diffusion model weights stay the same and the training process produces a relatively tiny (a few KBs) file.\n\nBecause textual inversion creates embeddings, it cannot be used on its own like DreamBooth and requires another model.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n```", "[`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings.\n\n<Tip>\n\nTo learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading_adapters#textual-inversion) loading guide.\n\n</Tip>\n\n## TextualInversionLoaderMixin\n\n[[autodoc]] loaders.textual_inversion.TextualInversionLoaderMixin", "- Learn how to [load Textual Inversion embeddings](../using-diffusers/loading_adapters) and also use them as negative embeddings.\n- Learn how to use [Textual Inversion](textual_inversion_inference) for inference with Stable Diffusion 1/2 and Stable Diffusion XL.", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"", "## Epilogue: Textual Inversion + Dreambooth\n\nWe also ran a final experiment where we combined [Textual Inversion](https://textual-inversion.github.io) with Dreambooth. Both techniques have a similar goal, but their approaches are different.\n\nIn this experiment we first ran textual inversion for 2000 steps. From that model, we then ran Dreambooth for an additional 500 steps using a learning rate of `1e-6`. These are the results:\n\n![Textual Inversion + Dreambooth](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/h_textual_inversion_dreambooth.jpg)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided gives a clear explanation of Textual Inversion, mentioning that it is a method to personalize text2image models using a small number of examples and that it creates new embeddings without altering the model weights. It also correctly identifies the purpose of Textual Inversion as teaching new concepts to a diffusion model through personalization. The response is in line with the reference answer, which describes Textual Inversion as a training method for personalizing models by learning new text embeddings from a few example images. The response is accurate and factual, although it includes additional details not present in the reference answer, such as the specific number of examples needed and the fact that the model weights remain the same. These details, however, do not detract from the correctness of the response. Therefore, the response meets the criteria for a score of 5 on the rubric."}, {"question": "What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n", "true_answer": "64", "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md", "generated_answer": "Based on the provided context, the recommended multiple of batch size for fp16 data type on an A100 GPU is a power of 2, specifically 2^N, with N being a positive integer. This is recommended for achieving optimal performance when using the AutoGPTQ and bitsandbytes quantization techniques on an A100 GPU. Additionally, it is suggested to use batch sizes and input/output neuron counts that are multiples of 8, but this", "retrieved_docs": ["Finally, if all of the above is still not enough, even after switching to a server-grade GPU like A100, consider moving \nto a multi-GPU setup. All these approaches are still valid in a multi-GPU setup, plus you can leverage additional parallelism \ntechniques outlined in the [multi-GPU section](perf_train_gpu_many). \n\n## Batch size choice\n\nTo achieve optimal performance, start by identifying the appropriate batch size. It is recommended to use batch sizes and \ninput/output neuron counts that are of size 2^N. Often it's a multiple of 8, but it can be \nhigher depending on the hardware being used and the model's dtype.", "This is a benchmark sample for the batch size = 1 case. The benchmark was run on a single NVIDIA A100-SXM4-80GB GPU. We used a prompt length of 512, and generated exactly 512 new tokens. The first row is the unquantized `fp16` baseline, while the other rows show memory consumption and performance using different AutoGPTQ kernels.", "Bitsandbytes uses the fp4 scheme, with the compute in fp16.\n\n### Batch size = 1\n\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|None|None     |None|None      |None  |26.0         |36.958                |27.058            |29152.98        |\n| gptq | False | 4 | 128 | exllamav2 | 36.07 | 32.25 | 31.01 | 11313.75 |\n|gptq |False    |4   |128       |exllama|36.2         |33.711                |29.663            |10484.34        |\n|gptq |False    |4   |128       |autogptq-cuda-old|36.2         |46.44                 |21.53             |10344.62        |\n|bitsandbytes|None     |None|None      |None  |37.64        |52.00                 |19.23             |11018.36       |\n\n### Batch size = 2", "### Batch size = 2\n\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tokens|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Max memory (MB)|\n|-----|---------|----|----------|-----------------|-------------|----------|-------------|----------------------|------------------|---------------|\n|None|None     |None|None      |None             |512          |1         |27.22        |169.95                |11.77             |28524.37       |\n| gptq | False | 4 | 128 | exllamav2 | 512 | 1 | 6.63 | 212.07  | 9.43  | 10783.60 |\n|gptq |False    |4   |128       |exllama          |512          |1         |38.35        |190.44                |10.50             |9855.71        |\n|gptq |False    |4   |128       |autogptq-cuda-old|512          |1         |43.94        |443.80                |4.51              |9928.23        |\n|bitsandbytes|None|None|None|None|512|1  |37.46|212.76 |9.40 |10421.89|\n\n### Batch size = 4", "### Fitting on a 16GB VRAM GPU\n```shell \npip install bitsandbytes\n\n--train_batch_size=1 \\\n--gradient_accumulation_steps=4 \\\n--gradient_checkpointing \\\n--use_8bit_adam\n```\n\nThe combination of a batch size of 1 with 4 gradient accumulation steps is equivalent to using the original batch size of 4 we used in our example. In addition, we enabled gradient checkpointing and 8-bit Adam for additional memory savings.\n\n### Fitting on a 12GB VRAM GPU\n```shell\n--gradient_accumulation_steps=4 \\\n--gradient_checkpointing \\\n--use_8bit_adam\n--set_grads_to_none\n```\n\n### Fitting on a 8GB VRAM GPU\nPlease follow [our guide here](https://github.com/huggingface/diffusers/tree/main/examples/controlnet#training-on-an-8-gb-gpu)", "### Batch size = 8\n\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tokens|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Max memory (MB)|\n|-----|---------|----|----------|-----------------|-------------|----------|-------------|----------------------|------------------|---------------|\n|None|None     |None|None      |None             |512          |1         |27.22        |600.47                |13.32             |31673.30       |\n| gptq | False | 4 | 128 | exllamav2 | 512 | 1 | 6.63 | 753.06  | 10.62 | 13920.50 |\n|gptq |False    |4   |128       |exllama          |512          |1         |38.35        |659.61                |12.13             |13004.64       |\n|gptq |False    |4   |128       |autogptq-cuda-old|512          |1         |43.94        |909.09                |8.80              |12862.18       |\n|bitsandbytes|None|None|None|None|512|1  |37.46|643.42 |12.43|13539.37|\n\n### Batch size = 16", "### Batch size = 1\n\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tokens|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Max memory (MB)|\n|-----|---------|----|----------|-----------------|-------------|----------|-------------|----------------------|------------------|---------------|\n|None|None     |None|None      |None             |512          |1         |27.22        |96.38                 |10.38             |27999.54       |\n| gptq | False | 4 | 128 | exllamav2 | 512 | 1 | 6.63 | 116.07  | 8.62  | 10260.35 |\n|gptq |False    |4   |128       |exllama          |512          |1         |38.35        |112.54                |8.89              |9330.89        |\n|gptq |False    |4   |128       |autogptq-cuda-old|512          |1         |43.94        |368.13                |2.72              |9474.19        |\n|bitsandbytes|None|None|None|None|512|1  |37.46|139.17 |7.19 |9952.65 |\n\n### Batch size = 2"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response provided does not directly answer the question with a specific number as the reference answer does. Instead, it gives a general guideline for selecting batch sizes that are powers of 2 and multiples of 8 for optimal performance, which is related to the question but does not specify the recommended multiple as the reference answer does. The response is somewhat correct in that it addresses the concept of using powers of 2 for batch sizes, which is a common practice for optimizing performance on GPUs. However, it fails to provide the specific recommended multiple of 64 for the fp16 data type on an A100 GPU. Therefore, the response is somewhat correct, accurate, and/or factual, but it lacks the precision of the reference answer."}, {"question": "How do you run a Gradio Blocks app in reload mode using a Python IDE?\n", "true_answer": "Run `gradio run.py` in the terminal.", "source_doc": "gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md", "generated_answer": "To run a Gradio Blocks app in reload mode using a Python IDE, follow these steps:\n\n1. Make sure your app is written as a Blocks demo, with an instance of the `gr.Interface` class.\n2. Save your app as a Python file, such as `run.py`.\n3. In the terminal, navigate to the directory where your `run.py` file is located.\n4. Instead of running `python run.", "retrieved_docs": ["This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster.\n\n## Python IDE Reload \ud83d\udd25\n\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\n\nInstead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:\n\nIn the terminal, run `gradio run.py`. That's it!", "> [!TIP]\n > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:", "Which you could run like this: `gradio run.py --name Gretel`\n\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code. Meaning that this can be useful if you decide to [contribute to Gradio itself](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md) \u2705\n\n## Jupyter Notebook Magic \ud83d\udd2e\n\nWhat about if you use Jupyter Notebooks (or Colab Notebooks, etc.) to develop code? We got something for you too!\n\nWe've developed a **magic command** that will create and run a Blocks demo for you. To use this, load the gradio extension at the top of your notebook:\n\n`%load_ext gradio`\n\nThen, in the cell that you are developing your Gradio demo, simply write the magic command **`%%blocks`** at the top, and then write the layout and components like you would normally:\n\n```py\n%%blocks\n\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(f\"# Greetings {args.name}!\")\n    inp = gr.Textbox()\n    out = gr.Textbox()", "Developing Faster with Auto-Reloading\n\n**Prerequisite**: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).\n\nThis guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\n\n## Why Auto-Reloading?\n\nWhen you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.\n\nTo make it faster and more convenient to write your code, we've made it easier to \"reload\" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \"magic command\" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).", "## \ud83d\ude80 Run a Gradio app\n\nYou can get started by creating an `app.py` file in the root:\n\n```\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n   gr.Button()\n    \nif __name__ == \"__main__\":\n   demo.launch()\n```\n\nthen run:\n\n```\ngradio app.py\n```\n\nThis will start the backend server in reload mode, which will watch for changes in the `gradio` folder and reload the app if changes are made. By default, Gradio will launch on port 7860. You can also just use `python app.py`, but this won't automatically trigger updates. \n\n\nIf you're making frontend changes, start the frontend server:\n\n```\npnpm dev\n```\nThis will open a separate browser tab. By default, Gradio will launch this on port 9876. Any changes to the frontend will also reload automatically in the browser. For more information about developing in the frontend, you can refer to [js/README.md](js/README.md).\n\nWe also have demos of all our components in the `/gradio/demo` directory. To get our simple gradio Chatbot running locally:", "There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as my_demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    my_demo.launch()\n```\n\nThen you would launch it in reload mode like this: `gradio run.py my_demo`.\n\nBy default, the Gradio use UTF-8 encoding for scripts. **For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:", "In the terminal, run `gradio run.py`. That's it!\n\nNow, you'll see that after you'll see something like this:\n\n```bash\nWatching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'\n\nRunning on local URL:  http://127.0.0.1:7860\n```\n\nThe important part here is the line that says `Watching...` What's happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically \ud83e\udd73\n\n\u26a0\ufe0f Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show_error` in `launch()` will not be reflected in the app."], "eval_score_GPT4": "2", "eval_feedback_GPT4": "The response provided does not accurately describe how to run a Gradio Blocks app in reload mode using a Python IDE. The steps mentioned are incomplete and do not specifically address the 'reload mode' aspect of the task. The response cuts off mid-sentence and does not provide the necessary command or steps to achieve the reload mode functionality. According to the reference answer, the correct command should be `gradio run.py`, which is not mentioned in the response. Based on the score rubric, the response is mostly incorrect as it does not provide the correct information to complete the task."}, {"question": "What command is used to install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment?\n", "true_answer": "pip install \"transformers[sentencepiece]\"", "source_doc": "huggingface/course/blob/main/chapters/en/chapter0/1.mdx", "generated_answer": "The command to install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment is:\n\n```bash\npip install -e \".[dev]\"\n```\n\nThis should be run inside the virtual environment created with `python -m venv .env` and activated with `source .env/bin/activate` (or `.env\\Scripts\\activate` on Windows). If you encounter issues with the number of", "retrieved_docs": ["<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.\n\n</Tip>\n\nNow you can easily update your clone to the latest version of \ud83e\udd17 Transformers with the following command:\n\n```bash\ncd ~/transformers/\ngit pull\n```\n\nYour Python environment will find the `main` version of \ud83e\udd17 Transformers on the next run.\n\n## Install with conda\n\nInstall from the conda channel `huggingface`:\n\n```bash\nconda install -c huggingface transformers\n```\n\n## Cache setup\n\nPretrained models are downloaded and locally cached at: `~/.cache/huggingface/hub`. This is the default directory given by the shell environment variable `TRANSFORMERS_CACHE`. On Windows, the default directory is given by `C:\\Users\\username\\.cache\\huggingface\\hub`. You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:", "```bash\npython -m venv .env\n```\n\nActivate the virtual environment. On Linux and MacOs:\n\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n\n```bash\n.env/Scripts/activate\n```\n\nNow you're ready to install \ud83e\udd17 Transformers with the following command:\n\n```bash\npip install transformers\n```\n\nFor CPU-support only, you can conveniently install \ud83e\udd17 Transformers and a deep learning library in one line. For example, install \ud83e\udd17 Transformers and PyTorch with:\n\n```bash\npip install 'transformers[torch]'\n```\n\n\ud83e\udd17 Transformers and TensorFlow 2.0:\n\n```bash\npip install 'transformers[tf-cpu]'\n```\n\n<Tip warning={true}>\n\nM1 / ARM Users\n    \nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\n\n</Tip>\n\n\ud83e\udd17 Transformers and Flax:\n\n```bash\npip install 'transformers[flax]'\n```\n\nFinally, check if \ud83e\udd17 Transformers has been properly installed by running the following command. It will download a pretrained model:", "* Your **OS type and version** and **Python**, **PyTorch** and\n  **TensorFlow** versions when applicable.\n* A short, self-contained, code snippet that allows us to reproduce the bug in\n  less than 30s.\n* The *full* traceback if an exception is raised.\n* Attach any other additional information, like screenshots, you think may help.\n\nTo get the OS and software versions automatically, run the following command:\n\n```bash\ntransformers-cli env\n```\n\nYou can also run the same command from the root of the repository:\n\n```bash\npython src/transformers/commands/transformers_cli.py env\n```\n\n### Do you want a new feature?\n\nIf there is a new feature you'd like to see in \ud83e\udd17 Transformers, please open an issue and describe:\n\n1. What is the *motivation* behind this feature? Is it related to a problem or frustration with the library? Is it a feature related to something you need for a project? Is it something you worked on and think it could benefit the community?", "* Your **OS type and version** and **Python**, **PyTorch** and\n  **TensorFlow** versions when applicable.\n* A short, self-contained, code snippet that allows us to reproduce the bug in\n  less than 30s.\n* The *full* traceback if an exception is raised.\n* Attach any other additional information, like screenshots, you think may help.\n\nTo get the OS and software versions automatically, run the following command:\n\n```bash\ntransformers-cli env\n```\n\nYou can also run the same command from the root of the repository:\n\n```bash\npython src/transformers/commands/transformers_cli.py env\n```\n\n### Do you want a new feature?\n\nIf there is a new feature you'd like to see in \ud83e\udd17 Transformers, please open an issue and describe:\n\n1. What is the *motivation* behind this feature? Is it related to a problem or frustration with the library? Is it a feature related to something you need for a project? Is it something you worked on and think it could benefit the community?", "This installs a very light version of \ud83e\udd17 Transformers. In particular, no specific machine learning frameworks (like PyTorch or TensorFlow) are installed. Since we'll be using a lot of different features of the library, we recommend installing the development version, which comes with all the required dependencies for pretty much any imaginable use case:\n\n```\n!pip install transformers[sentencepiece]\n```\n\nThis will take a bit of time, but then you'll be ready to go for the rest of the course!\n\n## Using a Python virtual environment[[using-a-python-virtual-environment]]\n\nIf you prefer to use a Python virtual environment, the first step is to install Python on your system. We recommend following [this guide](https://realpython.com/installing-python/) to get started.", "2. Clone your `transformers` fork to your local disk, and add the base repository as a remote:\n\n```bash\ngit clone https://github.com/[your Github handle]/transformers.git\ncd transformers\ngit remote add upstream https://github.com/huggingface/transformers.git\n```\n\n3. Set up a development environment, for instance by running the following command:\n\n```bash\npython -m venv .env\nsource .env/bin/activate\npip install -e \".[dev]\"\n```\n\nDepending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\nfailure with this command. If that's the case make sure to install TensorFlow then do:\n\n```bash\npip install -e \".[quality]\"\n```\n\n**Note:** You don't need to have CUDA installed. Making the new model work on CPU is sufficient.\n\n4. Create a branch with a descriptive name from your main branch\n\n```bash\ngit checkout -b add_tf_brand_new_bert\n```\n\n5. Fetch and rebase to current main\n\n```bash\ngit fetch upstream\ngit rebase upstream/main\n```", "### 2. Next prepare your environment\n\n1.  Fork the [repository](https://github.com/huggingface/transformers)\n    by clicking on the 'Fork' button on the repository's page. This\n    creates a copy of the code under your GitHub user account.\n\n2.  Clone your `transformers` fork to your local disk, and add the base\n    repository as a remote:\n\n    ```bash\n    git clone https://github.com/[your Github handle]/transformers.git\n    cd transformers\n    git remote add upstream https://github.com/huggingface/transformers.git\n    ```\n\n3.  Set up a development environment, for instance by running the\n    following command:\n\n    ```bash\n    python -m venv .env\n    source .env/bin/activate\n    pip install -e \".[dev]\"\n    ```\n\nand return to the parent directory\n\n```bash\ncd ..\n```\n\n4.  We recommend adding the PyTorch version of *[camelcase name of model]* to\n    Transformers. To install PyTorch, please follow the instructions [here](https://pytorch.org/get-started/locally/)."], "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response provides a command that is generally used for installing a Python package in editable mode, which is a valid way to install development versions of packages. However, the response does not match the reference answer, which specifies the installation of the \ud83e\udd17 Transformers library with an additional dependency 'sentencepiece'. The response does not mention 'sentencepiece', which is a key part of the reference answer. Therefore, the response is somewhat correct in the context of installing a development version but lacks the specificity of the reference answer regarding the 'sentencepiece' dependency."}, {"question": "How can you install the Hugging Face Unity API in your Unity project?\n", "true_answer": "To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.", "source_doc": "huggingface/blog/blob/main/unity-api.md", "generated_answer": "To install the Hugging Face Unity API in your Unity project, follow these steps:\n\n1. Open your Unity project.\n2. Go to \"Window\" > \"Package Manager\".\n3. Click the \"+\" button and select \"Add Package from git URL\".\n4. Enter \"https://github.com/huggingface/unity-api.git\" and click \"Add\".\n5. Once the package is installed, the Hugging Face API", "retrieved_docs": ["--\ntitle: \"How to Install and Use the Hugging Face Unity API\"\nthumbnail: /blog/assets/124_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n\n# How to Install and Use the Hugging Face Unity API\n\n<!-- {authors} --> \n\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API.\n\n## Installation\n\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`", "## Usage Tips\n\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\n2. Address slow response times or performance issues by changing model endpoints to lower resource models.\n\n## Conclusion\n\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!", "<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif\">\n</figure> \n\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n7. Test the API key by clicking `Test API key` in the API Wizard.\n8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.\n9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.", "## Step 6: Optionally, Download the Hugging Face Unity WebGL Template\n\nYou can enhance your game's appearance in a Space by downloading the Hugging Face Unity WebGL template, available [here](https://github.com/huggingface/Unity-WebGL-template-for-Hugging-Face-Spaces). Just download the repository and drop it in your project files.\n\nThen, in the Player Settings panel, switch the WebGL template to Hugging Face. To do so, in Player Settings, click \"Resolution and Presentation\", then select the Hugging Face WebGL template.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/6.png\">\n</figure> \n\n## Step 7: Change the Compression Format to Disabled\n\nIn the Player Settings panel, navigate to the \"Publishing Settings\" section and change the Compression Format to \"Disabled\".", "/* other code */\n```\n\n## Supported Tasks and Custom Models\n\nThe Hugging Face Unity API also currently supports the following tasks:\n\n- [Conversation](https://huggingface.co/tasks/conversational)\n- [Text Generation](https://huggingface.co/tasks/text-generation)\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\n- [Text Classification](https://huggingface.co/tasks/text-classification)\n- [Question Answering](https://huggingface.co/tasks/question-answering)\n- [Translation](https://huggingface.co/tasks/translation)\n- [Summarization](https://huggingface.co/tasks/summarization)\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\n\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\n\nTo use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\n\n## Usage Tips", "### Prerequisites\n\nThis tutorial assumes basic knowledge of Unity. It also requires you to have installed the [Hugging Face Unity API](https://github.com/huggingface/unity-api). For instructions on setting up the API, check out our [earlier blog post](https://huggingface.co/blog/unity-api).\n\n## Steps\n\n### 1. Set up the Scene\n\nIn this tutorial, we'll set up a very simple scene where the player can start and stop a recording, and the result will be converted to text.\n\nBegin by creating a Unity project, then creating a Canvas with four UI elements:\n\n1. **Start Button**: This will start the recording.\n2. **Stop Button**: This will stop the recording.\n3. **Text (TextMeshPro)**: This is where the result of the speech recognition will be displayed.\n\n### 2. Set up the Script\n\nCreate a script called `SpeechRecognitionTest` and attach it to an empty GameObject.", "![gif](assets/47_sb3/spaceinvaders.gif)\n\n\ud83d\udc49 [The tutorial](https://github.com/huggingface/huggingface_sb3/blob/main/Stable_Baselines_3_and_Hugging_Face_%F0%9F%A4%97_tutorial.ipynb)\n\n\n### Conclusion\n\nWe're excited to see what you're working on with Stable-baselines3 and try your models in the Hub \ud83d\ude0d.\n\nAnd we would love to hear your feedback \ud83d\udc96. \ud83d\udce7 Feel free to [reach us](mailto:thomas.simonini@huggingface.co).\n\nFinally, we would like to thank the SB3 team and in particular [Antonin Raffin](https://araffin.github.io/) for their precious help for the integration of the library \ud83e\udd17.\n\n### Would you like to integrate your library to the Hub?\n\nThis integration is possible thanks to the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library which has all our widgets and the API for all our supported libraries. If you would like to integrate your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you!"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct, accurate, and factual. It follows the same steps as the reference answer, providing a clear and correct method to install the Hugging Face Unity API by using the Unity Package Manager and adding the package from a git URL. However, the response is incomplete as it ends abruptly without finishing the last sentence. This suggests that there might be additional steps or information that was intended to be included but was not. Therefore, the response cannot be scored as completely correct."}, {"question": "What is the pretraining objective of the Wav2Vec2 context network?\n", "true_answer": "The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks_explained.md", "generated_answer": "The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label). (Source: Extract 0)", "retrieved_docs": ["4. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).\n\nNow that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech recognition!\n\n### Audio classification\n\nTo use the pretrained model for audio classification, add a sequence classification head on top of the base Wav2Vec2 model. The classification head is a linear layer that accepts the encoder's hidden states. The hidden states represent the learned features from each audio frame which can have varying lengths. To create one vector of fixed-length, the hidden states are pooled first and then transformed into logits over the class labels. The cross-entropy loss is calculated between the logits and target to find the most likely class.", "Using a novel contrastive pretraining objective, Wav2Vec2 learns\npowerful speech representations from more than 50.000 hours of unlabeled\nspeech. Similar, to [BERT\\'s masked language\nmodeling](http://jalammar.github.io/illustrated-bert/), the model learns\ncontextualized speech representations by randomly masking feature\nvectors before passing them to a transformer network.\n\n![wav2vec2\\_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/wav2vec2.png)\n\nFor the first time, it has been shown that pretraining, followed by\nfine-tuning on very little labeled speech data achieves competitive\nresults to state-of-the-art ASR systems. Using as little as 10 minutes\nof labeled data, Wav2Vec2 yields a word error rate (WER) of less than 5%\non the clean test set of\n[LibriSpeech](https://huggingface.co/datasets/librispeech_asr) - *cf.*\nwith Table 9 of the [paper](https://arxiv.org/pdf/2006.11477.pdf).", "**Wav2Vec2** is a popular pre-trained model for speech recognition.\nReleased in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Meta AI Research, the novel architecture catalyzed progress in\nself-supervised pretraining for speech recognition, *e.g.* [*G. Ng et\nal.*, 2021](https://arxiv.org/pdf/2104.03416.pdf), [*Chen et al*,\n2021](https://arxiv.org/abs/2110.13900), [*Hsu et al.*,\n2021](https://arxiv.org/abs/2106.07447) and [*Babu et al.*,\n2021](https://arxiv.org/abs/2111.09296). On the Hugging Face Hub,\nWav2Vec2's most popular pre-trained checkpoint currently amounts to\nover [**250,000** monthly\ndownloads](https://huggingface.co/facebook/wav2vec2-base-960h).\n\nUsing Connectionist Temporal Classification (CTC), pre-trained\nWav2Vec2-like checkpoints are extremely easy to fine-tune on downstream\nspeech recognition tasks. In a nutshell, fine-tuning pre-trained\nWav2Vec2 checkpoints works as follows:", "In this notebook, we will give an in-detail explanation of how\nWav2Vec2\\'s pretrained checkpoints can be fine-tuned on any English ASR\ndataset. Note that in this notebook, we will fine-tune Wav2Vec2 without\nmaking use of a language model. It is much simpler to use Wav2Vec2\nwithout a language model as an end-to-end ASR system and it has been\nshown that a standalone Wav2Vec2 acoustic model achieves impressive\nresults. For demonstration purposes, we fine-tune the \\\"base\\\"-sized\n[pretrained checkpoint](https://huggingface.co/facebook/wav2vec2-base)\non the rather small [Timit](https://huggingface.co/datasets/timit_asr)\ndataset that contains just 5h of training data.\n\nWav2Vec2 is fine-tuned using Connectionist Temporal Classification\n(CTC), which is an algorithm that is used to train neural networks for\nsequence-to-sequence problems and mainly in Automatic Speech Recognition\nand handwriting recognition.", "**Wav2Vec2** is a popular pre-trained model for speech recognition.\nReleased in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Meta AI Research, the novel architecture catalyzed progress in\nself-supervised pretraining for speech recognition, *e.g.* [*G. Ng et\nal.*, 2021](https://arxiv.org/pdf/2104.03416.pdf), [*Chen et al*,\n2021](https://arxiv.org/abs/2110.13900), [*Hsu et al.*,\n2021](https://arxiv.org/abs/2106.07447) and [*Babu et al.*,\n2021](https://arxiv.org/abs/2111.09296). On the Hugging Face Hub,\nWav2Vec2's most popular pre-trained checkpoint currently amounts to\nover [**250,000** monthly\ndownloads](https://huggingface.co/facebook/wav2vec2-base-960h).", "### Pretraining Wav2Vec2\n\nThe `run_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss objective (see official [paper](https://arxiv.org/abs/2006.11477) for more information). \nIt is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed (please refer to [this guide](https://huggingface.co/transformers/main/main_classes/deepspeed.html#deepspeed-trainer-integration) for more information).\n\nHere is an example of how you can use DeepSpeed ZeRO-2 to pretrain a small Wav2Vec2 model:", "The abstract from the paper is the following:\n\n*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\nrecognition with limited amounts of labeled data.*"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns well with the reference answer, providing a clear explanation of the pretraining objective of the Wav2Vec2 context network. The response adds a bit more detail by mentioning the encouragement for the model to find the most similar context vector and quantized speech unit, which is the target label. This additional information is consistent with the nature of the contrastive task and does not detract from the correctness or factual accuracy of the answer."}, {"question": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n", "true_answer": "distilbert base uncased finetuned sst2 english", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md", "generated_answer": "The default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst-2-english\". This information can be found in the context provided, specifically in the code snippet for using the pipeline for sentiment analysis:\n\n```python\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(\"sentiment-analysis\")\n```\n\nThe pipeline is automatically downloaded", "retrieved_docs": ["```py\nfrom transformers import pipeline\n\n# Replace this with your own checkpoint\nmodel_checkpoint = \"huggingface-course/bert-finetuned-ner\"\ntoken_classifier = pipeline(\n    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n)\ntoken_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n```\n\n```python out\n[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},\n {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},\n {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n```\n\nGreat! Our model is working as well as the default one for this pipeline!", "## The Transformers library\nI was a bit confused at the beginning when I downloaded the .h5 file. I thought it would be compatible with `tensorflow.keras.models.load_model`, but this wasn't the case. After a few minutes of research I was able to figure out that the file was a weights checkpoint rather than a Keras model.\nAfter that, I tried out the API that Hugging Face offers and read a bit more about the pipeline feature they offer. Since the results of the API & the pipeline were great, I decided that I could serve the model through the pipeline on my own server.\n\nBelow is the [official example](https://github.com/huggingface/transformers#quick-tour) from the Transformers GitHub page.\n\n```python\nfrom transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('We are very happy to include pipeline into the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9978193640708923}]\n```", "## \u0c24\u0c4d\u0c35\u0c30\u0c3f\u0c24 \u0c2a\u0c30\u0c4d\u0c2f\u0c1f\u0c28\n\n\u0c07\u0c1a\u0c4d\u0c1a\u0c3f\u0c28 \u0c07\u0c28\u0c4d\u200c\u0c2a\u0c41\u0c1f\u0c4d (\u0c1f\u0c46\u0c15\u0c4d\u0c38\u0c4d\u0c1f\u0c4d, \u0c07\u0c2e\u0c47\u0c1c\u0c4d, \u0c06\u0c21\u0c3f\u0c2f\u0c4b, ...)\u0c2a\u0c48 \u0c24\u0c15\u0c4d\u0c37\u0c23\u0c2e\u0c47 \u0c2e\u0c4b\u0c21\u0c32\u0c4d\u200c\u0c28\u0c41 \u0c09\u0c2a\u0c2f\u0c4b\u0c17\u0c3f\u0c02\u0c1a\u0c21\u0c3e\u0c28\u0c3f\u0c15\u0c3f, \u0c2e\u0c47\u0c2e\u0c41 `pipeline` API \u0c28\u0c3f \u0c05\u0c02\u0c26\u0c3f\u0c38\u0c4d\u0c24\u0c3e\u0c2e\u0c41. \u0c2a\u0c48\u0c2a\u0c4d\u200c\u0c32\u0c48\u0c28\u0c4d\u200c\u0c32\u0c41 \u0c06 \u0c2e\u0c4b\u0c21\u0c32\u0c4d \u0c36\u0c3f\u0c15\u0c4d\u0c37\u0c23 \u0c38\u0c2e\u0c2f\u0c02\u0c32\u0c4b \u0c09\u0c2a\u0c2f\u0c4b\u0c17\u0c3f\u0c02\u0c1a\u0c3f\u0c28 \u0c2a\u0c4d\u0c30\u0c40\u0c2a\u0c4d\u0c30\u0c3e\u0c38\u0c46\u0c38\u0c3f\u0c02\u0c17\u0c4d\u200c\u0c24\u0c4b \u0c15\u0c42\u0c21\u0c3f\u0c28 \u0c2a\u0c4d\u0c30\u0c40\u0c1f\u0c4d\u0c30\u0c48\u0c28\u0c4d\u0c21\u0c4d \u0c2e\u0c4b\u0c21\u0c32\u0c4d\u200c\u0c28\u0c41 \u0c38\u0c2e\u0c42\u0c39\u0c2a\u0c30\u0c41\u0c38\u0c4d\u0c24\u0c3e\u0c2f\u0c3f. \u0c38\u0c3e\u0c28\u0c41\u0c15\u0c42\u0c32 \u0c2e\u0c30\u0c3f\u0c2f\u0c41 \u0c2a\u0c4d\u0c30\u0c24\u0c3f\u0c15\u0c42\u0c32 \u0c2a\u0c3e\u0c20\u0c3e\u0c32\u0c28\u0c41 \u0c35\u0c30\u0c4d\u0c17\u0c40\u0c15\u0c30\u0c3f\u0c02\u0c1a\u0c21\u0c3e\u0c28\u0c3f\u0c15\u0c3f \u0c2a\u0c48\u0c2a\u0c4d\u200c\u0c32\u0c48\u0c28\u0c4d\u200c\u0c28\u0c41 \u0c24\u0c4d\u0c35\u0c30\u0c17\u0c3e \u0c0e\u0c32\u0c3e \u0c09\u0c2a\u0c2f\u0c4b\u0c17\u0c3f\u0c02\u0c1a\u0c3e\u0c32\u0c4b \u0c07\u0c15\u0c4d\u0c15\u0c21 \u0c09\u0c02\u0c26\u0c3f:\n\n```python\n>>> from transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('We are very happy to introduce pipeline to the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```\n\n\u0c30\u0c46\u0c02\u0c21\u0c35 \u0c32\u0c48\u0c28\u0c4d \u0c15\u0c4b\u0c21\u0c4d \u0c21\u0c4c\u0c28\u0c4d\u200c\u0c32\u0c4b\u0c21\u0c4d \u0c2e\u0c30\u0c3f\u0c2f\u0c41 \u0c2a\u0c48\u0c2a\u0c4d\u200c\u0c32\u0c48\u0c28\u0c4d \u0c09\u0c2a\u0c2f\u0c4b\u0c17\u0c3f\u0c02\u0c1a\u0c47 \u0c2a\u0c4d\u0c30\u0c40\u0c1f\u0c4d\u0c30\u0c48\u0c28\u0c4d\u0c21\u0c4d \u0c2e\u0c4b\u0c21\u0c32\u0c4d\u200c\u0c28\u0c41 \u0c15\u0c3e\u0c37\u0c4d \u0c1a\u0c47\u0c38\u0c4d\u0c24\u0c41\u0c02\u0c26\u0c3f, \u0c2e\u0c42\u0c21\u0c35\u0c26\u0c3f \u0c07\u0c1a\u0c4d\u0c1a\u0c3f\u0c28 \u0c1f\u0c46\u0c15\u0c4d\u0c38\u0c4d\u0c1f\u0c4d\u200c\u0c2a\u0c48 \u0c2e\u0c42\u0c32\u0c4d\u0c2f\u0c3e\u0c02\u0c15\u0c28\u0c02 \u0c1a\u0c47\u0c38\u0c4d\u0c24\u0c41\u0c02\u0c26\u0c3f. \u0c07\u0c15\u0c4d\u0c15\u0c21 \u0c38\u0c2e\u0c3e\u0c27\u0c3e\u0c28\u0c02 99.97% \u0c35\u0c3f\u0c36\u0c4d\u0c35\u0c3e\u0c38\u0c02\u0c24\u0c4b \"\u0c2a\u0c3e\u0c1c\u0c3f\u0c1f\u0c3f\u0c35\u0c4d\".\n\n\u0c1a\u0c3e\u0c32\u0c3e \u0c2a\u0c28\u0c41\u0c32\u0c41 NLP\u0c32\u0c4b \u0c15\u0c3e\u0c28\u0c40 \u0c15\u0c02\u0c2a\u0c4d\u0c2f\u0c42\u0c1f\u0c30\u0c4d \u0c35\u0c3f\u0c1c\u0c28\u0c4d \u0c2e\u0c30\u0c3f\u0c2f\u0c41 \u0c38\u0c4d\u0c2a\u0c40\u0c1a\u0c4d\u200c\u0c32\u0c4b \u0c15\u0c42\u0c21\u0c3e \u0c2e\u0c41\u0c02\u0c26\u0c41\u0c17\u0c3e \u0c36\u0c3f\u0c15\u0c4d\u0c37\u0c23 \u0c2a\u0c4a\u0c02\u0c26\u0c3f\u0c28 `pipeline` \u0c38\u0c3f\u0c26\u0c4d\u0c27\u0c02\u0c17\u0c3e \u0c09\u0c28\u0c4d\u0c28\u0c3e\u0c2f\u0c3f. \u0c09\u0c26\u0c3e\u0c39\u0c30\u0c23\u0c15\u0c41, \u0c2e\u0c28\u0c02 \u0c1a\u0c3f\u0c24\u0c4d\u0c30\u0c02\u0c32\u0c4b \u0c17\u0c41\u0c30\u0c4d\u0c24\u0c3f\u0c02\u0c1a\u0c3f\u0c28 \u0c35\u0c38\u0c4d\u0c24\u0c41\u0c35\u0c41\u0c32\u0c28\u0c41 \u0c38\u0c41\u0c32\u0c2d\u0c02\u0c17\u0c3e \u0c38\u0c02\u0c17\u0c4d\u0c30\u0c39\u0c3f\u0c02\u0c1a\u0c35\u0c1a\u0c4d\u0c1a\u0c41:", "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\ntokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\noutput = model(**tokens)\n```\n{:else}\n```py\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\ntokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\noutput = model(**tokens)\n```\n{/if}", "## Using a model for question answering[[using-a-model-for-question-answering]]\n\nLike with any other pipeline, we start by tokenizing our input and then send it through the model. The checkpoint used by default for the `question-answering` pipeline is [`distilbert-base-cased-distilled-squad`](https://huggingface.co/distilbert-base-cased-distilled-squad) (the \"squad\" in the name comes from the dataset on which the model was fine-tuned; we'll talk more about the SQuAD dataset in [Chapter 7](/course/chapter7/7)):\n\n{#if fw === 'pt'}\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\nmodel_checkpoint = \"distilbert-base-cased-distilled-squad\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n\ninputs = tokenizer(question, context, return_tensors=\"pt\")\noutputs = model(**inputs)\n```\n\n{:else}\n\n```py\nfrom transformers import AutoTokenizer, TFAutoModelForQuestionAnswering", "Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this guide, you'll use the [`pipeline`] for sentiment analysis as an example:\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(\"sentiment-analysis\")\n```\n\nThe [`pipeline`] downloads and caches a default [pretrained model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) and tokenizer for sentiment analysis. Now you can use the `classifier` on your target text:\n\n```py\n>>> classifier(\"We are very happy to show you the \ud83e\udd17 Transformers library.\")\n[{'label': 'POSITIVE', 'score': 0.9998}]\n```\n\nIf you have more than one input, pass your inputs as a list to the [`pipeline`] to return a list of dictionaries:", "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\nsequence = \"I've been waiting for a HuggingFace course my whole life.\"\n\ntokens = tokenizer.tokenize(sequence)\nids = tokenizer.convert_tokens_to_ids(tokens)\ninput_ids = torch.tensor(ids)\n# This line will fail.\nmodel(input_ids)\n```\n\n```python out\nIndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n```\n{:else}\n```py\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n\nsequence = \"I've been waiting for a HuggingFace course my whole life.\""], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer and provides additional context by showing the code snippet used to invoke the sentiment analysis pipeline, which implicitly uses the default checkpoint \"distilbert-base-uncased-finetuned-sst-2-english\"."}, {"question": "What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n", "true_answer": "To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.", "source_doc": "huggingface/optimum/blob/main/docs/source/notebooks.md", "generated_answer": "The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to demonstrate how to utilize DeepSpeed, a library designed for speed and scale in distributed training of large models with billions of parameters, to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi, a high-performance computing (HPC)", "retrieved_docs": ["| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |", "| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |", "Papers:\n\n- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)\n- [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)\n- [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)\n\nFinally, please, remember that, HuggingFace [`Trainer`] only integrates DeepSpeed, therefore if you\nhave any problems or questions with regards to DeepSpeed usage, please, file an issue with [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues).", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# DeepSpeed\n\n[DeepSpeed](https://www.deepspeed.ai/) is a library designed for speed and scale for distributed training of large models with billions of parameters. At its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes. This drastically reduces memory usage, allowing you to scale your training to billion parameter models. To unlock even more memory efficiency, ZeRO-Offload reduces GPU compute and memory by leveraging CPU resources during optimization.", "* Paper: [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054). The paper is very interesting, but it's very terse.\n* Here is a good [video discussion](https://www.youtube.com/watch?v=tC01FRB0M7w) of the paper with visuals\n* Paper: [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840). Just published - this one goes into the details of ZeRO Offload feature.\n* DeepSpeed [configuration and tutorials](https://www.deepspeed.ai/getting-started/)\n* In addition to the paper, I highly recommend to read the following detailed blog posts with diagrams:\n  - [DeepSpeed: Extreme-scale model training for everyone]( https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)", "--\ntitle: \"Accelerate Large Model Training using DeepSpeed\"\nthumbnail: /blog/assets/83_accelerate_deepspeed/deepspeed-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n\n# Accelerate Large Model Training using DeepSpeed\n\n\nIn this post we will look at how we can leverage the **[Accelerate](https://github.com/huggingface/accelerate)** library for training large models which enables users to leverage the ZeRO features of **[DeeSpeed](https://www.deepspeed.ai)**.\n\n# Motivation \ud83e\udd17\n**Tired of Out of Memory (OOM) errors while trying to train large models? We've got you covered. Large models are very performant [1] but difficult to train with the available hardware. To get the most of the available hardware for training large models one can leverage Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]**.", "To go further, we are looking forward to using HPU graphs for training models even faster and to presenting how to use DeepSpeed ZeRO-3 on Gaudi2 to accelerate the training of your LLMs. Stay tuned!\n\nIf you are interested in accelerating your Machine Learning training and inference workflows using the latest AI hardware accelerators and software libraries, check out our [Expert Acceleration Program](https://huggingface.co/support). To learn more about Habana solutions, [read about our partnership and contact them here](https://huggingface.co/hardware/habana). To learn more about Hugging Face efforts to make AI hardware accelerators easy to use, check out our [Hardware Partner Program](https://huggingface.co/hardware).\n\n\n### Related Topics"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It accurately mentions the use of DeepSpeed in conjunction with Habana Gaudi for training large models and specifies the model in question as the 1.6B-parameter GPT2-XL. However, it includes additional context about DeepSpeed being designed for speed and scale in distributed training, which, while relevant, is not necessary to fulfill the purpose of the notebook as per the reference answer. The response could be more concise to match the reference answer more closely."}, {"question": "What command line module does PyTorch provide to run a script on multiple GPUs?\n", "true_answer": "torchrun", "source_doc": "huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md", "generated_answer": "PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. It allows for distributed training and inference by defining setup configurations and passing arguments such as `--nproc_per_node` and `--nnodes`. This module is part of PyTorch's `torch.distributed` library, which is designed to work on distributed setups with multiple nodes, machines, or GPUs. Alternatively, the `accelerate`", "retrieved_docs": ["However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through *distributed* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\n\n## PyTorch Distributed Data Parallelism\n\nAs the name implies, `torch.distributed` is meant to work on *distributed* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both.\n\nTo convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)", "```bash\nwget https://raw.githubusercontent.com/huggingface/transformers/main/scripts/distributed/torch-distributed-gpu-test.py\n```\n\nFor example to test how 2 GPUs interact do:\n\n```bash\npython -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py\n```\nIf both processes can talk to each and allocate GPU memory each will print an OK status.\n\nFor more GPUs or nodes adjust the arguments in the script.\n\nYou will find a lot more details inside the diagnostics script and even a recipe to how you could run it in a SLURM environment.\n\nAn additional level of debug is to add `NCCL_DEBUG=INFO` environment variable as follows:\n\n```bash\nNCCL_DEBUG=INFO python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py\n```\n\nThis will dump a lot of NCCL-related debug information, which you can then search online if you find that some problems are reported. Or if you're not sure how to interpret the output you can share the log file in an Issue.", "```bash\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\n```\n\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\n\nNow let's talk about Accelerate, a library aimed to make this process more seameless and also help with a few best practices\n\n## \ud83e\udd17 Accelerate\n\n[Accelerate](https://huggingface.co/docs/accelerate) is a library designed to allow you to perform what we just did above, without needing to modify your code greatly. On top of this, the data pipeline innate to Accelerate can also improve performance to your code as well.\n\nFirst, let's wrap all of the above code we just performed into a single function, to help us visualize the difference:\n\n```python\ndef train_ddp(rank, world_size):\n    setup(rank, world_size)\n    # Build DataLoaders\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307), (0.3081))\n    ])", "```py\n>>> from multiprocess import set_start_method\n>>> import torch\n>>> import os\n>>>\n>>> for i in range(torch.cuda.device_count()):  # send model to every GPU\n...     model.to(f\"cuda:{i}\")\n>>>\n>>> def gpu_computation(example, rank):\n...     torch.cuda.set_device(f\"cuda:{rank}\")  # use one GPU\n...     # Your big GPU call goes here, for example\n...     inputs = tokenizer(texts, truncation=True, return_tensors=\"pt\").to(f\"cuda:{rank}\")\n...     outputs = model.generate(**inputs)\n...     example[\"generated_text\"] = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n...     return example\n>>>\n>>> if __name__ == \"__main__\":\n...     set_start_method(\"spawn\")\n...     updated_dataset = dataset.map(gpu_computation, with_rank=True, num_proc=torch.cuda.device_count())\n```\n\nThe main use-case for rank is to parallelize computation across several GPUs. This requires setting `multiprocess.set_start_method(\"spawn\")`. If you don't you'll receive the following CUDA error:", "Now we can fit very large models into a single GPU, but the training might still be very slow. The simplest strategy in this scenario is data parallelism: we replicate the same training setup into separate GPUs and pass different batches to each GPU. With this, you can parallelize the forward/backward passes of the model and scale with the number of GPUs. \n\n![chapter10_ddp.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/chapter10_ddp.png)\n\nWe use either the `transformers.Trainer` or `accelerate`, which both support data parallelism without any code changes, by simply passing arguments when calling the scripts with `torchrun` or `accelerate launch`. The following runs a training script with 8 GPUs on a single machine with `accelerate` and `torchrun`, respectively.\n\n```bash\naccelerate launch --multi_gpu --num_machines 1  --num_processes 8 my_accelerate_script.py\ntorchrun --nnodes 1  --nproc_per_node 8 my_torch_script.py\n```", "### Script to compare pre-training with PyTorch on 8 GPU V100's\n\nFor comparison you can run the same pre-training with PyTorch on GPU. Note that we have to make use of `gradient_accumulation` \nbecause the maximum batch size that fits on a single V100 GPU is 32 instead of 128.\nHaving created the tokenzier and configuration in `norwegian-roberta-base`, we create the following symbolic links:\n\n```bash\nln -s ~/transformers/examples/pytorch/language-modeling/run_mlm.py ./\n```\n\n, set some environment variables:\n\n```bash\nexport NUM_GPUS=8\nexport TOKENIZERS_PARALLELISM=0\nexport MODEL_DIR=\"./norwegian-roberta-base\"\nmkdir -p ${MODEL_DIR}\n```\n\n, and can start training as follows:", "image = sd(prompt).images[0]\n    image.save(f\"./{'_'.join(prompt)}.png\")\n```\n\nTo run the distributed inference, call [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn) to run the `run_inference` function on the number of GPUs defined in `world_size`:\n\n```py\ndef main():\n    world_size = 2\n    mp.spawn(run_inference, args=(world_size,), nprocs=world_size, join=True)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\nOnce you've completed the inference script, use the `--nproc_per_node` argument to specify the number of GPUs to use and call `torchrun` to run the script:\n\n```bash\ntorchrun run_distributed.py --nproc_per_node=2\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies `torchrun` as the command line module provided by PyTorch to run a script on multiple GPUs. It also accurately describes the functionality of the module, mentioning distributed training and inference, and the use of arguments such as `--nproc_per_node` and `--nnodes`. The information provided is factual and aligns with the reference answer, which simply states \"torchrun\". The response goes beyond the reference answer by providing additional context and information about the module's capabilities and its place within the `torch.distributed` library. However, the response is cut off and ends with \"Alternatively, the `accelerate`\", which seems to be an incomplete sentence. This does not detract from the fact that the response correctly and factually identifies the command line module in question. Based on the score rubric, the response is completely correct, accurate, and factual.\n\nFeedback: The response accurately identifies `torchrun` as the PyTorch command line module for running scripts on multiple GPUs and provides additional factual details about its usage and capabilities."}, {"question": "What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n", "true_answer": "google/vit-base-patch16-224", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-with-vision-transformers.md", "generated_answer": "Answer: The most popular vision transformer model on the Hugging Face Model Hub for image classification at the time of writing is `google/vit-base-patch16-224`.", "retrieved_docs": ["1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.", "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.", "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.", "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.", "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.", "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (\u6765\u81ea Google AI) \u4f34\u968f\u8bba\u6587 [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) \u7531 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby \u53d1\u5e03\u3002\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (\u6765\u81ea UCLA NLP) \u4f34\u968f\u8bba\u6587 [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) \u7531 Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang \u53d1\u5e03\u3002", "Let's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started).\n\n## Step 1 \u2014 Choosing a Vision Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.\n\nExpand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n\nAt the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.\n\n## Step 2 \u2014 Loading the Vision Transformer Model with Gradio"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, indicating that the information given is what was requested in the instruction."}, {"question": "What is the command to upload an ESPnet model to a Hugging Face repository?\n", "true_answer": "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/espnet.md", "generated_answer": "The command to upload an ESPnet model to a Hugging Face repository is:\n\n```bash\nmlagents-push-to-hf --run-id=\"First Training\" --local-dir=\"results/First Training\" --repo-id=\"ThomasSimonini/ESPnet-Model\" --commit-message=\"ESPnet Model\"\n```\n\nReplace \"First Training\" with the name of your training run ID, \"results/First", "retrieved_docs": ["The `run.sh` script allows to upload a given model to a Hugging Face repository.\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\n## Additional resources\n\n* ESPnet [docs](https://espnet.github.io/espnet/index.html).\n* ESPnet model zoo [repository](https://github.com/espnet/espnet_model_zoo).\n* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md).", "```py\nimport soundfile\nfrom espnet2.bin.tts_inference import Text2Speech\n\ntext2speech = Text2Speech.from_pretrained(\"model_name\")\nspeech = text2speech(\"foobar\")[\"wav\"]\nsoundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM_16\")\n```\n\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png\"/>\n</div>\n\n## Sharing your models\n\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model)).", "Using ESPnet at Hugging Face\n\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\n\n## Exploring ESPnet in the Hub\n\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). \n\nAll models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more.\n2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\n3. An interactive widget you can use to play out with the model directly in the browser.\n4. An Inference API that allows to make inference requests.", "## Sharing your models\n\nYou can easily upload your models using `mlagents-push-to-hf`:\n\n```\nmlagents-push-to-hf --run-id=\"First Training\" --local-dir=\"results/First Training\" --repo-id=\"ThomasSimonini/MLAgents-Pyramids\" --commit-message=\"Pyramids\"\n```\n\nYou need to define four parameters:\n- `--run-id`: the name of the training run id.\n- `--local-dir`: where the model was saved.\n- `--repo-id`: the name of the Hugging Face repo you want to create or update. It\u2019s `<your huggingface username>/<the repo name>`.\n- `--commit-message`.\n\n\n## Additional resources\n\n* ML-Agents [documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Hugging-Face-Integration.md)\n* Official Unity ML-Agents Spaces [demos](https://huggingface.co/unity)", "<Youtube id=\"DQ-CpJn6Rc4\"/>\n\nWe've prepared a [template model repository](https://huggingface.co/lewtun/distilbert-base-uncased-finetuned-squad-d5716d28) for this section, and if you want to run the code in this chapter you'll first need to copy the model to your account on the [Hugging Face Hub](https://huggingface.co). To do so, first log in by running either the following in a Jupyter notebook:\n\n```python\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\nor the following in your favorite terminal:\n\n```bash\nhuggingface-cli login\n```\n\nThis will prompt you to enter your username and password, and will save a token under *~/.cache/huggingface/*. Once you've logged in, you can copy the template repository with the following function:\n\n```python\nfrom distutils.dir_util import copy_tree\nfrom huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name", "```py\n>>> tokenizer.push_to_hub(\"my-awesome-model\")\n```\n\nOr perhaps you'd like to add the TensorFlow version of your fine-tuned PyTorch model:\n\n```py\n>>> tf_model.push_to_hub(\"my-awesome-model\")\n```\n\nNow when you navigate to your Hugging Face profile, you should see your newly created model repository. Clicking on the **Files** tab will display all the files you've uploaded to the repository.\n\nFor more details on how to create and upload files to a repository, refer to the Hub documentation [here](https://huggingface.co/docs/hub/how-to-upstream).\n\n## Upload with the web interface\n\nUsers who prefer a no-code approach are able to upload a model through the Hub's web interface. Visit [huggingface.co/new](https://huggingface.co/new) to create a new repository:\n\n![new_model_repo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png)\n\nFrom here, add some information about your model:", "```bash\nsudo apt install tesseract-ocr\npip install -q pytesseract\n```\n\nOnce you have installed all of the dependencies, restart your runtime.\n\nWe encourage you to share your model with the community. Log in to your Hugging Face account to upload it to the \ud83e\udd17 Hub.\nWhen prompted, enter your token to log in:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\nLet's define some global variables.\n\n```py\n>>> model_checkpoint = \"microsoft/layoutlmv2-base-uncased\"\n>>> batch_size = 4\n```\n\n## Load the data\n\nIn this guide we use a small sample of preprocessed DocVQA that you can find on \ud83e\udd17 Hub. If you'd like to use the full\nDocVQA dataset, you can register and download it on [DocVQA homepage](https://rrc.cvc.uab.es/?ch=17). If you do so, to\nproceed with this guide check out [how to load files into a \ud83e\udd17 dataset](https://huggingface.co/docs/datasets/loading#local-and-remote-files).\n\n```py\n>>> from datasets import load_dataset"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response provided includes a command that is not related to ESPnet or Hugging Face, but rather to the ML-Agents toolkit. The command structure and parameters do not match the reference answer, which is specific to ESPnet and Hugging Face. The response fails to provide the correct command for uploading an ESPnet model to a Hugging Face repository, as it incorrectly references ML-Agents and uses parameters that are not applicable to the task at hand. Therefore, the response does not meet the criteria for correctness, accuracy, or factualness in relation to the reference answer."}, {"question": "What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n", "true_answer": "requirements.txt", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/custom_dependencies.mdx", "generated_answer": "The file that should be added to a model repository to install custom Python dependencies for Inference Endpoints is a `requirements.txt` file. (Source: Extract 0)", "retrieved_docs": ["Add custom Dependencies\n\nInference Endpoints\u2019 base image includes all required libraries to run inference on \ud83e\udd17 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\n* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\n* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).\n\nTo add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within.\n\n```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```", "# create a Repository instance\nmodel_repo = Repository(use_auth_token=token,\n                        clone_from=repo_url,\n                        local_dir=local_path,\n                        git_user=hf_username,\n                        git_email=hf_email)\n\n# push model to the hub\nmodel_repo.push_to_hub()\n```\n\n---\n\n## Test inference\n\nAfter we uploaded our model we can access it at `https://huggingface.co/{hf_username}/{repository_name}` \n\n```python\nprint(f\"https://huggingface.co/{hf_username}/{repository_name}\")\n```\n\nAnd use the \"Hosted Inference API\" widget to test it. \n\n[https://huggingface.co/philschmid/bart-large-cnn-samsum](https://huggingface.co/philschmid/bart-large-cnn-samsum)\n\n![inference](assets/19_sagemaker_distributed_training_seq2seq/inference-test.png)", "## User defined code and modules\n\nThe Hugging Face Inference Toolkit allows the user to override the default methods of the `HuggingFaceHandlerService`. You will need to create a folder named `code/` with an `inference.py` file in it. See [here](#create-a-model-artifact-for-deployment) for more details on how to archive your model artifacts. For example:  \n\n```bash\nmodel.tar.gz/\n|- pytorch_model.bin\n|- ....\n|- code/\n  |- inference.py\n  |- requirements.txt \n```\n\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:", "## Core ML Inference in Python\n\n### Prerequisites\n\n```bash\npip install huggingface_hub\npip install git+https://github.com/apple/ml-stable-diffusion\n```\n\n### Download the Model Checkpoints\n\nTo run inference in Python, you have to use one of the versions stored in the `packages` folders, because the compiled ones are only compatible with Swift. You may choose whether you want to use the `original` or `split_einsum` attention styles.\n\nThis is how you'd download the `original` attention variant from the Hub:\n\n```Python\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nrepo_id = \"apple/coreml-stable-diffusion-v1-4\"\nvariant = \"original/packages\"\n\nmodel_path = Path(\"./models\") / (repo_id.split(\"/\")[-1] + \"_\" + variant.replace(\"/\", \"_\"))\nsnapshot_download(repo_id, allow_patterns=f\"{variant}/*\", local_dir=model_path, local_dir_use_symlinks=False)\nprint(f\"Model downloaded at {model_path}\")\n```", "```python\n# copy inference.py into the code/ directory of the model directory.\n!cp -r code/ tmp/code/\n# create a model.tar.gz archive with all the model artifacts and the inference.py script.\n%cd tmp\n!tar zcvf model.tar.gz *\n%cd ..\n```\n\nNow we can upload our `model.tar.gz` to our session S3 bucket with `sagemaker`.\n\n```python\nfrom sagemaker.s3 import S3Uploader\n\n# create s3 uri\ns3_model_path = f\"s3://{sess.default_bucket()}/{model_id}\"\n\n# upload model.tar.gz\ns3_model_uri = S3Uploader.upload(local_path=\"tmp/model.tar.gz\",desired_s3_uri=s3_model_path)\nprint(f\"model artifcats uploaded to {s3_model_uri}\")\n```\n\n## 4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n\nAfter we have uploaded our\u00a0`model.tar.gz`\u00a0to Amazon S3 can we create a custom\u00a0`HuggingfaceModel`. This class will be used to create and deploy our real-time inference endpoint on Amazon SageMaker.\n\n```python\nfrom sagemaker.huggingface.model import HuggingFaceModel", "## Other considerations\n\n### Deployment Options\n\nCurrently you can deploy an Inference Endpoint from the [GUI](https://ui.endpoints.huggingface.co/new) or using a [RESTful API](https://huggingface.co/docs/inference-endpoints/api_reference). You can also make use of our command line tool [hugie](https://github.com/MantisAI/hfie) (which will be the subject of a future blog) to launch Inference Endpoints in one line of code by passing a configuration, it\u2019s really this simple:\n\n```bash\nhugie endpoint create example/development.json\n```", "```python\n# Start an Inference Endpoint running Zephyr-7b-beta on TGI\n>>> from huggingface_hub import create_inference_endpoint\n>>> endpoint = create_inference_endpoint(\n...     \"aws-zephyr-7b-beta-0486\",\n...     repository=\"HuggingFaceH4/zephyr-7b-beta\",\n...     framework=\"pytorch\",\n...     task=\"text-generation\",\n...     accelerator=\"gpu\",\n...     vendor=\"aws\",\n...     region=\"us-east-1\",\n...     type=\"protected\",\n...     instance_size=\"medium\",\n...     instance_type=\"g5.2xlarge\",\n...     custom_image={\n...         \"health_route\": \"/health\",\n...         \"env\": {\n...             \"MAX_BATCH_PREFILL_TOKENS\": \"2048\",\n...             \"MAX_INPUT_LENGTH\": \"1024\",\n...             \"MAX_TOTAL_TOKENS\": \"1512\",\n...             \"MODEL_ID\": \"/repository\"\n...         },\n...         \"url\": \"ghcr.io/huggingface/text-generation-inference:1.1.0\",\n...     },\n... )\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that a `requirements.txt` file should be added to a model repository for installing custom Python dependencies for Inference Endpoints."}, {"question": "How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n", "true_answer": "3-5 images", "source_doc": "huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md", "generated_answer": "According to the context provided, Textual Inversion allows teaching new concepts to Stable Diffusion using just 3-5 sample images. (Source: Extract 1) Therefore, the answer to the question is 3-5 images.", "retrieved_docs": ["# Textual inversion\n\n[[open-in-colab]]\n\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer).\n\nThis guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text_inversion) training guide.\n\nImport the necessary libraries:\n\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import make_image_grid\n```", "## Textual Inversion\n\nTextual Inversion lets you personalize a Stable Diffusion model on your own images with just 3-5 samples. With this tool, you can train a model on a concept, and then share the concept with the rest of the community! \n\n![image info](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/textual_inversion.jpeg)\n\nIn just a couple of days, the community shared over 200 concepts! Check them out!", "Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare Datasets", "Using the pretrained models we can provide control images (for example, a depth map) to control Stable Diffusion text-to-image generation so that it follows the structure of the depth image and fills in the details.\n\nThe abstract of the paper is the following:", "## Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that 3-5 images are needed to teach new concepts to Stable Diffusion using Textual Inversion."}, {"question": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n", "true_answer": "10GB", "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md", "generated_answer": "The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB. This feature was introduced in version 4.18.0 to handle the issue of large model checkpoints taking up too much storage space. If a checkpoint exceeds 10GB, it will be automatically sharded into smaller pieces with a maximum size specified by the user with the `max_shard_size` parameter", "retrieved_docs": ["</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```", "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)", "We call the checkpoints saved in several files like BLOOM *sharded checkpoints*, and we have standardized their format as such: \n- One file (called `pytorch_model.bin.index.json`) contains some metadata and a map parameter name to file name, indicating where to find each weight\n- All the other files are standard PyTorch state dicts, they just contain a part of the model instead of the whole one. You can have a look at the content of the index file [here](https://huggingface.co/bigscience/bloom/blob/main/pytorch_model.bin.index.json).\n\nTo load such a sharded checkpoint into a model, we just need to loop over the various shards. Accelerate provides a function called `load_checkpoint_in_model` that will do this for you if you have cloned one of the repos of the Hub, or you can directly use the `from_pretrained` method of Transformers, which will handle the downloading and caching for you:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM", "This option can be activated with `low_cpu_mem_usage=True`. The model is first created on the Meta device (with empty weights) and the state dict is then loaded inside it (shard by shard in the case of a sharded checkpoint). This way the maximum RAM used is the full size of the model only.\n\n```py\nfrom transformers import AutoModelForSeq2SeqLM\n\nt0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", low_cpu_mem_usage=True)\n```\n\nMoreover, you can directly place the model on different devices if it doesn't fully fit in RAM (only works for inference for now). With `device_map=\"auto\"`, Accelerate will determine where to put each layer to maximize the use of your fastest devices (GPUs) and offload the rest on the CPU, or even the hard drive if you don't have enough GPU RAM (or CPU RAM). Even if the model is split across several devices, it will run as you would normally expect.", "The `partition.py` file defines the `PyTree` of `ParitionSpec` for the GPTNeo model which describes how the model will be sharded.\nThe actual sharding is auto-matically handled by `pjit`. The weights are sharded across all local devices.\nTo adapt the script for other models, we need to also change the `ParitionSpec` accordingly.\n\nTODO: Add more explantion.\n\nBefore training, let's prepare our model first. To be able to shard the model, the sharded dimention needs to be a multiple of devices it'll be sharded on. But GPTNeo's vocab size is 50257, so we need to resize the embeddings accordingly. \n\n```python\nfrom transformers import FlaxGPTNeoForCausalLM, GPTNeoConfig \nmodel = FlaxGPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")", "```python\ntokenizer.model_max_length\n```\n\n```python out\n512\n```\n\nThis value is derived from the *tokenizer_config.json* file associated with a checkpoint; in this case we can see that the context size is 512 tokens, just like with BERT.\n\n<Tip>\n\n\u270f\ufe0f **Try it out!** Some Transformer models, like [BigBird](https://huggingface.co/google/bigbird-roberta-base) and [Longformer](hf.co/allenai/longformer-base-4096), have a much longer context length than BERT and other early Transformer models. Instantiate the tokenizer for one of these checkpoints and verify that the `model_max_length` agrees with what's quoted on its model card.\n\n</Tip>\n\nSo, in order to run our experiments on GPUs like those found on Google Colab, we'll pick something a bit smaller that can fit in memory:\n\n```python\nchunk_size = 128\n```\n\n<Tip warning={true}>\n\nNote that using a small chunk size can be detrimental in real-world scenarios, so you should use a size that corresponds to the use case you will apply your model to.", "I discovered that there were 4 checkpoints in there. I had no idea what to do about it, so I started with a simpler job of using just the first checkpoint. Later I discovered that `fairseq` used all 4 checkpoints in an ensemble to get the best predictions, and that `transformers` currently doesn't support that feature. When the porting was completed and I was able to measure the performance scores, I found out that the `model4.pt` checkpoint provided the best score. But during the porting performance didn't matter much. Since I was using only one checkpoint it was crucial that when I was comparing outputs, I had `fairseq` also use just one and the same checkpoint."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the maximum size of a model checkpoint before it is automatically sharded as 10GB, which is in line with the reference answer. The additional information provided about the feature's introduction and the `max_shard_size` parameter, while not required, does not detract from the accuracy of the response regarding the specific question asked."}, {"question": "What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n", "true_answer": "To track their machine learning experiments at every stage, from training to production.", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md", "generated_answer": "Answer: Weights and Biases (W&B) is a platform designed to help data scientists and machine learning scientists track, visualize, and collaborate on their experiments and models. Its purpose is to provide a centralized location for managing the entire machine learning workflow, from data preparation and model training to deployment and monitoring. W&B allows users to easily compare and reproduce experiments, analyze performance metrics, and share results with others. It also provides tools for visualizing and exploring data", "retrieved_docs": ["## Limitations and Bias\nMSE has the disadvantage of heavily weighting outliers -- given that it squares them, this results in large errors weighing more heavily than small ones. It can be used alongside [MAE](https://huggingface.co/metrics/mae), which is complementary given that it does not square the errors. \n\n## Citation(s)\n```bibtex\n@article{scikit-learn,\n  title={Scikit-learn: Machine Learning in {P}ython},\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n  journal={Journal of Machine Learning Research},\n  volume={12},\n  pages={2825--2830},\n  year={2011}\n}\n```", "## Limitations and Bias\nMSE has the disadvantage of heavily weighting outliers -- given that it squares them, this results in large errors weighing more heavily than small ones. It can be used alongside [MAE](https://huggingface.co/metrics/mae), which is complementary given that it does not square the errors. \n\n## Citation(s)\n```bibtex\n@article{scikit-learn,\n  title={Scikit-learn: Machine Learning in {P}ython},\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n  journal={Journal of Machine Learning Research},\n  volume={12},\n  pages={2825--2830},\n  year={2011}\n}\n```", "--\ntitle: \"Let's talk about biases in machine learning! Ethics and Society Newsletter #2\" \nthumbnail: /blog/assets/122_ethics_soc_2/thumbnail-solstice.png\nauthors:\n- user: yjernite\n---\n\n# Machine Learning in development: Let's talk about bias!\n\n\n_Bias in ML is ubiquitous, and Bias in ML is complex; so complex in fact that no single technical intervention is likely to meaningfully address the problems it engenders. ML models, as sociotechnical systems, amplify social trends that may exacerbate inequities and harmful biases in ways that depend on their deployment context and are constantly evolving._\n\n_This means that developing ML systems with care requires vigilance and responding to feedback from those deployment contexts, which in turn we can facilitate by sharing lessons across contexts and developing tools to analyze signs of bias at every level of ML development._", "This may not come as much of a surprise given the ML research community\u2019s [focus on the value of \u201cgeneralization\u201d](https://dl.acm.org/doi/10.1145/3531146.3533083) \u2014 the most cited motivation for work in the field after \u201cperformance\u201d. However, while tools for bias assessment that apply to a wide range of settings are valuable to **enable a broader analysis of common trends** in model behaviors, their ability to target the mechanisms that lead to discrimination in **concrete use cases is inherently limited**. Using them to guide specific decisions within the ML development cycle usually requires an extra step or two  to take the system\u2019s specific use context and affected people into consideration.", "## Limitations and Bias\n*Note any limitations or biases that the metric has.*\n\n\n## Citation\n```bibtex\n@article{scikit-learn,\n  title={Scikit-learn: Machine Learning in {P}ython},\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n  journal={Journal of Machine Learning Research},\n  volume={12},\n  pages={2825--2830},\n  year={2011}\n}\n```\n\n## Further References\n\n- This Hugging Face implementation uses [this scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)", "## Limitations and Bias\n*Note any limitations or biases that the metric has.*\n\n\n## Citation\n```bibtex\n@article{scikit-learn,\n  title={Scikit-learn: Machine Learning in {P}ython},\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n  journal={Journal of Machine Learning Research},\n  volume={12},\n  pages={2825--2830},\n  year={2011}\n}\n```\n\n## Further References\n\n- This Hugging Face implementation uses [this scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)", "## Limitations and Bias\n\n\n\n## Citation(s)\n```bibtex\n@article{scikit-learn,\n    title={Scikit-learn: Machine Learning in {P}ython},\n    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n           and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n           and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n           Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n    journal={Journal of Machine Learning Research},\n    volume={12},\n    pages={2825--2830},\n    year={2011}\n}\n```\n\n\n## Further References"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct, accurate, and factual. It aligns well with the reference answer by mentioning the tracking of experiments, which is a key aspect of W&B's purpose. Additionally, the response expands on the reference answer by detailing the various stages of the machine learning workflow that W&B supports, such as data preparation, model training, deployment, and monitoring. It also correctly identifies the capabilities of W&B in terms of collaboration, comparison, reproduction of experiments, performance metrics analysis, and sharing of results. The mention of visualization and exploration tools is an added detail that is factual and relevant to the purpose of W&B. Therefore, the response meets the criteria for a high score on the rubric."}, {"question": "What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n", "true_answer": "Optimum", "source_doc": "huggingface/blog/blob/main/intel.md", "generated_answer": "The name of the open-source library created by Hugging Face to simplify Transformer acceleration is Optimum (source: question 3).", "retrieved_docs": ["## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.", "![alt_text](/blog/assets/06_ray_tune/ray-hf.jpg \"image_tooltip\")\n\n\nIn the Transformers 3.1 release, [Hugging Face Transformers](https://github.com/huggingface/transformers) and [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) teamed up to provide a simple yet powerful integration. [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) is a popular Python library for hyperparameter tuning that provides many state-of-the-art algorithms out of the box, along with integrations with the best-of-class tooling, such as [Weights and Biases](https://wandb.ai/) and tensorboard.\n\nTo demonstrate this new [Hugging Face](https://github.com/huggingface/transformers) + [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) integration, we leverage the [Hugging Face Datasets library](https://github.com/huggingface/datasets) to fine tune BERT on [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398).\n\nTo run this example, please first run:", "3. **Open-source libraries**: the Hugging Face [libraries](https://huggingface.co/docs) let you download pre-trained models with a single line of code, and you can start experimenting with your data in minutes. From training to deployment to hardware optimization, customers can rely on a consistent set of community-driven tools that work the same everywhere, from their laptops to their production environment.", "The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let\u2019s take a deeper look at it.\n\n## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance.", "Using Adapter Transformers at Hugging Face\n\n`adapter-transformers` is a library that extends \ud83e\udd17 `transformers` by allowing to integrate, train and use Adapters and other efficient fine-tuning methods. The library is fully compatible with \ud83e\udd17 `transformers`. Adapters are small learnt layers inserted within each layer of a pre-trained model. You can learn more about this in the [original paper](https://arxiv.org/abs/2007.07779). \n\n## Exploring adapter-transformers in the Hub\n\nYou can find over a hundred `adapter-transformer` models by filtering at the left of the [models page](https://huggingface.co/models?library=adapter-transformers&sort=downloads). Some adapter models can be found in the Adapter Hub [repository](https://github.com/adapter-hub/hub). Models from both sources are then aggregated in the [AdapterHub](https://adapterhub.ml/explore/).\n\n\n## Using existing models", "Also if you want to learn more about accelerating transformers you should also check out Hugging Face [optimum](https://github.com/huggingface/optimum). \n\n---\n\nThanks for reading! If you have any questions, feel free to contact me, through\u00a0[Github](https://github.com/huggingface/transformers), or on the\u00a0[forum](https://discuss.huggingface.co/c/sagemaker/17). You can also connect with me on\u00a0[Twitter](https://twitter.com/_philschmid)\u00a0or\u00a0[LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).", "- [\ud83e\udd17 accelerate](https://github.com/huggingface/accelerate) provides\nutilities for working with [large models](https://huggingface.co/docs/accelerate/usage_guides/big_modeling).\n- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) makes [8-bit quantization](https://github.com/TimDettmers/bitsandbytes#features) available to all PyTorch models.\n- [\ud83e\udd17 safetensors](https://github.com/huggingface/safetensors) not only ensures that save code is executed but also significantly speeds up the loading time of large models.\n\nDiffusers seamlessly integrates the above libraries to allow for a\nsimple API when optimizing large models.\n\nThe free-tier Google Colab is both CPU RAM constrained (13 GB RAM) as\nwell as GPU VRAM constrained (15 GB RAM for T4), which makes running the\nwhole >10B IF model challenging!\n\nLet\\'s map out the size of IF\\'s model components in full float32\nprecision:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, stating that Optimum is the name of the open-source library created by Hugging Face for Transformer acceleration. The additional information \"(source: question 3)\" does not detract from the correctness of the answer."}, {"question": "What parameter is used to ensure that elements in a row have the same height in Gradio?\n", "true_answer": "equal_height", "source_doc": "gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md", "generated_answer": "The parameter used to ensure that elements in a row have the same height in Gradio is the `scale` parameter when applied to components within a Row layout component. This parameter sets the relative width of the component, and when applied with the same value to multiple components within a Row, they will have the same height. If absolute dimensions are preferred, the `height` parameter can be used instead. For more information, refer to the Gradio documentation on Rows and Columns. (Source:", "retrieved_docs": ["Learn more about Columns in the [docs](https://gradio.app/docs/#column).\n\n# Dimensions\n\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\n\nBelow is an example illustrating the use of viewport width (vw):\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    im = gr.ImageEditor(\n        width=\"50vw\",\n    )\n\ndemo.launch()\n```\n\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n\n\n```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"", "Here's an example of how to use these attributes to create a Gradio app that does not lazy load and has an initial height of 0px.\n\n```html\n<gradio-app\n\tspace=\"gradio/Echocardiogram-Segmentation\"\n\teager=\"true\"\n\tinitial_height=\"0px\"\n></gradio-app>\n```\n\nHere's another example of how to use the `render` event. An event listener is used to capture the `render` event and will call the `handleLoadComplete()` function once rendering is complete. \n\n```html\n<script>\n\tfunction handleLoadComplete() {\n\t\tconsole.log(\"Embedded space has finished rendering\");\n\t}\n\n\tconst gradioApp = document.querySelector(\"gradio-app\");\n\tgradioApp.addEventListener(\"render\", handleLoadComplete);\n</script>\n```", "```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    with gr.Column(elem_classes=[\"container\"]):\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\n\ndemo.launch()\n```\n\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.\n\nYou can apply any valid CSS unit for these parameters. For a comprehensive list of CSS units, refer to [this guide](https://www.w3schools.com/cssref/css_units.php). We recommend you always consider responsiveness and test your interfaces on various screen sizes to ensure a consistent user experience.\n\n\n\n## Tabs and Accordions", "Learn more about Rows in the [docs](https://gradio.app/docs/#row).\n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n\n$code_rows_and_columns\n$demo_rows_and_columns\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/#column).\n\n# Dimensions", "- `spacing_size`: This sets the padding within and spacing between elements. In the default theme, this is set to `gradio.themes.sizes.spacing_md`.\n- `radius_size`: This sets the roundedness of corners of elements. In the default theme, this is set to `gradio.themes.sizes.radius_md`.\n- `text_size`: This sets the font size of text. In the default theme, this is set to `gradio.themes.sizes.text_md`.\n\nYou could modify these values using their string shortcuts, such as\n\n```python\nwith gr.Blocks(theme=gr.themes.Default(spacing_size=\"sm\", radius_size=\"none\")) as demo:\n    ...\n```\n\nor you could use the `Size` objects directly, like this:\n\n```python\nwith gr.Blocks(theme=gr.themes.Default(spacing_size=gr.themes.sizes.spacing_sm, radius_size=gr.themes.sizes.radius_none)) as demo:\n    ...\n```\n\n<div class=\"wrapper\">\n<iframe\n\tsrc=\"https://gradio-theme-extended-step-2.hf.space?__theme=light\"\n\tframeborder=\"0\"\n></iframe>\n</div>\n\nThe predefined size objects are:", "Gradio Demo: blocks_layout\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    with gr.Row():\n        gr.Image(interactive=True, scale=2)\n        gr.Image()\n    with gr.Row():\n        gr.Textbox(label=\"Text\")\n        gr.Number(label=\"Count\", scale=2)\n        gr.Radio(choices=[\"One\", \"Two\"])\n    with gr.Row():\n        gr.Button(\"500\", scale=0, min_width=500)\n        gr.Button(\"A\", scale=0)\n        gr.Button(\"grow\")\n    with gr.Row():\n        gr.Textbox()\n        gr.Textbox()\n        gr.Button()        \n    with gr.Row():\n        with gr.Row():\n            with gr.Column():\n                gr.Textbox(label=\"Text\")\n                gr.Number(label=\"Count\")\n                gr.Radio(choices=[\"One\", \"Two\"])\n            gr.Image()\n            with gr.Column():\n                gr.Image(interactive=True)\n                gr.Image()\n    gr.Image()\n    gr.Textbox(label=\"Text\")\n    gr.Number(label=\"Count\")\n    gr.Radio(choices=[\"One\", \"Two\"])", "* Add missing `display: flex` property to `Row` so that flex styling is applied to children by [@hannahblair] in [PR 4896](https://github.com/gradio-app/gradio/pull/4896)\n* Fixed bug where `gr.Video` could not preprocess urls by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4904](https://github.com/gradio-app/gradio/pull/4904)\n* Fixed copy button rendering in API page on Safari by [@aliabid94](https://github.com/aliabid94) in [PR 4924](https://github.com/gradio-app/gradio/pull/4924)\n* Fixed `gr.Group` and `container=False`. `container` parameter only available for `Textbox`, `Number`, and `Dropdown`, the only elements where it makes sense. By [@aliabid94](https://github.com/aliabid94) in [PR 4916](https://github.com/gradio-app/gradio/pull/4916)\n* Fixed broken image link in auto-generated `app.py` from `ThemeClass.push_to_hub` by [@deepkyu](https://github.com/deepkyu) in [PR 4944](https://github.com/gradio-app/gradio/pull/4944)\n\n### Other Changes:"], "eval_score_GPT4": "2", "eval_feedback_GPT4": "Feedback: The response provided is mostly incorrect. The `scale` parameter mentioned is not related to ensuring elements in a row have the same height in Gradio. Instead, it is related to the relative width of components. The correct parameter for ensuring equal height of elements in a row is `equal_height`, which is not mentioned in the response. The additional mention of the `height` parameter is somewhat related but does not directly address the question about equal height in a row. The response does not fully align with the reference answer and is therefore not completely accurate or factual."}, {"question": "What is the command to install the latest version of Optimum with OpenVINO support?\n", "true_answer": "pip install --upgrade-strategy eager optimum[\"openvino\"]", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md", "generated_answer": "To install the latest version of Optimum with OpenVINO support, you can use the following command with the `--upgrade-strategy eager` option:\n\n```bash\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\nThis will ensure that `optimum-intel` is using the latest version of Optimum with OpenVINO support.", "retrieved_docs": ["Optimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. \n\nOptimum Intel and OpenVINO can be installed as follows:\n\n```\npip install optimum[openvino]\n```\n\nStarting from the code above, we only need to replace `StableDiffusionPipeline` with `OVStableDiffusionPipeline`. To load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model.\n\n```python\nfrom optimum.intel.openvino import OVStableDiffusionPipeline\n...\nov_pipe = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\nlatency = elapsed_time(ov_pipe, prompt)\nprint(latency)\n\n# Don't forget to save the exported model\nov_pipe.save_pretrained(\"./openvino\")\n```\n\nOpenVINO automatically optimizes the model for the `bfloat16` format. Thanks to this, the average latency is now **16.7 seconds**, a sweet 2x speedup.", "### OpenVINO\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install --upgrade-strategy eager optimum[openvino,nncf]\n```\n\nIt is possible to export \ud83e\udd17 Transformers and Diffusers models to the OpenVINO format easily:\n\n```bash\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert_sst2_ov\n```\n\nIf you add `--int8`, the weights will be quantized to INT8. Static quantization can also be applied on the activations using [NNCF](https://github.com/openvinotoolkit/nncf), more information can be found in the [documentation](https://huggingface.co/docs/optimum/main/en/intel/optimization_ov).\n\nTo load a model and run inference with OpenVINO Runtime, you can just replace your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. To load a PyTorch checkpoint and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model.", "The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nTo install from source:\n\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\n\nFor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:\n\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```\n\n## Accelerated Inference\n\n\ud83e\udd17 Optimum provides multiple tools to export and run optimized models on various ecosystems: \n\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models)\n- TensorFlow Lite\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)\n- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_inference)", "The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you can install the base library from source as follows:\n\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\n\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator_type]` to the `pip` command, e.g.\n\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```", "You'll need to install \ud83e\udd17 Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\n\n```bash\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO.\n\n## Stable Diffusion\n\nTo load and run inference, use the [`~optimum.intel.OVStableDiffusionPipeline`]. If you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set `export=True`:\n\n```python\nfrom optimum.intel import OVStableDiffusionPipeline\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipeline = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\nprompt = \"sailing ship in storm by Rembrandt\"\nimage = pipeline(prompt).images[0]\n\n# Don't forget to save the exported model\npipeline.save_pretrained(\"openvino-sd-v1-5\")\n```", "## Optimum Intel\n\n### OpenVINO", "## Optimum Intel\n\n### OpenVINO"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer and includes the correct command to install the latest version of Optimum with OpenVINO support. The additional explanation about the `--upgrade-strategy eager` option is accurate and does not detract from the correctness of the command."}]