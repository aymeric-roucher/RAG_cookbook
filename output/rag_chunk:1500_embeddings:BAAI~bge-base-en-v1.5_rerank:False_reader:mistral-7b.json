[{"question": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n", "true_answer": "x86_64-unknown-linux-musl", "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md", "generated_answer": "The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture. (Answer according to context provided.)", "retrieved_docs": ["`tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-arm64-musl`\n\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-x64-gnu`\n\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-freebsd-x64`\n\nThis is the **x86_64-unknown-freebsd** binary for `tokenizers`", "`tokenizers-linux-arm64-gnu`\n\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-darwin-x64`\n\nThis is the **x86_64-apple-darwin** binary for `tokenizers`", "`tokenizers-linux-arm-gnueabihf`\n\nThis is the **armv7-unknown-linux-gnueabihf** binary for `tokenizers`"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, indicating a full understanding of the architecture for which the `tokenizers-linux-x64-musl` binary is designed."}, {"question": "What is the purpose of the BLIP-Diffusion model?\n", "true_answer": "The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md", "generated_answer": "The BLIP-Diffusion model, proposed in the paper \"BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing,\" enables zero-shot subject-driven generation and control-guided zero-shot generation. Its purpose is to allow for the generation of images based on a given subject or control input, without the need for additional training data for that specific subject or control input. This is achieved through the use of a pre", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# BLIP-Diffusion\n\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. \n\n\nThe abstract from the paper is:", "The original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization.\n\n`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/).\n\n<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n\n</Tip>\n\n\n## BlipDiffusionPipeline\n[[autodoc]] BlipDiffusionPipeline\n    - all\n    - __call__\n\n## BlipDiffusionControlNetPipeline\n[[autodoc]] BlipDiffusionControlNetPipeline\n    - all\n    - __call__", "We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available [here](https://github.com/heejkoo/Awesome-Diffusion-Models) as well as @crowsonkb and @rromb for useful discussions and insights.\n\n## Citation\n\n```bibtex\n@misc{von-platen-etal-2022-diffusers,\n  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},\n  title = {Diffusers: State-of-the-art diffusion models},\n  year = {2022},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/huggingface/diffusers}}\n}\n```", "## What about the general-purpose Stable Diffusion model?\n\nAs we showed with the Pokemon image generation task, it is possible to achieve a high level of optimization of the Stable Diffusion pipeline when using a relatively small amount of training resources. At the same time, it is well-known that training a general-purpose Stable Diffusion model is an [expensive task](https://www.mosaicml.com/blog/training-stable-diffusion-from-scratch-part-2). However, with enough budget and HW resources, it is possible to optimize the general-purpose model using the described approach and tune it to produce high-quality images. The only caveat we have is related to the token merging method that reduces the model capacity substantially. The rule of thumb here is the more complicated the dataset you have for the training, the less merging ratio you should use during the optimization.\n\nIf you enjoyed reading this post, you might also be interested in checking out [this post](https://huggingface.co/blog/stable-diffusion-inference-intel) that discusses other complementary approaches to optimize the performance of Stable Diffusion on 4th generation Intel Xeon CPUs.", "We will document some of the techniques `diffusers` supports to control generation of diffusion models. Much is cutting edge research and can be quite nuanced. If something needs clarifying or you have a suggestion, don't hesitate to open a discussion on the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) or a [GitHub issue](https://github.com/huggingface/diffusers/issues).\n\nWe provide a high level explanation of how the generation can be controlled as well as a snippet of the technicals. For more in depth explanations on the technicals, the original papers which are linked from the pipelines are always the best resources.\n\nDepending on the use case, one should choose a technique accordingly. In many cases, these techniques can be combined. For example, one can combine Textual Inversion with SEGA to provide more semantic guidance to the outputs generated using Textual Inversion.\n\nUnless otherwise mentioned, these are techniques that work with existing models and don't require their own weights.", "The abstract from the paper is:\n\n*Model-based reinforcement learning methods often use learning only for the purpose of estimating an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize long-horizon decision-making and test-time flexibility.*\n\nYou can find additional information about the model on the [project page](https://diffusion-planning.github.io/), the [original codebase](https://github.com/jannerm/diffuser), or try it out in a demo [notebook](https://colab.research.google.com/drive/1rXm8CX4ZdN5qivjJ2lhwhkOmt_m0CvU0#scrollTo=6HXJvhyqcITc&uniqifier=1).", "In [Training Diffusion Models with Reinforcement Learning, Black](https://arxiv.org/abs/2305.13301) et al. show how to augment diffusion models to leverage RL to fine-tune them with respect to an objective function via a method named Denoising Diffusion Policy Optimization (DDPO).\n\nIn this blog post, we discuss how DDPO came to be, a brief description of how it works, and how DDPO can be incorporated into an RLHF workflow to achieve model outputs more aligned with the human aesthetics. We then quickly switch gears to talk about how you can apply DDPO to your  models with the newly integrated `DDPOTrainer` from the `trl` library and discuss our findings from running DDPO on Stable Diffusion. \n\n## The Advantages of DDPO\n\nDDPO is not the only working answer to the question of how to attempt to fine-tune diffusion models with RL. \n\nBefore diving in, there are two key points to remember when it comes to understanding the advantages of one RL solution over the other\n\n1. Computational efficiency is key. The more complicated your data distribution gets, the higher your computational costs get.\n2. Approximations are nice, but because approximations are not the real thing, associated errors stack up."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the BLIP-Diffusion model as a tool for generating images based on a given subject or control input, aligning with the reference answer's mention of controllable text-to-image generation and editing. The response adds detail by mentioning zero-shot subject-driven generation and control-guided zero-shot generation, which are specific features of the model that support its purpose. The response is accurate and factual, providing a clear understanding of the model's purpose without introducing any incorrect information."}, {"question": "How can a user claim authorship of a paper on the Hugging Face Hub?\n", "true_answer": "By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/paper-pages.md", "generated_answer": "To claim authorship of a paper on the Hugging Face Hub, a user should follow these steps:\n\n1. Go to the Paper page of the paper in question.\n2. Click on the user's name in the corresponding Paper page.\n3. Click on \"claim authorship\" in the pop-up window.\n4. This will redirect the user to the paper settings page.\n5. Confirm the request by clicking on \"confirm request\" in the paper", "retrieved_docs": ["The Hub will attempt to automatically match paper to users based on their email. \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\n</div>\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings-dark.png\"/>\n</div>\n\n\n## Frequently Asked Questions\n\n### Can I control which Paper pages show in my profile?\n\nYes! You can visit your Papers in [settings](https://huggingface.co/settings/papers), where you will see a list of verified papers. There, you can click the \"Show on profile\" checkbox to hide/show it in your profile. \n\n### Do you support ACL anthology?", "### Do you support ACL anthology?\n\nWe're starting with Arxiv as it accounts for 95% of the paper URLs Hugging Face users have linked in their repos organically. We'll check how this evolve and potentially extend to other paper hosts in the future.\n\n### Can I have a Paper page even if I have no model/dataset/Space?\n\nYes. You can go to [the main Papers page](https://huggingface.co/papers), click search and write the name of the paper or the full Arxiv id. If the paper does not exist, you will get an option to index it. You can also just visit the page `hf.co/papers/xxxx.yyyyy` replacing with the arxiv id of the paper you wish to index.", "--\ntitle: \"The Hugging Face Hub for Galleries, Libraries, Archives and Museums\"\nthumbnail: /blog/assets/144_hf_hub_glam_guide/thumbnail.png\nauthors:\n- user: davanstrien\n---\n\n## The Hugging Face Hub for Galleries, Libraries, Archives and Museums \n\n\n\n### What is the Hugging Face Hub?\n\nHugging Face aims to make high-quality machine learning accessible to everyone. This goal is pursued in various ways, including developing open-source code libraries such as the widely-used Transformers library, offering [free courses](https://huggingface.co/learn), and providing the Hugging Face Hub. \n\nThe [Hugging Face Hub](https://huggingface.co/) is a central repository where people can share and access machine learning models, datasets and demos. The Hub hosts over 190,000 machine learning models, 33,000 datasets and over 100,000 machine learning applications and demos. These models cover a wide range of tasks from pre-trained language models, text, image and audio classification models, object detection models, and a wide range of generative models. \n\nThe models, datasets and demos hosted on the Hub span a wide range of domains and languages, with regular community efforts to expand the scope of what is available via the Hub. This blog post intends to offer people working in or with the galleries, libraries, archives and museums (GLAM) sector to understand how they can use &mdash; and contribute to &mdash; the Hugging Face Hub.", "--\ntitle: \"Welcome fastText to the Hugging Face Hub\"\nthumbnail: /blog/assets/147_fasttext/thumbnail.png\nauthors:\n- user: sheonhan\n- user: juanpino\n  guest: true\n---\n\n# Welcome fastText to the Hugging Face Hub\n\n\n[fastText](https://fasttext.cc/) is a library for efficient learning of text representation and classification. [Open-sourced](https://fasttext.cc/blog/2016/08/18/blog-post.html) by Meta AI in 2016, fastText integrates key ideas that have been influential in natural language processing and machine learning over the past few decades: representing sentences using bag of words and bag of n-grams, using subword information, and utilizing a hidden representation to share information across classes. \n\nTo speed up computation, fastText uses hierarchical softmax, capitalizing on the imbalanced distribution of classes. All these techniques offer users scalable solutions for text representation and classification.\n\nHugging Face is now hosting official mirrors of word vectors of all 157 languages and the latest model for language identification. This means that using Hugging Face, you can easily download and use the models with a few commands. \n\n### Finding models", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# How to contribute to huggingface_hub, the GitHub repository?\n\nEveryone is welcome to contribute, and we value everybody's contribution. Code is not the only way to help the community.\nAnswering questions, helping others, reaching out and improving the documentations are immensely valuable to the community.\n\nIt also helps us if you spread the word: reference the library from blog posts\non the awesome projects it made possible, shout out on Twitter every time it has\nhelped you, or simply star the repo to say \"thank you\".\n\nWhichever way you choose to contribute, please be mindful to respect our\n[code of conduct](https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md).", "--\ntitle: 'Welcome fastai to the Hugging Face Hub'\nthumbnail: /blog/assets/64_fastai/fastai_hf_blog.png\nauthors:\n- user: espejelomar\n---\n\n# Welcome fastai to the Hugging Face Hub\n\n\n## Making neural nets uncool again... and sharing them\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/64_fastai_hub.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nFew have done as much as the [fast.ai](https://www.fast.ai/) ecosystem to make Deep Learning accessible. Our mission at Hugging Face is to democratize good Machine Learning. Let's make exclusivity in access to Machine Learning, including [pre-trained models](https://huggingface.co/models), a thing of the past and let's push this amazing field even further.", "--\ntitle: \"Welcome PaddlePaddle to the Hugging Face Hub\" \nthumbnail: /blog/assets/126_paddlepaddle/thumbnail.jpg\nauthors:\n- user: PaddlePaddle\n  guest: true\n---\n\n# Welcome PaddlePaddle to the Hugging Face Hub\n\n\nWe are happy to share an open source collaboration between Hugging Face and [PaddlePaddle](https://www.paddlepaddle.org.cn/en) on a shared mission to advance and democratize AI through open source!\n\nFirst open sourced by Baidu in 2016, PaddlePaddle enables developers of all skill levels to adopt and implement Deep Learning at scale. As of Q4 2022, PaddlePaddle is being used by more than 5.35 million developers and 200,000 enterprises, ranking first in terms of market share among Deep Learning platforms in China. PaddlePaddle features popular open source repositories such as the [Paddle](https://github.com/PaddlePaddle/Paddle) Deep Learning Framework, model libraries across different modalities (e.g. [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR), [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection), [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP), [PaddleSpeech](https://github.com/PaddlePaddle/PaddleSpeech)), [PaddleSlim](https://github.com/PaddlePaddle/PaddleSlim) for model compression, [FastDeploy](https://github.com/PaddlePaddle/FastDeploy) for model deployment and many more.\n\n![thumbnail](assets/126_paddlepaddle/thumbnail.jpg)"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and factual. It outlines a series of steps that a user should follow to claim authorship of a paper on the Hugging Face Hub, which aligns well with the reference answer. However, the response includes an additional step of going to the Paper page, which is not mentioned in the reference answer but is implied as necessary to find the user's name. The response also adds a step about a pop-up window, which is not mentioned in the reference answer but does not contradict it and seems like a plausible part of the process. The final steps of clicking \"claim authorship\" and confirming the request in the paper settings are accurately reflected in the response, which is in line with the reference answer. Therefore, the response is mostly correct, accurate, and factual."}, {"question": "What is the purpose of the /healthcheck endpoint in the Datasets server API?\n", "true_answer": "Ensure the app is running", "source_doc": "huggingface/datasets-server/blob/main/services/api/README.md", "generated_answer": "The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is running. It is a simple endpoint that can be used to check the availability and responsiveness of the Datasets server. When accessed, it returns a success message indicating that the server is healthy and operational. This endpoint can be useful for monitoring and troubleshooting purposes.", "retrieved_docs": ["## Check if a dataset is valid\n\n`/is-valid` checks whether a specific dataset loads without any error. This endpoint's query parameter requires you to specify the name of the dataset:\n\n<inferencesnippet>\n<python>\n```python\nimport requests\nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\nAPI_URL = \"https://datasets-server.huggingface.co/is-valid?dataset=rotten_tomatoes\"\ndef query():\n    response = requests.get(API_URL, headers=headers)\n    return response.json()\ndata = query()\n```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const response = await fetch(\n        \"https://datasets-server.huggingface.co/is-valid?dataset=rotten_tomatoes\",\n        {\n            headers: { Authorization: `Bearer ${API_TOKEN}` },\n            method: \"GET\"\n        }\n    );\n    const result = await response.json();\n    return result;\n}\nquery().then((response) => {\n    console.log(JSON.stringify(response));\n});\n```\n</js>\n<curl>\n```curl\ncurl https://datasets-server.huggingface.co/is-valid?dataset=rotten_tomatoes \\\n        -X GET \\\n        -H \"Authorization: Bearer ${API_TOKEN}\"\n```\n</curl>\n</inferencesnippet>\n\nThe response looks like this if a dataset is valid:\n\n```json\n{\n  \"viewer\": true,\n  \"preview\": true\n}\n```\n\nIf only the first rows of a dataset are available, then the response looks like:\n\n```json\n{\n  \"viewer\": false,\n  \"preview\": true\n}\n```\n\nFinally, if the dataset is not valid at all, then the response is:", "## Endpoints\n\nThe admin service provides endpoints:\n\n- `/healthcheck`\n- `/metrics`: give info about the cache and the queue\n- `/cache-reports{processing_step}`: give detailed reports on the content of the cache for a processing step\n- `/cache-reports-with-content{processing_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\n- `/force-refresh{processing_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\n  - `dataset`: `?dataset={dataset}`\n  - `config`: `?dataset={dataset}&config={config}`\n  - `split`: `?dataset={dataset}&config={config}&split={split}`\n- `/recreate-dataset`: deletes all the cache entries related to a specific dataset, then run all the steps in order. It's a POST endpoint. Pass the requested parameters:\n  - `dataset`: the dataset name\n  - `priority`: `low` (default), `normal` or `high`", "</Tip>\n\nIn this case, an error is raised to alert that the dataset has changed.\nTo ignore the error, one needs to specify `verification_mode=\"no_checks\"` in [`load_dataset`].\nAnytime you see a verification error, feel free to open a discussion or pull request in the corresponding dataset \"Community\" tab, so that the integrity checks for that dataset are updated.\n\n## Security\n\nThe dataset repositories on the Hub are scanned for malware, see more information [here](https://huggingface.co/docs/hub/security#malware-scanning).\n\nMoreover the datasets without a namespace (originally contributed on our GitHub repository) have all been reviewed by our maintainers.\nThe code of these datasets is considered **safe**.\nIt concerns datasets that are not under a namespace, e.g. \"squad\" or \"glue\", unlike the other datasets that are named \"username/dataset_name\" or \"org/dataset_name\".", "Datasets server API - search service\n\n> /search endpoint\n> /filter endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### Duckdb index full text search\n- `DUCKDB_INDEX_CACHE_DIRECTORY`: directory where the temporal duckdb index files are downloaded. Defaults to empty.\n- `DUCKDB_INDEX_TARGET_REVISION`: the git revision of the dataset where the index file is stored in the dataset repository.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /search: get a slice of a search result over a dataset split\n- /filter: filter rows of a dataset split", "Datasets server API - rows endpoint\n\n> /rows endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split", "Check dataset validity\n\nBefore you download a dataset from the Hub, it is helpful to know if a specific dataset you're interested in is available. Datasets Server provides the `/is-valid` endpoint to check if a specific dataset works without any errors.\n\nThe API endpoint will return an error for datasets that cannot be loaded with the [\ud83e\udd17 Datasets](https://github.com/huggingface/datasets) library, for example, because the data hasn't been uploaded or the format is not supported.\n\n<Tip warning={true}>\n  The largest datasets are partially supported by Datasets Server. If they are{\" \"}\n  <a href=\"https://huggingface.co/docs/datasets/stream\">streamable</a>, Datasets\n  Server can extract the first 100 rows without downloading the whole dataset.\n  This is especially useful for previewing large datasets where downloading the\n  whole dataset may take hours! See the <code>preview</code> field in the\n  response of <code>/is-valid</code> to check if a dataset is partially\n  supported.\n</Tip>\n\nThis guide shows you how to check dataset validity programmatically, but free to try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-17b761d0-b2b8-4638-a4f7-73be9049c324), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/isValidDataset).\n\n## Check if a dataset is valid", "Datasets server API\n\n> API on \ud83e\udd17 datasets\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /webhook: Add, update or remove a dataset\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, stating that the purpose of the /healthcheck endpoint is to ensure that the app is running. It expands on the reference answer by explaining that it checks the availability and responsiveness of the Datasets server and returns a success message when the server is healthy. This additional information supports the main point without introducing inaccuracies. Therefore, the response is completely correct, accurate, and factual."}, {"question": "What is the default context window size for Local Attention in the LongT5 model?\n", "true_answer": "127 tokens", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md", "generated_answer": "The default context window size for Local Attention in the LongT5 model is 127 tokens, as mentioned in the context provided. This information can be found in the description of the LongT5 model, which is an extension of the T5 model that uses either Local Attention or Transient-Global Attention for handling input sequences of a length up to 16,384 tokens. The Local Attention mechanism allows each input token to attend only to a fixed number", "retrieved_docs": ["Note that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\nattending tokens so that global attention is *symmetric*.\n\nThe user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global_attention_mask`:\n\n- 0: the token attends \"locally\",\n- 1: the token attends \"globally\".\n\nFor more information please also refer to [`~LongformerModel.forward`] method.\n\nUsing Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to\n\\\\(\\mathcal{O}(n_s \\times w)\\\\), with \\\\(n_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize. It is assumed that the number of \"globally\" attending tokens is insignificant as compared to the number of\n\"locally\" attending tokens.\n\nFor more information, please refer to the official [paper](https://arxiv.org/pdf/2004.05150.pdf).\n\n\n## Training\n\n[`LongformerForMaskedLM`] is trained the exact same way [`RobertaForMaskedLM`] is\ntrained and should be used as follows:", "- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.\n- *Transient Global Attention* is an extension of the *Local Attention*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and", "#### Follow-up questions\n\n* The increasing size (throughout the layers) of the dilated windowed self-attention echoes findings in computer vision on increasing the receptive field of stacked CNN. How do these two findings relate? What are the transposable learnings?\n* Longformer\u2019s Encoder-Decoder architecture works well for tasks that do not require a long target length (e.g. summarization). However, how would it work for long-range seq2seq tasks which require a long target length (e.g. document translation, speech recognition, etc.) especially considering the cross-attention layer of encoder-decoder\u2019s models?\n* In practice, the sliding window self-attention relies on many indexing operations to ensure a symmetric query-key weights matrix. Those operations are very slow on TPUs which highlights the question of the applicability of such patterns on other hardware.\n\n### [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)\n\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap", "#### Main findings\n\n* The authors proposed the dilated windowed self-attention (Figure c) and showed that it yields better results on language modeling compared to just windowed/sparse self-attention (Figure b). The window sizes are increased through the layers. This pattern further outperforms previous architectures (such as Transformer-XL, or adaptive span attention) on downstream benchmarks.\n* Global attention allows the information to flow through the whole sequence and applying the global attention to task-motivated tokens (such as the tokens of the question in QA, CLS token for sentence classification) leads to stronger performance on downstream tasks. Using this global pattern, Longformer can be successfully applied to document-level NLP tasks in the transfer learning setting.\n* Standard pre-trained models can be adapted to long-range inputs by simply replacing the standard self-attention with the long-range self-attention proposed in this paper and then fine-tuning on the downstream task. This avoids costly pre-training specific to long-range inputs.\n\n#### Follow-up questions", "The abstract from the paper is the following:\n\n*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.*\n\nThis model was contributed by [stancld](https://huggingface.co/stancld).\nThe original code can be found [here](https://github.com/google-research/longt5).\n\n## Usage tips", "*We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its", "The abstract from the paper is the following:\n\n*We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision.\nNA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a\nlinear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's\nreceptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike\nSwin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package\nwith efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less\nmemory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA\nthat boosts image classification and downstream vision performance. Experimental results on NAT are competitive;\nNAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9%\nImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size. *\n\n<img\nsrc=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/neighborhood-attention-pattern.jpg\"\nalt=\"drawing\" width=\"600\"/>"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the default context window size for Local Attention in the LongT5 model as 127 tokens, which is in agreement with the reference answer. The additional information provided about the LongT5 model and its capabilities does not detract from the accuracy of the response regarding the specific detail in question."}, {"question": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n", "true_answer": "from_pretrained()", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md", "generated_answer": "To load a checkpoint for a task using `AutoPipeline`, you can use the `AutoPipelineForText2Image`, `AutoPipelineForImage2Image`, or `AutoPipelineForInpainting` classes provided by Diffusers. These classes automatically detect the correct pipeline class to use based on the task, making it easier to load a pretrained checkpoint without knowing the specific pipeline class name. You can instantiate the checkpoint using the task-specific `AutoPipelineForText", "retrieved_docs": ["The `AutoPipeline` class is designed to simplify the variety of pipelines in \ud83e\udd17 Diffusers. It is a generic, *task-first* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name.\n\n<Tip>\n\nTake a look at the [AutoPipeline](../api/pipelines/auto_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting.\n\n</Tip>\n\nThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights.\n\n## Choose an AutoPipeline for your task\n\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\nimage\n```", "If you're interested in using one of the official checkpoints for a task, explore the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations!\n\n</Tip>\n\n## StableDiffusionInpaintPipeline\n\n[[autodoc]] StableDiffusionInpaintPipeline\n\t- all\n\t- __call__\n\t- enable_attention_slicing\n\t- disable_attention_slicing\n\t- enable_xformers_memory_efficient_attention\n\t- disable_xformers_memory_efficient_attention\n\t- load_textual_inversion\n\t- load_lora_weights\n\t- save_lora_weights\n\n## StableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput\n\n## FlaxStableDiffusionInpaintPipeline\n\n[[autodoc]] FlaxStableDiffusionInpaintPipeline\n\t- all\n\t- __call__\n\n## FlaxStableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput", "As you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it's going to be used for. For example, here we are loading the `camembert-base` checkpoint in the `fill-mask` pipeline, which is completely fine. But if we were to load this checkpoint in the `text-classification` pipeline, the results would not make any sense because the head of `camembert-base` is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/tasks.png\" alt=\"The task selector on the web interface.\" width=\"80%\"/>\n</div>\n\nYou can also instantiate the checkpoint using the model architecture directly:\n\n{#if fw === 'pt'}\n```py\nfrom transformers import CamembertTokenizer, CamembertForMaskedLM\n\ntokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\nmodel = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n```\n\nHowever, we recommend using the [`Auto*` classes](https://huggingface.co/transformers/model_doc/auto.html?highlight=auto#auto-classes) instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the `Auto*` classes makes switching checkpoints simple:", "Therefore you have two ways to take advantage of this very beneficial feature:\n\n1. If you want to use a HF Transformers models you can do `model.gradient_checkpointing_enable()` or use `--gradient_checkpointing` in the HF Trainer, which will automatically enable this for you. `torch.utils.checkpoint` is used there.\n2. If you write your own model and you want to use DeepSpeed's activation checkpointing you can use the [API prescribed there](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html). You can also take the HF Transformers modeling code and replace `torch.utils.checkpoint` with the DeepSpeed's API. The latter is more flexible since it allows you to offload the forward activations to the CPU memory instead of recalculating them.\n\n\n### Optimizer and Scheduler\n\nAs long as you don't enable `offload_optimizer` you can mix and match DeepSpeed and HuggingFace schedulers and\noptimizers, with the exception of using the combination of HuggingFace scheduler and DeepSpeed optimizer:\n\n| Combos       | HF Scheduler | DS Scheduler |\n|:-------------|:-------------|:-------------|\n| HF Optimizer | Yes          | Yes          |\n| DS Optimizer | No           | Yes          |\n\nIt is possible to use a non-DeepSpeed optimizer when `offload_optimizer` is enabled, as long as it has both CPU and\nGPU implementation (except LAMB).\n\n\n\n\n<a id='deepspeed-optimizer'></a>\n\n#### Optimizer", "If you're interested in using one of the official checkpoints for a task, explore the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations!\n\n</Tip>\n\n## Text-to-image\n\n```py\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nimport torch\n\nrepo_id = \"stabilityai/stable-diffusion-2-base\"\npipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision=\"fp16\")\n\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\n\nprompt = \"High quality photo of an astronaut riding a horse in space\"\nimage = pipe(prompt, num_inference_steps=25).images[0]\nimage\n```\n\n## Inpainting\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\ninit_image = load_image(img_url).resize((512, 512))\nmask_image = load_image(mask_url).resize((512, 512))\n\nrepo_id = \"stabilityai/stable-diffusion-2-inpainting\"\npipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision=\"fp16\")", "<Tip warning={true}>\n\n\ud83e\uddea Many of the SDXL ControlNet checkpoints are experimental, and there is a lot of room for improvement. Feel free to open an [Issue](https://github.com/huggingface/diffusers/issues/new/choose) and leave us feedback on how we can improve!\n\n</Tip>\n\nIf you don't see a checkpoint you're interested in, you can train your own SDXL ControlNet with our [training script](../../../../../examples/controlnet/README_sdxl).\n\n<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n\n</Tip>\n\n## StableDiffusionXLControlNetPipeline\n[[autodoc]] StableDiffusionXLControlNetPipeline\n\t- all\n\t- __call__\n\n## StableDiffusionXLControlNetImg2ImgPipeline\n[[autodoc]] StableDiffusionXLControlNetImg2ImgPipeline\n\t- all\n\t- __call__\n\n## StableDiffusionXLControlNetInpaintPipeline\n[[autodoc]] StableDiffusionXLControlNetInpaintPipeline\n\t- all\n\t- __call__\n\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput", "```bash\nexport TRAIN_DIR=\"path_to_your_dataset\"\n\naccelerate launch --mixed_precision=\"fp16\" train_text_to_image_decoder.py \\\n  --train_data_dir=$TRAIN_DIR \\\n  --resolution=768 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --checkpoints_total_limit=3 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --validation_prompts=\"A robot pokemon, 4k photo\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub \\\n  --output_dir=\"kandi22-decoder-pokemon-model\" \n```\n\n\nOnce the training is finished the model will be saved in the `output_dir` specified in the command. In this example it's `kandi22-decoder-pokemon-model`. To load the fine-tuned model for inference just pass that path to `AutoPipelineForText2Image`\n\n```python\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(output_dir, torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\nprompt='A robot pokemon, 4k photo'\nimages = pipe(prompt=prompt).images\nimages[0].save(\"robot-pokemon.png\")\n```\n\nCheckpoints only save the unet, so to run inference from a checkpoint, just load the unet\n```python\nfrom diffusers import AutoPipelineForText2Image, UNet2DConditionModel\n\nmodel_path = \"path_to_saved_model\"\n\nunet = UNet2DConditionModel.from_pretrained(model_path + \"/checkpoint-<N>/unet\")"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided does not mention the `from_pretrained()` method, which is the correct way to load a checkpoint for a task using `AutoPipeline`. Instead, the response incorrectly suggests using classes like `AutoPipelineForText2Image`, `AutoPipelineForImage2Image`, or `AutoPipelineForInpainting` from Diffusers, which are not directly related to the `AutoPipeline` or the method of loading checkpoints. The response is therefore completely incorrect and not factual as per the reference answer and the score rubric."}, {"question": "What is the purpose of Diffusers library?\n", "true_answer": "To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md", "generated_answer": "The purpose of the Diffusers library, as stated in the context, is to provide a modular toolbox for diffusion models across modalities, with a focus on usability over performance, simple over easy, and customizability over abstractions. It offers state-of-the-art diffusion pipelines, interchangeable noise schedulers, and pretrained models that can be used to generate images, audio, and even 3D structures of molecules. The library aims to be", "retrieved_docs": ["## Diffusers\n\nDiffusers is a generative AI library for creating images and videos from text or images with diffusion models. LoRA is an especially popular training method for diffusion models because you can very quickly train and share diffusion models to generate images in new styles. To make it easier to use and try multiple LoRA models, Diffusers uses the PEFT library to help manage different adapters for inference.\n\nFor example, load a base model and then load the [artificialguybr/3DRedmond-V1](https://huggingface.co/artificialguybr/3DRedmond-V1) adapter for inference with the [`load_lora_weights`](https://huggingface.co/docs/diffusers/v0.24.0/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) method. The `adapter_name` argument in the loading method is enabled by PEFT and allows you to set a name for the adapter so it is easier to reference.\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n).to(\"cuda\")\npipeline.load_lora_weights(\n    \"peft-internal-testing/artificialguybr__3DRedmond-V1\", \n    weight_name=\"3DRedmond-3DRenderStyle-3DRenderAF.safetensors\", \n    adapter_name=\"3d\"\n)\nimage = pipeline(\"sushi rolls shaped like kawaii cat faces\").images[0]\nimage\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/test-lora-diffusers.png\"/>\n</div>", "## Usability over Performance\n\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.\n- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.\n\n## Simple over easy", "## Usability over Performance\n\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.\n- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.\n\n## Simple over easy", "## Popular libraries using \ud83e\udde8 Diffusers\n\n- https://github.com/microsoft/TaskMatrix\n- https://github.com/invoke-ai/InvokeAI\n- https://github.com/apple/ml-stable-diffusion\n- https://github.com/Sanster/lama-cleaner\n- https://github.com/IDEA-Research/Grounded-Segment-Anything\n- https://github.com/ashawkey/stable-dreamfusion\n- https://github.com/deep-floyd/IF\n- https://github.com/bentoml/BentoML\n- https://github.com/bmaltais/kohya_ss\n- +7000 other amazing GitHub repositories \ud83d\udcaa\n\nThank you for using us \u2764\ufe0f.\n\n## Credits\n\nThis library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We'd like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:\n\n- @CompVis' latent diffusion models library, available [here](https://github.com/CompVis/latent-diffusion)\n- @hojonathanho original DDPM implementation, available [here](https://github.com/hojonathanho/diffusion) as well as the extremely useful translation into PyTorch by @pesser, available [here](https://github.com/pesser/pytorch_diffusion)\n- @ermongroup's DDIM implementation, available [here](https://github.com/ermongroup/ddim)\n- @yang-song's Score-VE and Score-VP implementations, available [here](https://github.com/yang-song/score_sde_pytorch)", "\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\n\ud83e\udd17 Diffusers offers three core components:\n\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code.\n- Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality.\n- Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.\n\n## Installation", "--\ntitle: What's new in Diffusers? \ud83c\udfa8\nthumbnail: /blog/assets/102_diffusers_2nd_month/inpainting.png\nauthors:\n- user: osanseviero\n---\n\n# What's new in Diffusers? \ud83c\udfa8\n\n\nA month and a half ago we released `diffusers`, a library that provides a modular toolbox for diffusion models across modalities. A couple of weeks later, we released support for Stable Diffusion, a high quality text-to-image model, with a free demo for anyone to try out. Apart from burning lots of GPUs, in the last three weeks the team has decided to add one or two new features to the library that we hope the community enjoys! This blog post gives a high-level overview of the new features in `diffusers` version 0.3! Remember to give a \u2b50 to the [GitHub repository](https://github.com/huggingface/diffusers).", "If you\u2019re building products on top of \ud83e\udd17\u00a0Diffusers, we\u2019d love to chat to understand how we can make the library better together! Feel free to reach out to patrick@hf.co or sayak@hf.co.\n\n## Looking forward\n\nAs we celebrate our first anniversary, we're grateful to our community and open-source contributors who have helped us come so far in such a short time. We're happy to share that we'll be presenting a \ud83e\udd17\u00a0Diffusers demo at ICCV 2023 this fall \u2013 if you're attending, do come and see us! We'll continue to develop and improve our library, making it easier for everyone to use. We're also excited to see what the community will create next with our tools and resources. Thank you for being a part of our journey so far, and we look forward to continuing to democratize good machine learning together! \ud83e\udd73\n\n\u2764\ufe0f Diffusers team\n\n---\n\n**Acknowledgements**: Thank you to [Omar Sanseviero](https://huggingface.co/osanseviero), [Patrick von Platen](https://huggingface.co/patrickvonplaten), [Giada Pistilli](https://huggingface.co/giadap) for their reviews, and [Chunte Lee](https://huggingface.co/Chunte) for designing the thumbnail."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, mentioning the library's purpose as a modular toolbox for diffusion models and highlighting its focus on usability, simplicity, and customizability. It also correctly identifies the library's capabilities in generating images, audio, and 3D structures, which are factual statements about the Diffusers library's functionalities. However, the response is incomplete as it ends abruptly with \"The library aims to be,\" leaving the sentence unfinished. Despite this, the information given before the cutoff is accurate and relevant to the question asked."}, {"question": "What method does the EulerAncestralDiscreteScheduler use for sampling?\n", "true_answer": "Ancestral sampling with Euler method steps.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler_ancestral.md", "generated_answer": "The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps, which is a fast sampling method that can often generate good outputs in 20-30 steps. This scheduler is based on the original k-diffusion implementation by Katherine Crowson.", "retrieved_docs": ["- [`EulerDiscreteScheduler`],\n- [`LMSDiscreteScheduler`],\n- [`DDIMScheduler`],\n- [`DDPMScheduler`],\n- [`HeunDiscreteScheduler`],\n- [`DPMSolverMultistepScheduler`],\n- [`DEISMultistepScheduler`],\n- [`PNDMScheduler`],\n- [`EulerAncestralDiscreteScheduler`],\n- [`UniPCMultistepScheduler`],\n- [`KDPM2DiscreteScheduler`],\n- [`DPMSolverSinglestepScheduler`],\n- [`KDPM2AncestralDiscreteScheduler`].\n\nWe will now compare the input prompt with all other schedulers. To change the scheduler of the pipeline you can make use of the\nconvenient [`~ConfigMixin.config`] property in combination with the [`~ConfigMixin.from_config`] function.\n\n```python\npipeline.scheduler.config\n```\n\nreturns a dictionary of the configuration of the scheduler:\n\n**Output**:\n```py\nFrozenDict([('num_train_timesteps', 1000),\n            ('beta_start', 0.00085),\n            ('beta_end', 0.012),\n            ('beta_schedule', 'scaled_linear'),\n            ('trained_betas', None),\n            ('skip_prk_steps', True),\n            ('set_alpha_to_one', False),\n            ('prediction_type', 'epsilon'),\n            ('timestep_spacing', 'leading'),\n            ('steps_offset', 1),\n            ('_use_default_values', ['timestep_spacing', 'prediction_type']),\n            ('_class_name', 'PNDMScheduler'),\n            ('_diffusers_version', '0.21.4'),\n            ('clip_sample', False)])\n```", "<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_lms.png\" width=\"400\"/>\n    <br>\n</p>\n\n\n[`EulerDiscreteScheduler`] and [`EulerAncestralDiscreteScheduler`] can generate high quality results with as little as 30 steps.\n\n```python\nfrom diffusers import EulerDiscreteScheduler\n\npipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_euler_discrete.png\" width=\"400\"/>\n    <br>\n</p>\n\n\nand:\n\n```python\nfrom diffusers import EulerAncestralDiscreteScheduler\n\npipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_euler_ancestral.png\" width=\"400\"/>\n    <br>\n</p>\n\n\n[`DPMSolverMultistepScheduler`] gives a reasonable speed/quality trade-off and can be run with as little as 20 steps.\n\n```python\nfrom diffusers import DPMSolverMultistepScheduler", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# EulerAncestralDiscreteScheduler\n\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteSchedulerOutput", "```python\npipeline.scheduler.compatibles\n[\n    diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler,\n    diffusers.schedulers.scheduling_unipc_multistep.UniPCMultistepScheduler,\n    diffusers.schedulers.scheduling_k_dpm_2_discrete.KDPM2DiscreteScheduler,\n    diffusers.schedulers.scheduling_deis_multistep.DEISMultistepScheduler,\n    diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler,\n    diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler,\n    diffusers.schedulers.scheduling_ddpm.DDPMScheduler,\n    diffusers.schedulers.scheduling_dpmsolver_singlestep.DPMSolverSinglestepScheduler,\n    diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete.KDPM2AncestralDiscreteScheduler,\n    diffusers.utils.dummy_torch_and_torchsde_objects.DPMSolverSDEScheduler,\n    diffusers.schedulers.scheduling_heun_discrete.HeunDiscreteScheduler,\n    diffusers.schedulers.scheduling_pndm.PNDMScheduler,\n    diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler,\n    diffusers.schedulers.scheduling_ddim.DDIMScheduler,\n]\n```\n\nThe Stable Diffusion model uses the [`PNDMScheduler`] by default which usually requires ~50 inference steps, but more performant schedulers like [`DPMSolverMultistepScheduler`], require only ~20 or 25 inference steps. Use the [`~ConfigMixin.from_config`] method to load a new scheduler:\n\n```python\nfrom diffusers import DPMSolverMultistepScheduler", "Once training is complete, you can use your T2I-Adapter for inference:\n\n```py\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteSchedulerTest\nfrom diffusers.utils import load_image\nimport torch\n\nadapter = T2IAdapter.from_pretrained(\"path/to/adapter\", torch_dtype=torch.float16)\npipeline = StableDiffusionXLAdapterPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", adapter=adapter, torch_dtype=torch.float16\n)\n\npipeline.scheduler = EulerAncestralDiscreteSchedulerTest.from_config(pipe.scheduler.config)\npipeline.enable_xformers_memory_efficient_attention()\npipeline.enable_model_cpu_offload()\n\ncontrol_image = load_image(\"./conditioning_image_1.png\")\nprompt = \"pale golden rod circle with old lace background\"\n\ngenerator = torch.manual_seed(0)\nimage = pipeline(\n    prompt, image=control_image, generator=generator\n).images[0]\nimage.save(\"./output.png\")\n```\n\n## Next steps\n\nCongratulations on training a T2I-Adapter model! \ud83c\udf89 To learn more:\n\n- Read the [Efficient Controllable Generation for SDXL with T2I-Adapters](https://huggingface.co/blog/t2i-sdxl-adapters) blog post to learn more details about the experimental results from the T2I-Adapter team.", "| Euler a             | [`EulerAncestralDiscreteScheduler`] |                                                                                                               |\n| Heun                | [`HeunDiscreteScheduler`]           |                                                                                                               |\n| LMS                 | [`LMSDiscreteScheduler`]            |                                                                                                               |\n| LMS Karras          | [`LMSDiscreteScheduler`]            | init with `use_karras_sigmas=True`                                                                            |\n| N/A                 | [`DEISMultistepScheduler`]          |                                                                                                               |\n| N/A                 | [`UniPCMultistepScheduler`]         |                                                                                                               |", "Then load the saved weights into the pipeline:\n\n```python\n>>> pipeline = DiffusionPipeline.from_pretrained(\"./stable-diffusion-v1-5\", use_safetensors=True)\n```\n\nNow, you can run the pipeline as you would in the section above.\n\n### Swapping schedulers\n\nDifferent schedulers come with different denoising speeds and quality trade-offs. The best way to find out which one works best for you is to try them out! One of the main features of \ud83e\udde8 Diffusers is to allow you to easily switch between schedulers. For example, to replace the default [`PNDMScheduler`] with the [`EulerDiscreteScheduler`], load it with the [`~diffusers.ConfigMixin.from_config`] method:\n\n```py\n>>> from diffusers import EulerDiscreteScheduler\n\n>>> pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_safetensors=True)\n>>> pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n```\n\nTry generating an image with the new scheduler and see if you notice a difference!\n\nIn the next section, you'll take a closer look at the components - the model and scheduler - that make up the [`DiffusionPipeline`] and learn how to use these components to generate an image of a cat.\n\n## Models"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies that the EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps. It also provides additional context about the speed of the method and its origin, which is not required but does not detract from the accuracy of the answer. The response is completely correct, accurate, and factual."}, {"question": "What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n", "true_answer": "IDEFICS", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md", "generated_answer": "The name of the large multimodal model that can solve image-text tasks and is based on Flamingo is IDEFICS (Image-aware Decoder Enhanced \u00e0 la Flamingo with Interleaved Cross-attentionS). It was introduced as an open-access reproduction of Flamingo, which was initially developed by DeepMind but not publicly released. IDEFICS comes in two variants, the base version and the instructed version, at both", "retrieved_docs": ["## What is IDEFICS?\n\nIDEFICS is an 80 billion parameters multimodal model that accepts sequences of images and texts as input and generates coherent text as output. It can answer questions about images, describe visual content, create stories grounded in multiple images, etc.\n\nIDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source model across various image-text understanding benchmarks. It comes in two variants - 80 billion parameters and 9 billion parameters.\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/idefics/Figure_Evals_IDEFICS.png\" width=\"600\" alt=\"Plot comparing the performance of Flamingo, OpenFlamingo and IDEFICS\"/>\n</p>\n\nWe also provide fine-tuned versions [idefics-80B-instruct](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct) and [idefics-9B-instruct](https://huggingface.co/HuggingFaceM4/idefics-9b-instruct) adapted for conversational use cases.\n\n## Training Data\n\nIDEFICS was trained on a mixture of openly available datasets: Wikipedia, Public Multimodal Dataset, and LAION, as well as a new 115B token dataset called [OBELICS](https://huggingface.co/datasets/HuggingFaceM4/OBELICS) that we created. OBELICS consists of 141 million interleaved image-text documents scraped from the web and contains 353 million images.", "## Introduction\n\nRecent years have seen rapid advancements in computer vision and natural language processing. Still, many real-world \nproblems are inherently multimodal - they involve several distinct forms of data, such as images and text. \nVisual-language models face the challenge of combining modalities so that they can open the door to a wide range of \napplications. Some of the image-to-text tasks that visual language models can tackle include image captioning, image-text \nretrieval, and visual question answering. Image captioning can aid the visually impaired, create useful product descriptions, \nidentify inappropriate content beyond text, and more. Image-text retrieval can be applied in multimodal search, as well \nas in applications such as autonomous driving. Visual question-answering can aid in education, enable multimodal chatbots, \nand assist in various domain-specific information retrieval applications.", "[IDEFICS](../model_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198), \na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image \nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content, \ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b) \nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the \ud83e\udd17 Hub. For each variant, you can also find fine-tuned instructed \nversions of the model adapted for conversational use cases.\n\nThis model is exceptionally versatile and can be used for a wide range of image and multimodal tasks. However, \nbeing a large model means it requires significant computational resources and infrastructure. It is up to you to decide whether \nthis approach suits your use case better than fine-tuning specialized models for each individual task.", "The abstract from the paper is the following:\n\n*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\ndirection would be to use a single holistic universal model, as a \"foundation\", that targets all modalities\nat once -- a true vision and language foundation model should be good at vision tasks, language tasks, and\ncross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate\nimpressive performance on a wide range of 35 tasks spanning these target modalities.*\n\nThis model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava).\n\n## FlavaConfig\n\n[[autodoc]] FlavaConfig\n\n## FlavaTextConfig\n\n[[autodoc]] FlavaTextConfig\n\n## FlavaImageConfig\n\n[[autodoc]] FlavaImageConfig\n\n## FlavaMultimodalConfig\n\n[[autodoc]] FlavaMultimodalConfig\n\n## FlavaImageCodebookConfig\n\n[[autodoc]] FlavaImageCodebookConfig\n\n## FlavaProcessor\n\n[[autodoc]] FlavaProcessor\n\n## FlavaFeatureExtractor\n\n[[autodoc]] FlavaFeatureExtractor\n\n## FlavaImageProcessor\n\n[[autodoc]] FlavaImageProcessor\n    - preprocess\n\n## FlavaForPreTraining", "The abstract from the paper is the following:\n\n*We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.*", "The abstract from the paper is the following:\n\n*Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks that require reasoning over one or multiple images to generate a text. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELICS dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELISC, we train an 80 billion parameters vision and language model on the dataset and obtain competitive performance on various multimodal benchmarks. We release the code to reproduce the dataset along with the dataset itself.*\n\nThis model was contributed by [HuggingFaceM4](https://huggingface.co/HuggingFaceM4). The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>). (TODO: don't have a public link yet).\n\n\n<Tip warning={true}>\n\nIDEFICS modeling code in Transformers is for finetuning and inferencing the pre-trained IDEFICS models.\n\nTo train a new IDEFICS model from scratch use the m4 codebase (a link will be provided once it's made public)\n\n</Tip>\n\n\n## IdeficsConfig", "--\ntitle: \"Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Langage Model\"\nthumbnail: /blog/assets/idefics/thumbnail.png\nauthors:\n- user: HugoLaurencon\n- user: davanstrien\n- user: stas\n- user: Leyo\n- user: SaulLu\n- user: TimeRobber\n  guest: true\n- user: skaramcheti\n  guest: true\n- user: aps\n  guest: true\n- user: giadap\n- user: yjernite\n- user: VictorSanh\n---\n\n# Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model\n\n\nWe are excited to release IDEFICS (**I**mage-aware **D**ecoder **E**nhanced \u00e0 la **F**lamingo with **I**nterleaved **C**ross-attention**S**), an open-access visual language model. IDEFICS is based on [Flamingo](https://huggingface.co/papers/2204.14198), a state-of-the-art visual language model initially developed by DeepMind, which has not been released publicly. Similarly to GPT-4, the model accepts arbitrary sequences of image and text inputs and produces text outputs. IDEFICS is built solely on publicly available data and models (LLaMA v1 and OpenCLIP) and comes in two variants\u2014the base version and the instructed version. Each variant is available at the 9 billion and 80 billion parameter sizes."], "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response provided is incorrect. The name IDEFICS (Image-aware Decoder Enhanced \u00e0 la Flamingo with Interleaved Cross-attentionS) does not correspond to any known large multimodal model based on Flamingo. The correct answer should simply be \"IDEFICS\" as per the reference answer, but the response adds incorrect and misleading information. Therefore, the response does not meet the criteria for being correct, accurate, and factual."}, {"question": "What is the purpose of the `gradio.Blocks` API?\n", "true_answer": "The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/7.mdx", "generated_answer": "The `gradio.Blocks` API allows for the creation of custom and complex machine learning applications using a visual block-based interface. It provides a way to combine multiple demos into one web app, assign event triggers to components, automatically determine which components should be interactive or static, and create multi-step demos by reusing component outputs. The API also supports layout output components and provides a command to make it easier to file bug reports. Examples of usage are provided in the documentation.", "retrieved_docs": ["You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications \u2014 for free!\n\n## What's Next?", "You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n#### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications \u2014 for free!\n\n### What's Next?", "### 9. What does Gradio `Blocks` allow you to do?\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"Combine multiple demos into one web app\",\n\t\t\texplain: \"You can use the `with gradio.Tabs():` to add tabs for multiple demos\",\n\t\t\tcorrect: true\n        },\n        {\n\t\t\ttext: \"Assign event triggers such as clicked/changed/etc to `Blocks` components\",\n\t\t\texplain: \"When you assign an event, you pass in three parameters: fn: the function that should be called, inputs: the (list) of input component(s), and outputs: the (list) of output components that should be called.\",\n\t\t\tcorrect: true\n        },\n        {\n\t\t\ttext: \"Automatically determine which `Blocks` component should be interactive vs. static\",\n\t\t\texplain: \"Based on the event triggers you define, `Blocks` automatically figures out whether a component should accept user input or not.\",\n\t\t\tcorrect: true\n        },\n\t\t {\n\t\t\ttext: \"Create multi-step demos; meaning allowing you to reuse the output of one component as the input to the next\",\n\t\t\texplain: \"You can use a component for the input of one event trigger but the output of another.\",\n            correct: true\n        },\n\t]}\n/>\n\n### 10. You can share a public link to a `Blocks` demo and host a `Blocks` demo on Hugging Face spaces.", "if you'd like to provide more accurate info, implement the `_block_input(self, gr)` and `_block_output(self, gr)` methods of the tool. The `gr` variable is the gradio module (the result of `import gradio as gr`). It will be\n   automatically imported by the `GradiTool` parent class and passed to the `_block_input` and `_block_output` methods.", "Gradio Demo: blocks_style\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr", "Gradio Demo: blocks_static\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Image(\n        \"https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=387&q=80\"\n    )\n    gr.Textbox(\"hi\")\n    gr.Number(3)\n\nif __name__ == \"__main__\":\n    demo.launch()\n\n```", "Examples of usage:\n\n```py\nwith gr.Blocks() as demo:\n    gr.Textbox(interactive=True, text_align=\"right\")\ndemo.launch()\n```\n\n```py\nwith gr.Blocks() as demo:\n    gr.Markdown(\"\u0633\u0644\u0627\u0645\", rtl=True)\ndemo.launch()\n```\n\n- The `get_api_info` method of `Blocks` now supports layout output components [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4871](https://github.com/gradio-app/gradio/pull/4871)\n\n- Added the support for the new command `gradio environment`to make it easier for people to file bug reports if we shipped an easy command to list the OS, gradio version, and versions of gradio/gradio-client dependencies. bu [@varshneydevansh](https://github.com/varshneydevansh) in [PR 4915](https://github.com/gradio-app/gradio/pull/4915).\n\n### Bug Fixes:\n\n- The `.change()` event is fixed in `Video` and `Image` so that it only fires once by [@abidlabs](https://github.com/abidlabs) in [PR 4793](https://github.com/gradio-app/gradio/pull/4793)\n- The `.change()` event is fixed in `Audio` so that fires when the component value is programmatically updated by [@abidlabs](https://github.com/abidlabs) in [PR 4793](https://github.com/gradio-app/gradio/pull/4793)"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It aligns well with the reference answer by mentioning the control over data flows and layout of applications, which is a key aspect of the `gradio.Blocks` API. The response goes into detail about the capabilities of the API, such as creating custom machine learning applications, combining multiple demos, assigning event triggers, and creating multi-step demos. It also correctly identifies additional features like layout output components and a command for filing bug reports. However, the mention of a visual block-based interface is not directly supported by the reference answer, which may introduce a slight inaccuracy if the `gradio.Blocks` API does not primarily function in that manner. Nonetheless, the response is comprehensive and provides a clear understanding of the purpose of the `gradio.Blocks` API, which is to allow full control over the data flows and layout for building complex applications."}, {"question": "What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n", "true_answer": "The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md", "generated_answer": "The purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" is to leverage the robust representations of images learned by contrastive models like CLIP, specifically for image generation. The model consists of a prior that generates a CLIP image embedding from a text caption, and a decoder that generates an image conditioned on the image embedding. This approach improves image diversity with minimal loss in photorealism and", "retrieved_docs": ["This model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/openai/CLIP).\n\n## Usage tips and example\n\nCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text\nfeatures. Both the text and visual features are then projected to a latent space with identical dimension. The dot\nproduct between the projected image and text features is then used as a similar score.\n\nTo feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-overlapping patches,\nwhich are then linearly embedded. A [CLS] token is added to serve as representation of an entire image. The authors\nalso add absolute position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder.\nThe [`CLIPImageProcessor`] can be used to resize (or rescale) and normalize images for the model.\n\nThe [`CLIPTokenizer`] is used to encode the text. The [`CLIPProcessor`] wraps\n[`CLIPImageProcessor`] and [`CLIPTokenizer`] into a single instance to both\nencode the text and prepare the images. The following example shows how to get the image-text similarity scores using\n[`CLIPProcessor`] and [`CLIPModel`].\n\n\n```python\n>>> from PIL import Image\n>>> import requests\n\n>>> from transformers import CLIPProcessor, CLIPModel", "The abstract from the paper is:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\n## Tips\n\nStable unCLIP takes  `noise_level` as input during inference which determines how much noise is added to the image embeddings. A higher `noise_level` increases variation in the final un-noised images. By default, we do not add any additional noise to the image embeddings (`noise_level = 0`).", "The abstract from the paper is following:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\nYou can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch).\n\n<Tip>", "| **Task**                     | **Description**                                                                                              | **Pipeline**\n|------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------|\n| Unconditional Image Generation          | generate an image from Gaussian noise | [unconditional_image_generation](./using-diffusers/unconditional_image_generation) |\n| Text-Guided Image Generation | generate an image given a text prompt | [conditional_image_generation](./using-diffusers/conditional_image_generation) |\n| Text-Guided Image-to-Image Translation     | adapt an image guided by a text prompt | [img2img](./using-diffusers/img2img) |\n| Text-Guided Image-Inpainting          | fill the masked part of an image given the image, the mask and a text prompt | [inpaint](./using-diffusers/inpaint) |\n| Text-Guided Depth-to-Image Translation | adapt parts of an image guided by a text prompt while preserving structure via depth estimation | [depth2img](./using-diffusers/depth2img) |", "#### Model\n\nOur model is just the fine-tuned version of the original CLIP model shown below. Inputs to the model are a batch of captions and a batch of images passed through the CLIP text encoder and image encoder respectively. The training process uses [contrastive learning](https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607) to learn a joint embedding representation of image and captions. In this embedding space, images and their respective captions are pushed close together, as are similar images and similar captions. Conversely, images and captions for different images, or dissimilar images and captions, are likely to be pushed further apart.\n\n<img src=\"/blog/assets/30_clip_rsicd/clip_schematic.png\"/>\n<center><i>CLIP Training and Inference (Image Credit: CLIP: Connecting Text and Images (https://openai.comclip/))</i></center>\n\n\n#### Data Augmentation\n\nIn order to regularize our dataset and prevent overfitting due to the size of the dataset, we used both image and text augmentation.\n\nImage augmentation was done inline using built-in transforms from Pytorch's [Torchvision](https://pytorch.org/vision/stable/index.html) package. The transformations used were Random Cropping, Random Resizing and Cropping, Color Jitter, and Random Horizontal and Vertical flipping.", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Prior Transformer\n\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process.\n\nThe abstract from the paper is:", "The abstract from the paper is:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\n## PriorTransformer\n\n[[autodoc]] PriorTransformer\n\n## PriorTransformerOutput\n\n[[autodoc]] models.prior_transformer.PriorTransformerOutput"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It accurately describes the purpose of the two-stage model as leveraging the representations learned by CLIP for image generation. It correctly identifies the two components of the model: a prior that generates a CLIP image embedding from text, and a decoder that generates an image based on this embedding. However, the response is incomplete as it ends abruptly and does not finish the explanation it seems to begin about the specific improvements the model brings to image diversity and photorealism. Despite this, the information given aligns well with the reference answer, which states the purpose of generating an image embedding from text and then an image from the embedding."}, {"question": "What command is used to install the requirements for a research project using \ud83e\udd17 Transformers?\n", "true_answer": "pip install -r requirements.txt", "source_doc": "huggingface/transformers/blob/main/examples/research_projects/README.md", "generated_answer": "To install the requirements for a research project using \ud83e\udd17 Transformers, you need to navigate to the folder of your chosen project and run the following command:\n\n```bash\npip install -r requirements.txt\n```\n\nThis command will install all the necessary packages required to run the project. Note that some research projects may require a specific version of Diffusers, which will be indicated in the requirements file of that folder. Updating them to the", "retrieved_docs": ["or for an editable install:\n\n```bash\npip install -e .[dev]\n```\n\ninside the Transformers repo. Since the number of optional dependencies of Transformers has grown a lot, it's possible you don't manage to get all of them. If the dev install fails, make sure to install the Deep Learning framework you are working with (PyTorch, TensorFlow and/or Flax) then do\n\n```bash\npip install transformers[quality]\n```\n\nor for an editable install:\n\n```bash\npip install -e .[quality]\n```\n\n\n## Tests\n\nAll the jobs that begin with `ci/circleci: run_tests_` run parts of the Transformers testing suite. Each of those jobs focuses on a part of the library in a certain environment: for instance `ci/circleci: run_tests_pipelines_tf` runs the pipelines test in an environment where TensorFlow only is installed.\n\nNote that to avoid running tests when there is no real change in the modules they are testing, only part of the test suite is run each time: a utility is run to determine the differences in the library between before and after the PR (what GitHub shows you in the \"Files changes\" tab) and picks the tests impacted by that diff. That utility can be run locally with:\n\n```bash\npython utils/tests_fetcher.py\n```\n\nfrom the root of the Transformers repo. It will:", "After that we can install the required dependencies\n\n```bash\n!pip install transformers \"datasets[s3]\" sagemaker --upgrade\n```\n\n[install](https://github.com/git-lfs/git-lfs/wiki/Installation) `git-lfs` for model upload.\n\n```bash\n!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash\n!sudo yum install git-lfs -y\n!git lfs install\n```\n\nTo run training on SageMaker we need to create a sagemaker Session and provide an IAM role with the right permission. This IAM role will be later attached to the `TrainingJob` enabling it to download data, e.g. from Amazon S3.\n\n```python\nimport sagemaker\n\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n\nprint(f\"IAM role arn used for running training: {role}\")\nprint(f\"S3 bucket used for storing artifacts: {sess.default_bucket()}\")\n\n```\n\n---\n\n# Choose \ud83e\udd17 Transformers `examples/` script\n\nThe [\ud83e\udd17 Transformers repository](https://github.com/huggingface/transformers/tree/master/examples) contains several `examples/`scripts for fine-tuning models on tasks from `language-modeling` to `token-classification`. In our case, we are using the `run_summarization.py` from the `seq2seq/` examples. \n\n***Note**: you can use this tutorial as-is to train your model on a different examples script.*", "While we strive to present as many use cases as possible, the example scripts are just that - examples. It is expected that they won't work out-of-the-box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs. To help you with that, most of the examples fully expose the preprocessing of the data, allowing you to tweak and edit them as required.\n\nPlease discuss on the [forum](https://discuss.huggingface.co/) or in an [issue](https://github.com/huggingface/transformers/issues) a feature you would like to implement in an example before submitting a PR; we welcome bug fixes, but since we want to keep the examples as simple as possible it's unlikely that we will merge a pull request adding more functionality at the cost of readability.\n\n## Important note\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, you have to **install the library from source** and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install .\n```\nThen cd in the example folder of your choice and run\n```bash\npip install -r requirements.txt\n```\n\nTo browse the examples corresponding to released versions of \ud83e\udd17 Transformers, click on the line below and then on your desired version of the library:", "To try it out, first install the necessary requirements\n\n\n```bash\npip install ray\npip install transformers\npip install -r transformers/examples/research_projects/rag/requirements.txt\n```\n\n\nThen, you can specify your data paths and other configurations and run [finetune-rag-ray.sh](https://github.com/huggingface/transformers/blob/master/examples/research_projects/rag/finetune_rag_ray.sh)!\n\n\n```bash\n# Sample script to finetune RAG using Ray for distributed retrieval.\n\n# Add parent directory to python path to access lightning_base.py\nexport PYTHONPATH=\"../\":\"${PYTHONPATH}\"\n\n# Start a single-node Ray cluster.\nray start --head\n\n# A sample finetuning run, you need to specify data_dir, output_dir and model_name_or_path\n# run ./examples/rag/finetune_rag_ray.sh --help to see all the possible options", "Then switch your current clone of \ud83e\udd17 Transformers to a specific version, like v3.5.1 for example:\n\n```bash\ngit checkout tags/v3.5.1\n```\n\nAfter you've setup the correct library version, navigate to the example folder of your choice and install the example specific requirements:\n\n```bash\npip install -r requirements.txt\n```\n\n## Run a script\n\n<frameworkcontent>\n<pt>\nThe example script downloads and preprocesses a dataset from the \ud83e\udd17 [Datasets](https://huggingface.co/docs/datasets/) library. Then the script fine-tunes a dataset with the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) on an architecture that supports summarization. The following example shows how to fine-tune [T5-small](https://huggingface.co/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset. The T5 model requires an additional `source_prefix` argument due to how it was trained. This prompt lets T5 know this is a summarization task.", "1. To begin with, we should clone transformers localy and install all the required packages.\n\nFirst, you need to clone the `transformers` repo with:\n\n```\n$ git clone https://github.com/huggingface/transformers.git\n```\n\nSecond, head over to the `examples/research_projects/wav2vec2` directory, where the `run_common_voice.py` script is located.\n\n```\n$ cd transformers/examples/research_projects/wav2vec2\n```\n\nThird, install the required packages. The\npackages are listed in the `requirements.txt` file and can be installed with\n\n```\n$ pip install -r requirements.txt\n```\n\n\t**Note**: Installing the latest version of `torchaudio` will also upgrade `torch` to it's latest stable version. If you are using specific version of `torch` then make sure\n\tto use the correct `torchaudio` version compatible with your version of `torch`. By default the `requirements.txt` will install the latest version of `torchaudio`.\n\n2. Next, take a look at the `run_common_voice.py` script to get an understanding of how it works. In short the script does the following:\n\n\t- Load the given common voice dataset\n\t- Create vocab for the language\n\t- Load the model with given hyperparameters\n\t- Pre-process the dataset to input into the model\n\t- Run training\n\t- Run evaluation", "Research projects\n\nThis folder contains various research projects using \ud83e\udde8 Diffusers.\nThey are not really maintained by the core maintainers of this library and often require a specific version of Diffusers that is indicated in the requirements file of each folder.\nUpdating them to the most recent version of the library will require some work.\n\nTo use any of them, just run the command\n\n```\npip install -r requirements.txt\n```\ninside the folder of your choice.\n\nIf you need help with any of those, please open an issue where you directly ping the author(s), as indicated at the top of the README of each folder."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual based on the reference answer. It correctly identifies the command 'pip install -r requirements.txt' as the method to install the requirements for a research project using \ud83e\udd17 Transformers. The additional context about navigating to the project folder and the mention of potential specific version requirements for Diffusers adds useful information without deviating from the accuracy of the response."}, {"question": "What task does the `roberta-large-mnli` checkpoint perform?\n", "true_answer": "Text classification", "source_doc": "huggingface/course/blob/main/chapters/en/chapter1/10.mdx", "generated_answer": "The `roberta-large-mnli` checkpoint is pre-trained on the Natural Language Inference (NLI) task from the MultiNLI (mnli) dataset. It is a large-sized RoBERTa model specifically trained for this task. In other words, it can be used for natural language inference, which involves predicting whether a hypothesis is entailed by a premise or not. This checkpoint can be loaded and fine-tuned for", "retrieved_docs": ["## Usage tips\n\n- The checkpoints are named **mobilenet\\_v2\\_*depth*\\_*size***, for example **mobilenet\\_v2\\_1.0\\_224**, where **1.0** is the depth multiplier (sometimes also referred to as \"alpha\" or the width multiplier) and **224** is the resolution of the input images the model was trained on.\n\n- Even though the checkpoint is trained on images of specific size, the model will work on images of any size. The smallest supported image size is 32x32.\n\n- One can use [`MobileNetV2ImageProcessor`] to prepare images for the model.\n\n- The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes). However, the model predicts 1001 classes: the 1000 classes from ImageNet plus an extra \u201cbackground\u201d class (index 0).\n\n- The segmentation model uses a [DeepLabV3+](https://arxiv.org/abs/1802.02611) head. The available semantic segmentation checkpoints are pre-trained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n\n- The original TensorFlow checkpoints use different padding rules than PyTorch, requiring the model to determine the padding amount at inference time, since this depends on the input image size. To use native PyTorch padding behavior, create a [`MobileNetV2Config`] with `tf_padding = False`.\n\nUnsupported features:", "## Models\n\n### RoBERTa\n\n#### Load RoBERTa Checkpoints for the Classification Task\n\nWe load the pre-trained RoBERTa model with a sequence classification head using the Hugging Face `AutoModelForSequenceClassification` class:\n\n```python\nfrom transformers import AutoModelForSequenceClassification \nroberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint, num_labels=2)\n```\n\n\n####  LoRA setup for RoBERTa classifier\n\nWe import LoRa configuration and set some parameters for RoBERTa classifier:\n- TaskType: Sequence classification\n- r(rank): Rank for our decomposition matrices\n- lora_alpha: Alpha parameter to scale the learned weights. LoRA paper advises fixing alpha at 16\n- lora_dropout: Dropout probability of the LoRA layers\n- bias: Whether to add bias term to LoRa layers\n\nThe code below uses the values recommended by the [Lora paper](https://arxiv.org/abs/2106.09685). [Later in this post](#hyperparameter-tuning) we will perform hyperparameter tuning of these parameters using `wandb`.\n\n```python\nfrom peft import get_peft_model, LoraConfig, TaskType\nroberta_peft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n)\nroberta_model = get_peft_model(roberta_model, roberta_peft_config)\nroberta_model.print_trainable_parameters()\n```", "If you're interested in using one of the official checkpoints for a task, explore the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations!\n\n</Tip>\n\n## StableDiffusionInpaintPipeline\n\n[[autodoc]] StableDiffusionInpaintPipeline\n\t- all\n\t- __call__\n\t- enable_attention_slicing\n\t- disable_attention_slicing\n\t- enable_xformers_memory_efficient_attention\n\t- disable_xformers_memory_efficient_attention\n\t- load_textual_inversion\n\t- load_lora_weights\n\t- save_lora_weights\n\n## StableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput\n\n## FlaxStableDiffusionInpaintPipeline\n\n[[autodoc]] FlaxStableDiffusionInpaintPipeline\n\t- all\n\t- __call__\n\n## FlaxStableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput", "### Performance\n\nWe get the following results for [roberta-base](https://huggingface.co/roberta-base) and [roberta-large](https://huggingface.co/roberta-large)\nmixed precision training(fp16) on sst2 dataset under PyTorch and ONNX Runtime backends. A single Nvidia A100 card was used to run the\nexperiment for 3 epochs::\n\n| Model           | Backend      | Runtime(s) | Train samples(/s) |\n| --------------- |------------- | ---------- | ----------------- |\n| roberta-base    | PyTorch      | 752.3      | 268.6             |\n| roberta-base    | ONNX Runtime | 729.7      | 276.9             |\n| roberta-large   | PyTorch      | 3523.7     | 57.3              |\n| roberta-large   | ONNX Runtime | 2986.6     | 67.7              |\n\nWe observe the gain of ONNX Runtime compared to PyTorch as follow:\n\n| Model         | Latency | Throughput |\n| ------------- | ------- | ---------- |\n| roberta-base  | 2.99%   | 3.08%      |\n| roberta-large | 15.24%  | 17.98%     |\n\n\n__Note__\n> *To enable ONNX Runtime training, your devices need to be equipped with GPU. Install the dependencies either with our prepared*\n*[Dockerfiles](https://github.com/huggingface/optimum/blob/main/examples/onnxruntime/training/docker/) or follow the instructions*\n*in [`torch_ort`](https://github.com/pytorch/ort/blob/main/torch_ort/docker/README.md).*\n\n> *The inference will use PyTorch by default, if you want to use ONNX Runtime backend instead, add the flag `--inference_with_ort`.*\n---", "```python\nfrom transformers import BigBirdModel\n\n# loading bigbird from its pretrained checkpoint\nmodel = BigBirdModel.from_pretrained(\"google/bigbird-roberta-base\")\n# This will init the model with default configuration i.e. attention_type = \"block_sparse\" num_random_blocks = 3, block_size = 64.\n# But You can freely change these arguments with any checkpoint. These 3 arguments will just change the number of tokens each query token is going to attend.\nmodel = BigBirdModel.from_pretrained(\"google/bigbird-roberta-base\", num_random_blocks=2, block_size=16)\n\n# By setting attention_type to `original_full`, BigBird will be relying on the full attention of n^2 complexity. This way BigBird is 99.9 % similar to BERT.\nmodel = BigBirdModel.from_pretrained(\"google/bigbird-roberta-base\", attention_type=\"original_full\")\n```\n\nThere are total **3 checkpoints** available in **\ud83e\udd17Hub** (at the point of writing this article): [`bigbird-roberta-base`](https://huggingface.co/google/bigbird-roberta-base), [`bigbird-roberta-large`](https://huggingface.co/google/bigbird-roberta-large), [`bigbird-base-trivia-itc`](https://huggingface.co/google/bigbird-base-trivia-itc). The first two checkpoints come from pretraining `BigBirdForPretraining` with `masked_lm loss`; while the last one corresponds to the checkpoint after finetuning `BigBirdForQuestionAnswering` on `trivia-qa` dataset.", "## Usage tips\n\n- To combine the Language-Independent Layout Transformer with a new RoBERTa checkpoint from the [hub](https://huggingface.co/models?search=roberta), refer to [this guide](https://github.com/jpWang/LiLT#or-generate-your-own-checkpoint-optional).\nThe script will result in `config.json` and `pytorch_model.bin` files being stored locally. After doing this, one can do the following (assuming you're logged in with your HuggingFace account):\n\n```\nfrom transformers import LiltModel\n\nmodel = LiltModel.from_pretrained(\"path_to_your_files\")\nmodel.push_to_hub(\"name_of_repo_on_the_hub\")\n```\n\n- When preparing data for the model, make sure to use the token vocabulary that corresponds to the RoBERTa checkpoint you combined with the Layout Transformer.\n- As [lilt-roberta-en-base](https://huggingface.co/SCUT-DLVCLab/lilt-roberta-en-base) uses the same vocabulary as [LayoutLMv3](layoutlmv3), one can use [`LayoutLMv3TokenizerFast`] to prepare data for the model.\nThe same is true for [lilt-roberta-en-base](https://huggingface.co/SCUT-DLVCLab/lilt-infoxlm-base): one can use [`LayoutXLMTokenizerFast`] for that model.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with LiLT.\n\n- Demo notebooks for LiLT can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/LiLT).", "#### (Important) More details on how to create a debugging environment for BigBird \n\n- BigBird has multiple pretrained checkpoints that should eventually all be ported to \n  \ud83e\udd17 Transformers. The pretrained checkpoints can be found [here](https://console.cloud.google.com/storage/browser/bigbird-transformer/pretrain;tab=objects?prefix=&forceOnObjectsSortingFiltering=false). \n\tThose checkpoints include both pretrained weights for encoder-only (BERT/RoBERTa) under the folder `bigbr_base` and encoder-decoder (PEGASUS) under the folder `bigbp_large`.\n\tYou should start by porting the `bigbr_base` model. The encoder-decoder model \n\tcan be ported afterward.\n\tfor an encoder-decoder architecture as well as an encoder-only architecture. \n- BigBird was written in tf.compat meaning that a mixture of a TensorFlow 1 and \n  TensorFlow 2 API was used.\n- The most important part of the BigBird code-base is [bigbird.bigbird.core](https://github.com/google-research/bigbird/tree/master/bigbird/core) which includes all logic necessary \n  to implement BigBird.\n- The first goal should be to successfully run a forward pass using the RoBERTa checkpoint `bigbr_base/model.ckpt-0.data-00000-of-00001` and `bigbr_base/model.ckpt-0.index`.\n\n\n### Port BigBird to \ud83e\udd17 Transformers\n\nNext, you can finally start adding new code to \ud83e\udd17 Transformers. Go into\nthe clone of your \ud83e\udd17 Transformers' fork:\n\n    cd transformers"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately describes the task that the `roberta-large-mnli` checkpoint performs, which is natural language inference as part of the MultiNLI dataset. The answer is factual and provides additional context about the nature of the task and the model's training. It aligns with the reference answer, which is text classification, since natural language inference is a form of text classification where the model classifies relationships between sentences. Therefore, the response is correct and factual."}, {"question": "What service is replacing the Paid tier of the Inference API at Hugging Face?\n", "true_answer": "Inference Endpoints", "source_doc": "huggingface/blog/blob/main/pricing-update.md", "generated_answer": "The service replacing the Paid tier of the Inference API at Hugging Face is [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index). It offers a higher free-tier for authenticated users, mandatory token usage for private models and endpoints, and dedicated infrastructure on a cloud provider of choice for production deployment. The pricing for Inference Endpoints is based on hourly compute and billed monthly, starting at $", "retrieved_docs": ["---\n\n## \ud83e\udd17 Accelerated Inference API\n\nThe [Accelerated Inference API](https://huggingface.co/inference-api) is our hosted service to run inference on any of the 10,000+ models publicly available on the \ud83e\udd17 Model Hub, or your own private models, via simple API calls. The API includes acceleration on CPU and GPU with [up to 100x speedup](https://huggingface.co/blog/accelerated-inference) compared to out of the box deployment of Transformers.\n\nTo integrate Few-Shot Learning predictions with `GPT-Neo` in your own apps, you can use the \ud83e\udd17 Accelerated Inference API with the code snippet below. You can find your API Token [here](https://huggingface.co/settings/token), if you don't have an account you can get started [here](https://huggingface.co/pricing).\n\n```python\nimport json\nimport requests\n\nAPI_TOKEN = \"\"\n\ndef query(payload='',parameters=None,options={'use_cache': False}):\n    API_URL = \"https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B\"\n\t\theaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n    body = {\"inputs\":payload,'parameters':parameters,'options':options}\n    response = requests.request(\"POST\", API_URL, headers=headers, data= json.dumps(body))\n    try:\n      response.raise_for_status()\n    except requests.exceptions.HTTPError:\n        return \"Error:\"+\" \".join(response.json()['error'])\n    else:\n      return response.json()[0]['generated_text']", "<Tip>\n\nAuthentication is NOT mandatory when using the Inference API. However, authenticated users get a higher free-tier to\nplay with the service. Token is also mandatory if you want to run inference on your private models or on private\nendpoints.\n\n</Tip>\n\n## Supported tasks\n\n[`InferenceClient`]'s goal is to provide the easiest interface to run inference on Hugging Face models. It\nhas a simple API that supports the most common tasks. Here is a list of the currently supported tasks:", "## What is Hugging Face Inference Endpoints\n\n[Hugging Face Inference Endpoints](https://ui.endpoints.huggingface.co/) offers an easy and secure way to deploy Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, and offering advanced security. \n\nHere are some of the most important features for LLM deployment:", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Run Inference on servers\n\nInference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive,\nrunning on a dedicated server can be an interesting option. The `huggingface_hub` library provides an easy way to call a\nservice that runs inference for hosted models. There are several services you can connect to:\n- [Inference API](https://huggingface.co/docs/api-inference/index): a service that allows you to run accelerated inference\non Hugging Face's infrastructure for free. This service is a fast way to get started, test different models, and\nprototype AI products.\n- [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index): a product to easily deploy models to production.\nInference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice.\n\nThese services can be called with the [`InferenceClient`] object. It acts as a replacement for the legacy\n[`InferenceApi`] client, adding specific support for tasks and handling inference on both\n[Inference API](https://huggingface.co/docs/api-inference/index) and [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index).\nLearn how to migrate to the new client in the [Legacy InferenceAPI client](#legacy-inferenceapi-client) section.", "### Text Generation Inference\n\nResponse time and latency for concurrent users are a big challenge for serving these large models. To tackle this problem, Hugging Face has released [text-generation-inference](https://github.com/huggingface/text-generation-inference) (TGI), an open-source serving solution for large language models built on Rust, Python, and gRPc. TGI is integrated into inference solutions of Hugging Face, [Inference Endpoints](https://huggingface.co/inference-endpoints), and [Inference API](https://huggingface.co/inference-api), so you can directly create an endpoint with optimized inference with few clicks, or simply send a request to Hugging Face's Inference API to benefit from it, instead of integrating TGI to your platform. \n\n![Screenshot from HuggingChat](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/os_llms/huggingchat_ui.png)", "</Tip>\n\n### Using a specific URL\n\nThe examples we saw above use the free-hosted Inference API. This proves to be very useful for prototyping\nand testing things quickly. Once you're ready to deploy your model to production, you'll need to use a dedicated infrastructure.\nThat's where [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) comes into play. It allows you to deploy\nany model and expose it as a private API. Once deployed, you'll get a URL that you can connect to using exactly the same\ncode as before, changing only the `model` parameter:\n\n```python\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient(model=\"https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if\")\n# or\n>>> client = InferenceClient()\n>>> client.text_to_image(..., model=\"https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if\")\n```\n\n### Authentication\n\nCalls made with the [`InferenceClient`] can be authenticated using a [User Access Token](https://huggingface.co/docs/hub/security-tokens).\nBy default, it will use the token saved on your machine if you are logged in (check out\n[how to authenticate](https://huggingface.co/docs/huggingface_hub/quick-start#authentication)). If you are not logged in, you can pass\nyour token as an instance parameter:\n\n```python\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient(token=\"hf_***\")\n```\n\n<Tip>", "Access \ud83e\udd17 Inference Endpoints\n\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account.\n\n<Tip>\n  \nYou can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method.\n  \n</Tip>\n\nThere are two pricing plans:\n\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. \n- There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested!\n\nAfter you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying! \ud83e\udd73"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies \"Inference Endpoints\" as the service replacing the Paid tier of the Inference API at Hugging Face. The additional details provided about the features and pricing of the Inference Endpoints service are accurate and factual, aligning with the reference answer's indication of the correct service. Therefore, the response meets the criteria for a score of 5."}, {"question": "What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n", "true_answer": "Grouped convolutions", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/squeezebert.md", "generated_answer": "SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers, as described in the context provided. This architectural feature is also known as \"mixconv\" and was proposed in the paper \"Squeeze-and-Excitation Networks\" by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. The implementation of this feature", "retrieved_docs": ["!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SqueezeBERT\n\n## Overview\n\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers.\n\nThe abstract from the paper is the following:", "## Squeeze-and-Excitation Networks\n\n* Implementation: [senet.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/senet.py)\nNOTE: I am deprecating this version of the networks, the new ones are part of `resnet.py`\n\n* Paper: `Squeeze-and-Excitation Networks` - https://arxiv.org/abs/1709.01507\n* Code: https://github.com/Cadene/pretrained-models.pytorch \n\n## TResNet\n\n* Implementation: [tresnet.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/tresnet.py)\n* Paper: `TResNet: High Performance GPU-Dedicated Architecture` - https://arxiv.org/abs/2003.13630\n* Code: https://github.com/mrT23/TResNet\n\n## VGG\n\n* Implementation: [vgg.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vgg.py)\n* Paper: `Very Deep Convolutional Networks For Large-Scale Image Recognition` - https://arxiv.org/pdf/1409.1556.pdf\n* Reference code: https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n\n## Vision Transformer\n\n* Implementation: [vision_transformer.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py)\n* Paper: `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale` - https://arxiv.org/abs/2010.11929\n* Reference code and pretrained weights: https://github.com/google-research/vision_transformer\n\n## VovNet V2 and V1", "## Squeeze-and-Excitation Networks [[senet.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/senet.py)]\nNOTE: I am deprecating this version of the networks, the new ones are part of `resnet.py`\n\n* Paper: `Squeeze-and-Excitation Networks` - https://arxiv.org/abs/1709.01507\n* Code: https://github.com/Cadene/pretrained-models.pytorch \n\n## TResNet [[tresnet.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/tresnet.py)]\n* Paper: `TResNet: High Performance GPU-Dedicated Architecture` - https://arxiv.org/abs/2003.13630\n* Code: https://github.com/mrT23/TResNet\n\n## VGG [[vgg.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vgg.py)]\n* Paper: `Very Deep Convolutional Networks For Large-Scale Image Recognition` - https://arxiv.org/pdf/1409.1556.pdf\n* Reference code: https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n\n## Vision Transformer [[vision_transformer.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py)]\n* Paper: `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale` - https://arxiv.org/abs/2010.11929\n* Reference code and pretrained weights: https://github.com/google-research/vision_transformer", "This model was contributed by [forresti](https://huggingface.co/forresti).\n\n## Usage tips\n\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n  rather than the left.\n- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\n  with a causal language modeling (CLM) objective are better in that regard.\n- For best results when finetuning on sequence classification tasks, it is recommended to start with the\n  *squeezebert/squeezebert-mnli-headless* checkpoint.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## SqueezeBertConfig\n\n[[autodoc]] SqueezeBertConfig\n\n## SqueezeBertTokenizer\n\n[[autodoc]] SqueezeBertTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## SqueezeBertTokenizerFast\n\n[[autodoc]] SqueezeBertTokenizerFast\n\n## SqueezeBertModel\n\n[[autodoc]] SqueezeBertModel\n\n## SqueezeBertForMaskedLM\n\n[[autodoc]] SqueezeBertForMaskedLM", "The feature is still experimental, because:\n+ we can make it much more robust if we have memory mapped/preprocessed datasets.\n+ The speedup over sortish sampler is not that large at the moment.\n\n# DistilBART\n<!---It should be called distilling bart and pegasus, but I don't want to break the link in the paper.-->\nThis section describes all code and artifacts from our [Paper](http://arxiv.org/abs/2010.13002)\n\n![DBART](https://huggingface.co/front/thumbnails/distilbart_large.png)", "- MixConv\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - MNAS\n    Training Data:\n    - ImageNet\n    ID: tf_mixnet_m\n    Crop Pct: '0.875'\n    Image Size: '224'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1709\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_m-0f4d8805.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 76.96%\n      Top 5 Accuracy: 93.16%\n- Name: tf_mixnet_s\n  In Collection: TF MixNet\n  Metadata:\n    FLOPs: 302587678\n    Parameters: 4130000\n    File Size: 16738218\n    Architecture:\n    - Batch Normalization\n    - Dense Connections\n    - Dropout\n    - Global Average Pooling\n    - Grouped Convolution\n    - MixConv\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - MNAS\n    Training Data:\n    - ImageNet\n    ID: tf_mixnet_s\n    Crop Pct: '0.875'\n    Image Size: '224'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1698\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_s-89d3354b.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet", "- MixConv\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - MNAS\n    Training Data:\n    - ImageNet\n    ID: tf_mixnet_m\n    Crop Pct: '0.875'\n    Image Size: '224'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1709\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_m-0f4d8805.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 76.96%\n      Top 5 Accuracy: 93.16%\n- Name: tf_mixnet_s\n  In Collection: TF MixNet\n  Metadata:\n    FLOPs: 302587678\n    Parameters: 4130000\n    File Size: 16738218\n    Architecture:\n    - Batch Normalization\n    - Dense Connections\n    - Dropout\n    - Global Average Pooling\n    - Grouped Convolution\n    - MixConv\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - MNAS\n    Training Data:\n    - ImageNet\n    ID: tf_mixnet_s\n    Crop Pct: '0.875'\n    Image Size: '224'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1698\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_s-89d3354b.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response correctly identifies that SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. However, the additional information provided about \"mixconv\" and the reference to the paper \"Squeeze-and-Excitation Networks\" is incorrect as \"mixconv\" is not synonymous with grouped convolutions and the paper mentioned is not related to SqueezeBERT. The response contains a mix of correct and incorrect information, which affects its accuracy."}, {"question": "What type of license is the HuggingFace Team's software distributed under?\n", "true_answer": "Apache License, Version 2.0", "source_doc": "huggingface/simulate/blob/main/docs/source/tutorials/interaction.mdx", "generated_answer": "The HuggingFace Team's software is distributed under the Apache License, Version 2.0, with the exception of a specific Markdown file (see question 5) that may contain specific syntax for their doc-builder. This license requires compliance with its terms and conditions, and software distributed under it is provided \"AS IS\" without any warranties or conditions, either express or implied. The specific language governing permissions and limitations under the license can be found on the Apache website. (An", "retrieved_docs": ["!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Under construction", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Under construction", "!---\nCopyright 2022 - The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MEGA\n\n## Overview", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Scenes\n\nUnder construction \ud83d\udea7."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the license as the Apache License, Version 2.0, which matches the reference answer. The additional details provided about the license and its terms, while not required, do not detract from the accuracy of the response. The response is factual and provides the correct information as per the reference answer."}, {"question": "What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n", "true_answer": "Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md", "generated_answer": "The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:\n1. Splitting the larger vocabulary embedding into two smaller matrices.\n2. Allowing layers to share parameters.\nThese techniques help to scale the model better compared to the original BERT, as empirical evidence shows that they lead to models that require fewer parameters and consume less memory while still achieving better performance on downstream tasks.", "retrieved_docs": ["* Paper: [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054). The paper is very interesting, but it's very terse.\n* Here is a good [video discussion](https://www.youtube.com/watch?v=tC01FRB0M7w) of the paper with visuals\n* Paper: [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840). Just published - this one goes into the details of ZeRO Offload feature.\n* DeepSpeed [configuration and tutorials](https://www.deepspeed.ai/getting-started/)\n* In addition to the paper, I highly recommend to read the following detailed blog posts with diagrams:\n  - [DeepSpeed: Extreme-scale model training for everyone]( https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n  - [ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)\n  - [Turing-NLG: A 17-billion-parameter language model by Microsoft](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)\n* DeepSpeed [examples on GitHub](https://github.com/microsoft/DeepSpeedExamples)\n\n# Gratitude", "## Usage tips\n\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\n  than the left.\n- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\n  similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\n  number of (repeating) layers.\n- Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters.\n- Layers are split in groups that share parameters (to save memory).\nNext sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A. The model must predict if they have been swapped or not.\n\n\n\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n\n\n## Resources", "So, if you reduce the precision, you reduce the memory each model parameter takes in storage, therefore reducing the model size! This also means that you reduce... the actual precision of the computations, which can reduce the model's performance. However, we found out that on bigger models, this performance degradation is actually very [limited](https://huggingface.co/blog/overview-quantization-transformers).\n\nTo go back to our above example, our 30B parameters model in `float16` requires a bit less than 66G of RAM, in `8bit` it only requires half that, so 33G of RAM, and it `4bit` we reach even half of this, so around 16G of RAM, making it considerably more accessible.\n\nThere are many ways to go from one precision to another, with many different \"translation\" schemes existing, each with its own benefits and drawbacks. Popular approaches include [bitsandbytes](https://huggingface.co/papers/2208.07339), [GPTQ](https://huggingface.co/papers/2210.17323), and [AWQ](https://huggingface.co/papers/2306.00978). Some users, such as [TheBloke](https://huggingface.co/TheBloke), are even converting popular models to make them accessible to the community. All are very recent and still developing, and we hope to see even more progress on this as time goes on.\n\n\n## What's next?\nThe year is not over yet! And these final ~~months~~ ~~days~~ hours have already come with the share of surprises: will a new architecture finally overperform the simple and efficient Transformer?", "- Splitting the embedding matrix into two smaller matrices.\n- Using repeating layers split among groups.\n\nThe abstract from the paper is the following:\n\n*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.*\n\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n\n## Usage tips", "However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\n\n### Decoder[[nlp-decoder]]", "## Efficient training strategies\n\nEven training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer) in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB` just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn\u2019t train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you\u2019ll run out sooner or later.\n\nAnother option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit. \n\n![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/lora-animated.gif)   \n*Low-Rank Adaptation of linear layers: extra parameters (in orange) are added next to the frozen layer (in blue), and the resulting encoded hidden states are added together with the hidden states of the frozen layer.*", "#### Using half-precision\n\nYou can speed up inference and reduce memory footprint by 50% simply by loading the model in half-precision.\n\n```python\nfrom transformers import BarkModel\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = BarkModel.from_pretrained(\"suno/bark-small\", torch_dtype=torch.float16).to(device)\n```\n\n#### Using CPU offload\n\nAs mentioned above, Bark is made up of 4 sub-models, which are called up sequentially during audio generation. In other words, while one sub-model is in use, the other sub-models are idle.\n\nIf you're using a CUDA device, a simple solution to benefit from an 80% reduction in memory footprint is to offload the submodels from GPU to CPU when they're idle. This operation is called *CPU offloading*. You can use it with one line of code as follows:\n\n```python\nmodel.enable_cpu_offload()\n```\n\nNote that \ud83e\udd17 Accelerate must be installed before using this feature. [Here's how to install it.](https://huggingface.co/docs/accelerate/basic_tutorials/install)\n\n#### Using Better Transformer\n\nBetter Transformer is an \ud83e\udd17 Optimum feature that performs kernel fusion under the hood. You can gain 20% to 30% in speed with zero performance degradation. It only requires one line of code to export the model to \ud83e\udd17 Better Transformer:\n\n```python\nmodel =  model.to_bettertransformer()\n```\n\nNote that \ud83e\udd17 Optimum must be installed before using this feature. [Here's how to install it.](https://huggingface.co/docs/optimum/installation)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided correctly identifies the two parameter-reduction techniques used in the ALBERT model, which are splitting the vocabulary embedding into smaller matrices and sharing parameters across layers. The explanation aligns well with the reference answer, indicating that the response is accurate and factual. The additional information about the empirical evidence and the benefits of these techniques in terms of model scaling and performance on downstream tasks is relevant and supports the correctness of the response."}, {"question": "What are the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library?\n", "true_answer": "1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter5/1.mdx", "generated_answer": "1. Load a dataset from the Hugging Face Hub using the `load_from_hf()` function from the `datasets` module.\n2. Preprocess the data using `Dataset.map()` to apply transformations and tokenization.\n3. Load and compute metrics using `Dataset.load()` and `Dataset.map()` respectively.\n\nHere's an example:\n\n```python\nfrom datasets import load_from_hf\nfrom transform", "retrieved_docs": ["Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={5}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nIn [Chapter 3](/course/chapter3) you got your first taste of the \ud83e\udd17 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n\n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics.\n\nBut this is just scratching the surface of what \ud83e\udd17 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\n* What do you do when your dataset is not on the Hub?\n* How can you slice and dice a dataset? (And what if you _really_ need to use Pandas?)\n* What do you do when your dataset is huge and will melt your laptop's RAM?\n* What the heck are \"memory mapping\" and Apache Arrow?\n* How can you create your own dataset and push it to the Hub?\n\nThe techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!", "</Tip>\n\nTo fine-tune a model in TensorFlow, follow these steps:\n1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n2. Instantiate a pre-trained model.\n3. Convert a \ud83e\udd17 Dataset to a `tf.data.Dataset`.\n4. Compile your model.\n5. Add callbacks and use the `fit()` method to run the training.\n6. Upload your model to \ud83e\udd17 Hub to share with the community.\n\nStart by defining the hyperparameters, optimizer and learning rate schedule:\n\n```py\n>>> from transformers import create_optimizer\n\n>>> batch_size = 16\n>>> num_epochs = 5\n>>> num_train_steps = len(food[\"train\"]) * num_epochs\n>>> learning_rate = 3e-5\n>>> weight_decay_rate = 0.01\n\n>>> optimizer, lr_schedule = create_optimizer(\n...     init_lr=learning_rate,\n...     num_train_steps=num_train_steps,\n...     weight_decay_rate=weight_decay_rate,\n...     num_warmup_steps=0,\n... )\n```\n\nThen, load ViT with [`TFAutoModelForImageClassification`] along with the label mappings:\n\n```py\n>>> from transformers import TFAutoModelForImageClassification\n\n>>> model = TFAutoModelForImageClassification.from_pretrained(\n...     checkpoint,\n...     id2label=id2label,\n...     label2id=label2id,\n... )\n```\n\nConvert your datasets to the `tf.data.Dataset` format using the [`~datasets.Dataset.to_tf_dataset`] and your `data_collator`:", "FrameworkSwitchCourse {fw} />\n\n# Fine-tuning, Check![[fine-tuning-check]]\n\n<CourseFloatingBanner\n    chapter={3}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThat was fun! In the first two chapters you learned about models and tokenizers, and now you know how to fine-tune them for your own data. To recap, in this chapter you:\n\n{#if fw === 'pt'}\n* Learned about datasets in the [Hub](https://huggingface.co/datasets)\n* Learned how to load and preprocess datasets, including using dynamic padding and collators\n* Implemented your own fine-tuning and evaluation of a model\n* Implemented a lower-level training loop\n* Used \ud83e\udd17 Accelerate to easily adapt your training loop so it works for multiple GPUs or TPUs\n\n{:else}\n* Learned about datasets in the [Hub](https://huggingface.co/datasets)\n* Learned how to load and preprocess datasets\n* Learned how to fine-tune and evaluate a model with Keras\n* Implemented a custom metric\n\n{/if}", "## How do I finetune this model?\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\n```python\nmodel = timm.create_model('res2net101_26w_4s', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\n\n## How do I train this model?\n\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\n\n## Citation\n\n```BibTeX\n@article{Gao_2021,\n   title={Res2Net: A New Multi-Scale Backbone Architecture},\n   volume={43},\n   ISSN={1939-3539},\n   url={http://dx.doi.org/10.1109/TPAMI.2019.2938758},\n   DOI={10.1109/tpami.2019.2938758},\n   number={2},\n   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},\n   year={2021},\n   month={Feb},\n   pages={652\u2013662}\n}\n```", "## How do I finetune this model?\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\n```python\nmodel = timm.create_model('densenet121', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\n\n## How do I train this model?\n\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\n\n## Citation\n\n```BibTeX\n@article{DBLP:journals/corr/HuangLW16a,\n  author    = {Gao Huang and\n               Zhuang Liu and\n               Kilian Q. Weinberger},\n  title     = {Densely Connected Convolutional Networks},\n  journal   = {CoRR},\n  volume    = {abs/1608.06993},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1608.06993},\n  archivePrefix = {arXiv},\n  eprint    = {1608.06993},\n  timestamp = {Mon, 10 Sep 2018 15:49:32 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/HuangLW16a.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n```\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/rwightman/pytorch-image-models}}\n}\n```", "## How do I finetune this model?\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\n```python\nmodel = timm.create_model('res2next50', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\n\n## How do I train this model?\n\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\n\n## Citation\n\n```BibTeX\n@article{Gao_2021,\n   title={Res2Net: A New Multi-Scale Backbone Architecture},\n   volume={43},\n   ISSN={1939-3539},\n   url={http://dx.doi.org/10.1109/TPAMI.2019.2938758},\n   DOI={10.1109/tpami.2019.2938758},\n   number={2},\n   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},\n   year={2021},\n   month={Feb},\n   pages={652\u2013662}\n}\n```", "## How do I finetune this model?\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\n```python\nmodel = timm.create_model('resnext101_32x8d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\n\n## How do I train this model?\n\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\n\n## Citation\n\n```BibTeX\n@article{DBLP:journals/corr/XieGDTH16,\n  author    = {Saining Xie and\n               Ross B. Girshick and\n               Piotr Doll{\\'{a}}r and\n               Zhuowen Tu and\n               Kaiming He},\n  title     = {Aggregated Residual Transformations for Deep Neural Networks},\n  journal   = {CoRR},\n  volume    = {abs/1611.05431},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1611.05431},\n  archivePrefix = {arXiv},\n  eprint    = {1611.05431},\n  timestamp = {Mon, 13 Aug 2018 16:45:58 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/XieGDTH16.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. The first step correctly identifies the need to load a dataset from the Hugging Face Hub, but the function name provided is incorrect; the correct function is `load_dataset()` not `load_from_hf()`. The second step is accurate in stating that data preprocessing should be done using `Dataset.map()`. However, the third step is inaccurate as it suggests using `Dataset.load()` to load metrics, which is not a function in the \ud83e\udd17 Datasets library. Metrics should be loaded using `load_metric()` function, and `Dataset.map()` is correctly mentioned for computing metrics. The response is mostly aligned with the reference answer but includes a critical error in the function name and an incorrect function for loading metrics."}, {"question": "What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n", "true_answer": "+800%", "source_doc": "huggingface/blog/blob/main/infinity-cpu-performance.md", "generated_answer": "According to the given context, Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers. This information is provided in the conclusion section of the context. The exact percentage of improvement may vary depending on the specific model and hardware configuration being used.", "retrieved_docs": ["<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Latency\" src=\"assets/46_infinity_cpu_performance/latency.png\"></medium-zoom>\n  <figcaption>Figure 4. Latency (Batch=1, Physical Cores=2)</figcaption>\n</figure>\n<br>\n\n---\n\n## Conclusion\n\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput.\n\nHugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n\nThe flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. \n\nIf you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n\n\n## Resources", "## Introduction \n\nTransfer learning has changed Machine Learning by reaching new levels of accuracy from Natural Language Processing (NLP) to Audio and Computer Vision tasks. At Hugging Face, we work hard to make these new complex models and large checkpoints as easily accessible and usable as possible. But while researchers and data scientists have converted to the new world of Transformers, few companies have been able to deploy these large, complex models in production at scale.\n\nThe main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware.\n\nWith [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n\n\n## What is Hugging Face Infinity", "## Solutions\n\n### **Coming soon: Infinity**\n\nTransformers latency down to 1ms? \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f\n\nWe have been working on a really sleek solution to achieve unmatched efficiency for state-of-the-art Transformer models, for companies to deploy in their own infrastructure.\n\n- Infinity comes as a single-container and can be deployed in any production environment.\n- It can achieve 1ms latency for BERT-like models on GPU and 4-10ms on CPU \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f\n- Infinity meets the highest security requirements and can be integrated into your system without the need for internet access. You have control over all incoming and outgoing traffic.\n\n\u26a0\ufe0f  Join us for a [live announcement and demo on Sep 28](https://app.livestorm.co/hugging-face/hugging-face-infinity-launch?type=detailed), where we will be showcasing Infinity for the first time in public!\n\n### **NEW: Hardware Acceleration**\n\nHugging Face is [partnering with leading AI hardware accelerators](http://hf.co/hardware) such as Intel, Qualcomm and GraphCore to make state-of-the-art production performance accessible and extend training capabilities on SOTA hardware. As the first step in this journey, we [introduced a new Open Source library](https://huggingface.co/blog/hardware-partners-program): \ud83e\udd17 Optimum - the ML optimization toolkit for production performance \ud83c\udfce. Learn more in this [blog post](https://huggingface.co/blog/graphcore). \n\n### **NEW: Inference on SageMaker**", "You can find more information about Hugging Face Infinity at [hf.co/infinity](https://huggingface.co/infinity), and if you are interested in testing it for yourself, you can sign up for a free trial at [hf.co/infinity-trial](https://huggingface.co/infinity-trial).\n\n---\n\n## Benchmark \n\nInference performance benchmarks often only measure the execution of the model. In this blog post, and when discussing the performance of Infinity, we always measure the end-to-end pipeline including pre-processing, prediction, post-processing. Please keep this in mind when comparing these results with other latency measurements. \n\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Pipeline\" src=\"assets/46_infinity_cpu_performance/pipeline.png\"></medium-zoom>\n  <figcaption>Figure 2. Infinity End-to-End Pipeline</figcaption>\n</figure>\n<br>\n\n### Environment\n\nAs a benchmark environment, we are going to use the [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i), which are compute-optimized instances powered by the 3rd generation of Intel Xeon Scalable processors. These new Intel-based instances are using the ice-lake Process Technology and support Intel AVX-512, Intel Turbo Boost, and Intel Deep Learning Boost.", "If you are interested in optimizing your models to run with maximum efficiency, check out the\u00a0[\ud83e\udd17 Optimum library](https://github.com/huggingface/optimum).\n\n---\n\nThanks for reading! If you have any questions, feel free to contact me, through\u00a0[Github](https://github.com/huggingface/transformers), or on the\u00a0[forum](https://discuss.huggingface.co/c/optimum/59). You can also connect with me on\u00a0[Twitter](https://twitter.com/_philschmid)\u00a0or\u00a0[LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).\n </html>", "## Conclusion\n\nAs you can see, training Hugging Face transformers on a cluster of Intel Xeon CPUs is a flexible, scalable, and cost-effective solution, especially if you're working with small or medium-sized models and datasets.\n\nHere are some additional resources to help you get started:\n\n* [Intel IPEX](https://github.com/intel/intel-extension-for-pytorch) on GitHub\n* Hugging Face documentation: \"[Efficient training on CPU](https://huggingface.co/docs/transformers/perf_train_cpu)\" and \"[Efficient training on many CPUs](https://huggingface.co/docs/transformers/perf_train_cpu_many)\".\n\nIf you have questions or feedback, we'd love to read them on the [Hugging Face forum](https://discuss.huggingface.co/).\n\nThanks for reading!", "## What is Hugging Face Infinity\n\nHugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.\n\nHugging Face Infinity consists of 2 main services:\n* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.\n* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.\n\nThe Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.\n\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Product overview\" src=\"assets/46_infinity_cpu_performance/overview.png\"></medium-zoom>\n  <figcaption>Figure 1. Infinity Overview</figcaption>\n</figure>\n<br>\n\nAn Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learning tasks as defined in the [Transformers Pipelines documentation](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines). As of the writing of this blog post, supported tasks include feature extraction/document embedding, ranking, sequence classification, and token classification."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the maximum improvement in throughput as 800%, which aligns with the reference answer. The additional context provided about the variability depending on the model and hardware does not detract from the accuracy of the core information given. Therefore, the response is completely correct, accurate, and factual."}, {"question": "What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n", "true_answer": "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl", "source_doc": "huggingface/blog/blob/main/spacy.md", "generated_answer": "The command to upload a spaCy pipeline to the Hugging Face Hub is `python -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]`. This command can be found in the context provided, specifically in the section titled \"Additional resources 1: ## Sharing your models\". The command is part of the `spacy-huggingface-hub` library", "retrieved_docs": ["You can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`.\n\n```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n\nThe command will output two things:\n\n* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en_core_web_sm\n* And how to install the pipeline directly from the Hub!\n\n\n### From a Python script\n\nYou can use the `push` function from Python. It returns a dictionary containing the `\"url\"` and \"`whl_url`\" of the published model and the wheel file, which you can later install with `pip install`.\n\n```py\nfrom spacy_huggingface_hub import push\n\nresult = push(\"./en_ner_fashion-0.0.0-py3-none-any.whl\")\nprint(result[\"url\"])\n```\n\n## Additional resources", "## Sharing your models\n\n### Using the spaCy CLI (recommended)\n\nThe `spacy-huggingface-hub` library extends `spaCy` native CLI so people can easily push their packaged models to the Hub.\n\nYou can install spacy-huggingface-hub from pip:\n\n```bash\npip install spacy-huggingface-hub\n```\n\nYou can then check if the command has been registered successfully\n\n```bash\npython -m spacy huggingface-hub --help\n```\n\nTo push with the CLI, you can use the `huggingface-hub push` command as seen below.\n\n```bash\npython -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n```", "```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n\nTry it out and share your models with the community!\n\n## Would you like to integrate your library to the Hub?\n\nThis integration is possible thanks to the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library which has all our widgets and the API for all our supported libraries. If you would like to integrate your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you!", "All spaCy canonical pipelines can now be found in the official spaCy [organization](https://huggingface.co/spacy), and any user can share their pipelines with a single command `python -m spacy huggingface-hub`. To read more about it, head to [https://huggingface.co/blog/spacy](https://huggingface.co/blog/spacy). You can try all canonical spaCy models directly in the Hub in the demo [Space](https://huggingface.co/spaces/spacy/pipeline-visualizer)!\n\n![spaCy NER example](assets/27_summer_at_huggingface/spacy_ner.jpeg)\n\nAnother exciting integration is Sentence Transformers. You can read more about it in the [blog announcement](https://huggingface.co/blog/sentence-transformers-in-the-hub): you can find over 200 [models](https://huggingface.co/models?library=sentence-transformers) in the Hub, easily share your models with the rest of the community and reuse models from the community.\n\nBut that's not all! You can now find over 100 Adapter Transformers in the Hub and try out Speechbrain models with widgets directly in the browser for different tasks such as audio classification. If you're interested in our collaborations to integrate new ML libraries to the Hub, you can read more about them [here](https://huggingface.co/docs/hub/libraries).\n\n![Filter of all libraries](assets/27_summer_at_huggingface/filters.png)\n\n\n## Solutions\n\n### **Coming soon: Infinity**\n\nTransformers latency down to 1ms? \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f", "## Upload the trained/fine-tuned model to the Hub\n\nAll the example scripts support automatic upload of your final model to the [Model Hub](https://huggingface.co/models) by adding a `--push_to_hub` argument. It will then create a repository with your username slash the name of the folder you are using as `output_dir`. For instance, `\"sgugger/test-mrpc\"` if your username is `sgugger` and you are working in the folder `~/tmp/test-mrpc`.\n\nTo specify a given repository name, use the `--hub_model_id` argument. You will need to specify the whole repository name (including your username), for instance `--hub_model_id sgugger/finetuned-bert-mrpc`. To upload to an organization you are a member of, just use the name of that organization instead of your username: `--hub_model_id huggingface/finetuned-bert-mrpc`.\n\nA few notes on this integration:\n\n- you will need to be logged in to the Hugging Face website locally for it to work, the easiest way to achieve this is to run `huggingface-cli login` and then type your username and password when prompted. You can also pass along your authentication token with the `--hub_token` argument.\n- the `output_dir` you pick will either need to be a new folder or a local clone of the distant repository you are using.", "--\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\nthumbnail: /blog/assets/23_spacy/thumbnail.png\n\nauthors:\n- user: osanseviero\n- user: ines\n---\n\n# Welcome spaCy to the Hugging Face Hub\n\n\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n\nHugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n\n### Finding models\n\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\n\n\n### Widgets", "he Push to Hub API. Let's have a look at the push_to_hub API. You will need to be logged in with your Hugging Face account, which you can do by executing this first cell or typing huggingface-cli login in a terminal. Just enter your username and password and click login, which will store an authentication token in the cache of the machine you're using. Now, let's launch the fine-tuning of a BERT model on the GLUE COLA dataset. We won't go over the fine-tuning code because you can find it in any Transformers tutorial, or by looking at the videos linked below. What interests us here, is how we can leverage the Model Hub during training. This is done with the push_to_hub=True passed in your TrainingArguments . This will automatically upload your model to the Hub each time it is saved (so every epoch in our case), which allows you to resume training from a different machine if the current one gets interrupted. The model will be uploaded in your namespace, with the name of the output directory as a repository name. You can pick another name by passing it to the hub_model_id argument, and you can also push inside an organization you are a member of by passing a full repository name. With that done, we can just launch training and wait a little bit. Note that the model is pushed asynchronously, meaning that the training continues while your model is uploaded to the Hub"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provides a command that is structurally correct and includes optional arguments that can be used with the command to upload a spaCy pipeline to the Hugging Face Hub. However, the response does not provide a specific example of a wheel file as the reference answer does. The reference answer gives a concrete example of a wheel file (`en_ner_fashion-0.0.0-py3-none-any.whl`) that would be used in the command, which is missing from the evaluated response. Despite this, the evaluated response does accurately describe the command and its components, which aligns with the reference answer. Therefore, the response is mostly correct, accurate, and factual, but not completely because it lacks the specificity of the example provided in the reference answer."}, {"question": "What is the time and memory complexity of the Nystr\u00f6mformer's approximation of self-attention?\n", "true_answer": "O(n)", "source_doc": "huggingface/blog/blob/main/nystromformer.md", "generated_answer": "The time complexity of the Nystr\u00f6mformer's approximation of self-attention is O(n), where n is the length of the input sequence. This is in contrast to the standard self-attention mechanism, which has a time complexity of O(n^2). The memory complexity of the Nystr\u00f6mformer's approximation is also improved, with a linear memory complexity of O(n) instead of the quadratic complexity of O(n^2) for standard", "retrieved_docs": ["--\ntitle: \"Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\"\nthumbnail: /blog/assets/86_nystromformer/thumbnail.png\nauthors:\n- user: asi\n  guest: true\n---\n\n# Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n## Introduction\n\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. \n\nThe [Nystr\u00f6mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nystr\u00f6mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nystr\u00f6m method and how it can be adapted to approximate self-attention.\n\n\n## Nystr\u00f6m method for matrix approximation", "The four papers above offer different ways to deal with the quadratic memory complexity of the self-attention mechanism, usually by reducing it to linear complexity. Linformer and Longformer both rely on the observation that the self-attention matrix does not contain \\\\(n \u00d7 n\\\\) worth of information (the attention matrix is low-rank and sparse). Performer gives a principled method to approximate the softmax-attention kernel (and any kernelizable attention mechanisms beyond softmax). Compressive Transformer offers an orthogonal approach to model long range dependencies based on recurrence.", "## How can we adapt the Nystr\u00f6m method to approximate self-attention?\n\nInstead of sampling from \\\\(S\\\\), the authors propose to sample landmarks (or Nystr\u00f6m points) from queries and keys. We denote the query landmarks and key landmarks as \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) respectively. \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) can be used to construct three matrices corresponding to those in the Nystr\u00f6m approximation of \\\\(S\\\\). We define the following matrices:\n\n$$\\tilde{F} = softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) \\hspace{40pt} \\tilde{A} = softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+ \\hspace{40pt} \\tilde{B} = softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}})$$\n\nThe sizes of \\\\(\\tilde{F}\\\\), \\\\(\\tilde{A}\\\\), and \\\\(\\tilde{B}) are \\\\(n \\times m, m \\times m,\\\\) and \\\\(m \\times n\\\\) respectively. \nWe replace the three matrices in the Nystr\u00f6m approximation of \\\\(S\\\\) with the new matrices we have defined to obtain an alternative Nystr\u00f6m approximation:\n\n$$\\begin{aligned}\\hat{S} &= \\tilde{F} \\tilde{A} \\tilde{B} \\\\ &= softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+  softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}}) \\end{aligned}$$\n\nThis is the Nystr\u00f6m approximation of the softmax matrix in the self-attention mechanism. We multiply this matrix with the values ( \\\\(V\\\\)) to obtain a linear approximation of self-attention. Note that we never calculated the product \\\\(QK^T\\\\), avoiding the \\\\(O(n^2)\\\\) complexity. \n\n\n## How do we select landmarks?", "It's important to note here that extending the input vectors for each chunked self-attention function allows *each* single output vector \\\\( \\mathbf{z}_{i} \\\\) of this self-attention function to learn better vector representations. E.g. each of the output vectors \\\\( \\mathbf{z}_{5}^{\\text{loc}}, \\mathbf{z}_{6}^{\\text{loc}}, \\mathbf{z}_{7}^{\\text{loc}}, \\mathbf{z}_{8}^{\\text{loc}} \\\\) can take into account all of the input vectors \\\\( \\mathbf{X}_{1:8} \\\\) to learn better representations.\n\nThe gain in memory consumption is quite obvious: The \\\\( \\mathcal{O}(n^2) \\\\) memory complexity is broken down for each segment individually so that the total asymptotic memory consumption is reduced to \\\\( \\mathcal{O}(n_{c} * l_{c}^2) = \\mathcal{O}(n * l_{c}) \\\\).", "Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n\nThe goal is to reduce the complexity of the self-attention with respect to the sequence length \\\\(n\\\\)) from quadratic to linear. This paper makes the observation that the attention matrices are low rank (i.e. they don\u2019t contain \\\\(n \u00d7 n\\\\) worth of information) and explores the possibility of using high-dimensional data compression techniques to build more memory efficient transformers.\n\nThe theoretical foundations of the proposed approach are based on the Johnson-Lindenstrauss lemma. Let\u2019s consider \\\\(m\\\\)) points in a high-dimensional space. We want to project them to a low-dimensional space while preserving the structure of the dataset (i.e. the mutual distances between points) with a margin of error \\\\(\\varepsilon\\\\). The Johnson-Lindenstrauss lemma states we can choose a small dimension \\\\(k \\sim 8 \\log(m) / \\varepsilon^2\\\\) and find a suitable projection into Rk in polynomial time by simply trying random orthogonal projections.\n\nLinformer projects the sequence length into a smaller dimension by learning a low-rank decomposition of the attention context matrix. The matrix multiplication of the self-attention can be then cleverly re-written such that no matrix of size \\\\(n \u00d7 n\\\\) needs to be ever computed and stored.\n\nStandard transformer:\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}(Q * K) * V$$\n\n                  (n * h)\t            (n * n)   (n * h)\n\nLinformer:", "The authors observe that if we could perform the matrix multiplication \\\\(K \u00d7 V\\\\) through the softmax ( \\\\(\\text{softmax}(Q \u00d7 K) \u00d7 V\\\\) ), we wouldn\u2019t have to compute the \\\\(Q x K\\\\) matrix of size \\\\(n x n\\\\) which is the memory bottleneck. They use random feature maps (aka random projections) to approximate the softmax by:\n\n$$\\text{softmax}(Q * K) \\sim Q\u2019 * K\u2019 = \\phi(Q) * \\phi(K)$$\n\n, where \\\\(phi\\\\) is a non-linear suitable function. And then:\n\n$$\\text{Attention}(Q, K, V) \\sim \\phi(Q) * (\\phi(K) * V)$$\n\nTaking inspiration from machine learning papers from the early 2000s, the authors introduce **FAVOR+** (**F**ast **A**ttention **V**ia **O**rthogonal **R**andom positive (**+**) **F**eatures) a procedure to find unbiased or nearly-unbiased estimations of the self-attention matrix, with uniform convergence and low estimation variance.\n\n#### Main findings\n\n* The FAVOR+ procedure can be used to approximate self-attention matrices with high accuracy, without any priors on the form of the attention matrix, making it applicable as a drop-in replacement of standard self-attention and leading to strong performances in multiple applications and modalities.\n* The very thorough mathematical investigation of how-to and not-to approximate softmax highlights the relevance of principled methods developed in the early 2000s even in the deep learning era.\n* FAVOR+ can also be applied to efficiently model other kernelizable attention mechanisms beyond softmax.\n\n#### Follow-up questions", "The abstract from the paper is the following:\n\n*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\ncommunity. To address this limitation, we propose Nystr\u00f6mformer -- a model that exhibits favorable scalability as a\nfunction of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention\nwith O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\nsequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr\u00f6mformer performs\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.*\n\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer).\n\n## Resources"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the time complexity of the Nystr\u00f6mformer's approximation of self-attention as O(n), which aligns with the reference answer. It also correctly states that the standard self-attention mechanism has a time complexity of O(n^2), providing a comparison that adds to the accuracy of the response. Furthermore, the response accurately mentions that the memory complexity of the Nystr\u00f6mformer's approximation is O(n), which is an improvement over the standard self-attention mechanism's memory complexity of O(n^2). The response is factual and provides a clear contrast between the Nystr\u00f6mformer's approximation and the standard self-attention mechanism in terms of both time and memory complexity."}, {"question": "What is the goal of the Named Entity Recognition task in token classification?\n", "true_answer": "The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/tasks.md", "generated_answer": "The goal of the Named Entity Recognition task in token classification is to find and label the named entities, such as persons, locations, or organizations, in a piece of text. This task involves attributing a label to each token in the text, with one class for each entity and another class for tokens that have no entity. The labels are used to evaluate the performance of the model in terms of accuracy, recall, precision, and F1-score. Examples of named entity recognition tasks", "retrieved_docs": ["Welcome to the Hugging Face tasks series! In this video we\u2019ll take a look at the token classification task.\nToken classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\nLet\u2019s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.\nAnother token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech.\nToken classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score. Then we calculate it for every class and take the overall average to evaluate our model.\nAn example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels.", "- **Named entity recognition (NER)**: Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \"no entity.\"\n- **Part-of-speech tagging (POS)**: Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.).\n- **Chunking**: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually `B-`) to any tokens that are at the beginning of a chunk, another label (usually `I-`) to tokens that are inside a chunk, and a third label (usually `O`) to tokens that don't belong to any chunk.\n\n<Youtube id=\"wVHdVlPScxA\"/>\n\nOf course, there are many other types of token classification problem; those are just a few representative examples. In this section, we will fine-tune a model (BERT) on a NER task, which will then be able to compute predictions like this one:", "### Token classification\n\nIn any NLP task, text is preprocessed by separating the sequence of text into individual words or subwords. These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. \n\nTwo common types of token classification are:\n\n* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names.\n* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb).\n\n```py\n>>> from transformers import pipeline", "In particular, we can see the dataset contains labels for the three tasks we mentioned earlier: NER, POS, and chunking. A big difference from other datasets is that the input texts are not presented as sentences or documents, but lists of words (the last column is called `tokens`, but it contains words in the sense that these are pre-tokenized inputs that still need to go through the tokenizer for subword tokenization).\n\nLet's have a look at the first element of the training set:\n\n```py\nraw_datasets[\"train\"][0][\"tokens\"]\n```\n\n```python out\n['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n```\n\nSince we want to perform named entity recognition, we will look at the NER tags:\n\n```py\nraw_datasets[\"train\"][0][\"ner_tags\"]\n```\n\n```python out\n[3, 0, 7, 0, 0, 0, 7, 0, 0]\n```\n\nThose are the labels as integers ready for training, but they're not necessarily useful when we want to inspect the data. Like for text classification, we can access the correspondence between those integers and the label names by looking at the `features` attribute of our dataset:\n\n```py\nner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\nner_feature\n```\n\n```python out\nSequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)\n```", "As an example, if you are working with a collection of digitized Norwegian documents with minimal metadata. One way to better understand what's in the collection is to use a Named Entity Recognition (NER) model. This model extracts entities from a text, for example, identifying the locations mentioned in a text. Knowing which entities are contained in a text can be a valuable way of better understanding what a document is about.\n\nWe can find NER models on the Hub by filtering models by task. In this case, we choose `token-classification`, which is the task which includes named entity recognition models. [This filter](https://huggingface.co/datasets?task_categories=task_categories:token-classification) returns models labelled as doing `token-classification`. Since we are working with Norwegian documents, we may also want to [filter by language](https://huggingface.co/models?pipeline_tag=token-classification&language=no&sort=downloads); this gets us to a smaller set of models we want to explore. Many of these models will also contain a [model widget](https://huggingface.co/saattrupdan/nbailab-base-ner-scandi), allowing us to test the model. \n\n\n![](https://i.imgur.com/9V9xni5.png)", "The abstract from the paper is the following:\n\n*Entity representations are useful in natural language tasks involving entities. In this paper, we propose new\npretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed\nmodel treats words and entities in a given text as independent tokens, and outputs contextualized representations of\nthem. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves\npredicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also\npropose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the\ntransformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model\nachieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains\nstate-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification),\nCoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question\nanswering).*\n\nThis model was contributed by [ikuyamada](https://huggingface.co/ikuyamada) and [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/studio-ousia/luke).\n\n## Usage tips", "As a result, the output contains a classification label from the list we have provided in the instructions, and it is a correct one!\n\n<Tip>\n\nYou may notice that in addition to the prompt, we pass a `max_new_tokens` parameter. It controls the number of tokens the \nmodel shall generate, and it is one of the many text generation parameters that you can learn about \nin [Text generation strategies](../generation_strategies) guide.\n\n</Tip>\n\n#### Named Entity Recognition\n\nNamed Entity Recognition (NER) is a task of finding named entities in a piece of text, such as a person, location, or organization.\nLet's modify the instructions in the prompt to make the LLM perform this task. Here, let's also set `return_full_text = False` \nso that output doesn't contain the prompt:\n\n```python\n>>> torch.manual_seed(1) # doctest: +IGNORE_RESULT\n>>> prompt = \"\"\"Return a list of named entities in the text.\n... Text: The Golden State Warriors are an American professional basketball team based in San Francisco.\n... Named entities:\n... \"\"\"\n\n>>> sequences = pipe(\n...     prompt,\n...     max_new_tokens=15,\n...     return_full_text = False,    \n... )\n\n>>> for seq in sequences:\n...     print(f\"{seq['generated_text']}\")\n- Golden State Warriors\n- San Francisco\n```\n\nAs you can see, the model correctly identified two named entities from the given text.\n\n#### Translation"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It accurately identifies the goal of Named Entity Recognition (NER) as finding and labeling named entities like persons, locations, or organizations within a text. The response goes beyond the reference answer by mentioning the process of attributing a label to each token and discussing the evaluation metrics such as accuracy, recall, precision, and F1-score. However, the additional details about evaluation metrics, while accurate, are not required to fulfill the goal of the NER task as per the instruction. The core of the response aligns with the reference answer, which warrants a high score."}, {"question": "What is the resolution of images used by the CLIPSeg model?\n", "true_answer": "352 x 352 pixels", "source_doc": "huggingface/blog/blob/main/clipseg-zero-shot.md", "generated_answer": "The resolution of images used by the CLIPSeg model is currently 352 x 352 pixels, as mentioned in the context provided. This results in relatively low-resolution output segmentation masks, and more precise segmentations may be needed for certain applications. However, the authors suggest fine-tuning a state-of-the-art segmentation model for more accurate results or using CLIPSeg to generate rough labels that can be refined in a labeling tool like", "retrieved_docs": ["Currently, CLIPSeg still has its limitations. For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg). Before we describe how to do that, let\u2019s first take a look at how CLIPSeg works.\n\n## CLIP: the magic model behind CLIPSeg", "segments_client.add_label(sample.uuid, 'ground-truth', attributes)\n```\n\nIf you take a look at the [uploaded prediction on\nSegments.ai](https://segments.ai/admin-tobias/clipseg/samples/71a80d39-8cf3-4768-a097-e81e0b677517/ground-truth),\nyou can see that it\\'s not perfect. However, you can manually correct\nthe biggest mistakes, and then you can use the corrected dataset to\ntrain a better model than CLIPSeg.\n\n<figure class=\"image table text-center m-0 w-9/12\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Thumbnails of the final segmentation labels on Segments.ai.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/segments-thumbs.png\"></medium-zoom>\n</figure>\n\n## Conclusion\n\nCLIPSeg is a zero-shot segmentation model that works with both text and image prompts. The model adds a decoder to CLIP and can segment almost anything. However, the output segmentation masks are still very low-res for now, so you\u2019ll probably still want to fine-tune a different segmentation model if accuracy is important.", "## CLIPSeg: image segmentation with CLIP\n\n[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo L\u00fcddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn\u2019t only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP. \n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clipseg-overview.png\"></medium-zoom>\n  <figcaption><a href=\"https://arxiv.org/abs/2112.10003\">Source</a></figcaption>\n</figure>", "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png\"\nalt=\"drawing\" width=\"600\"/> \n\n<small> CLIPSeg overview. Taken from the <a href=\"https://arxiv.org/abs/2112.10003\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/timojl/clipseg).\n\n## Usage tips\n\n- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latter is identical to [`CLIPModel`].\n- [`CLIPSegForImageSegmentation`] can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text\n(provided to the model as `input_ids`) or an image (provided to the model as `conditional_pixel_values`). One can also provide custom\nconditional embeddings (provided to the model as `conditional_embeddings`).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with CLIPSeg. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"image-segmentation\"/>", "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clip-tv-example.png\"></medium-zoom>\n  <figcaption>Example of image classification using CLIP (<a href=\"https://openai.com/blog/clip/\">source</a>).</figcaption>\n</figure>\n\nWhat\u2019s more, CLIP is not only useful for classification, but it can also be used for [image search](https://huggingface.co/spaces/DrishtiSharma/Text-to-Image-search-using-CLIP) (can you see how this is similar to classification?), [text-to-image models](https://huggingface.co/spaces/kamiyamai/stable-diffusion-webui) ([DALL-E 2](https://openai.com/dall-e-2/) is powered by CLIP), [object detection](https://segments.ai/zeroshot?utm_source=hf&utm_medium=blog&utm_campaign=clipseg) ([OWL-ViT](https://arxiv.org/abs/2205.06230)), and most importantly for us: image segmentation. Now you see why CLIP was truly a breakthrough in machine learning.\n\nThe reason why CLIP works so well is that the model was trained on a huge dataset of images with text captions. The dataset contained a whopping 400 million image-text pairs taken from the internet. These images contain a wide variety of objects and concepts, and CLIP is great at creating a representation for each of them.\n\n## CLIPSeg: image segmentation with CLIP", "--\ntitle: Zero-shot image segmentation with CLIPSeg\nthumbnail: /blog/assets/123_clipseg-zero-shot/thumb.png\nauthors:\n- user: segments-tobias\n  guest: true\n- user: nielsr\n---\n\n# Zero-shot image segmentation with CLIPSeg\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/123_clipseg-zero-shot.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n**This guide shows how you can use [CLIPSeg](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg), a zero-shot image segmentation model, using [`\ud83e\udd17 transformers`](https://huggingface.co/transformers). CLIPSeg creates rough segmentation masks that can be used for robot perception, image inpainting, and many other tasks. If you need more precise segmentation masks, we\u2019ll show how you can refine the results of CLIPSeg on [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg).**", "This is fairly close to what we are interested in here. Particularly we might be interested in how well the model deals with the kinds of images in our dataset (illustrations from mostly 19th century books). The images in our dataset are (probably) fairly different from the training data. The fact that some of the images also contain text might help CLIP since it displays some [OCR ability](https://openai.com/blog/clip/).\n\nHowever, looking at the out-of-scope use cases in the model card:\n\n> ### Out-of-Scope Use Cases\n>\n> Any deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in  a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class  taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of  CLIP's performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case > currently potentially harmful.  > [source](https://huggingface.co/openai/clip-vit-base-patch32)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the resolution of images used by the CLIPSeg model as 352 x 352 pixels, which is in agreement with the reference answer. The additional information provided about the implications of this resolution on the output segmentation masks and suggestions for improving results does not detract from the factual accuracy of the core information requested."}, {"question": "What can you use Gradio for?\n", "true_answer": "Create a demo for your machine learning model, share your machine learning model with others, and debug your model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/9.mdx", "generated_answer": "Gradio is an open-source Python package that allows you to quickly build demos and web applications for machine learning models, APIs, or any Python function, without requiring JavaScript, CSS, or web hosting experience. It provides a high-level class called `Interface` for creating demos and a low-level approach called `Blocks` for designing web apps with more flexible layouts and data flows. Gradio also offers a feature called `ChatInterface` for creating chatbots. Additionally", "retrieved_docs": ["### What is Gradio?\n\n[Gradio](https://github.com/gradio-app/gradio) is the defacto standard framework for building Machine Learning Web Applications and sharing them with the world - all with just python! \ud83d\udc0d\n\n## gradio_tools - An end-to-end example\n\nTo get started with `gradio_tools`, all you need to do is import and initialize your tools and pass them to the langchain agent!\n\nIn the following example, we import the `StableDiffusionPromptGeneratorTool` to create a good prompt for stable diffusion, the\n`StableDiffusionTool` to create an image with our improved prompt, the `ImageCaptioningTool` to caption the generated image, and\nthe `TextToVideoTool` to create a video from a prompt.\n\nWe then tell our agent to create an image of a dog riding a skateboard, but to please improve our prompt ahead of time. We also ask\nit to caption the generated image and create a video for it. The agent can decide which tool to use without us explicitly telling it.\n\n```python\nimport os\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise ValueError(\"OPENAI_API_KEY must be set\")\n\nfrom langchain.agents import initialize_agent\nfrom langchain.llms import OpenAI\nfrom gradio_tools import (StableDiffusionTool, ImageCaptioningTool, StableDiffusionPromptGeneratorTool,\n                          TextToVideoTool)\n\nfrom langchain.memory import ConversationBufferMemory", "To learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).\n\n\n## An Overview of Gradio\n\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do?\n\n### Chatbots with `gr.ChatInterface`\n\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight to [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).\n\n### Custom Demos with `gr.Blocks`\n\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction \u2014 still all in Python.", "To learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).\n\n\n### An Overview of Gradio\n\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do?\n\n#### Chatbots with `gr.ChatInterface`\n\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).\n\n#### Custom Demos with `gr.Blocks`\n\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction \u2014 still all in Python.", "gradio\n\n## 4.11.0\n\n### Features", "gradio\n\n## 4.11.0\n\n### Features", "Quickstart\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/lcm-screenshot-3.gif\" style=\"padding-bottom: 10px\">\n\nIt just takes a few lines of Python to create a beautiful demo like the one above, so let's get started \ud83d\udcab\n\n## Installation\n\n**Prerequisite**: Gradio requires [Python 3.8 or higher](https://www.python.org/downloads/)\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install gradio\n```\n\n\nTip: it is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems <a href=\"https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment\">are provided here</a>. \n\n## Building Your First Demo\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:\n\n\n$code_hello_world_4", "Note: if you use IFrames, you'll probably want to add a fixed `height` attribute and set `style=\"border:0;\"` to remove the boreder. In addition, if your app requires permissions such as access to the webcam or the microphone, you'll need to provide that as well using the `allow` attribute.\n\n## API Page\n\nYou can use almost any Gradio app as an API! In the footer of a Gradio app [like this one](https://huggingface.co/spaces/gradio/hello_world), you'll see a \"Use via API\" link.\n\n![Use via API](https://github.com/gradio-app/gradio/blob/main/guides/assets/use_via_api.png?raw=true)\n\nThis is a page that lists the endpoints that can be used to query the Gradio app, via our supported clients: either [the Python client](https://gradio.app/guides/getting-started-with-the-python-client/), or [the JavaScript client](https://gradio.app/guides/getting-started-with-the-js-client/). For each endpoint, Gradio automatically generates the parameters and their types, as well as example inputs.\n\nThe endpoints are automatically created when you launch a Gradio `Interface`. If you are using Gradio `Blocks`, you can also set up a Gradio API page, though we recommend that you explicitly name each event listener, such as\n\n```python\nbtn.click(add, [num1, num2], output, api_name=\"addition\")\n```\n\nThis will add and document the endpoint `/api/addition/` to the automatically generated API page. Otherwise, your API endpoints will appear as \"unnamed\" endpoints.\n\n\n## Authentication"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately describes Gradio as an open-source Python package used to build demos and web applications for machine learning models and other Python functions. It mentions the `Interface` class for creating demos, the `Blocks` approach for more flexible web app design, and the `ChatInterface` for chatbots. This information aligns well with the reference answer's points about creating demos, sharing machine learning models, and debugging models, as these are all functionalities that Gradio provides. The response is detailed and factual, providing a clear understanding of Gradio's capabilities, which are relevant to the instruction."}, {"question": "What TensorFlow API function is used to load a saved tensor file?\n", "true_answer": "safetensors.tensorflow.load_file", "source_doc": "huggingface/safetensors/blob/main/docs/source/api/tensorflow.mdx", "generated_answer": "The TensorFlow API function used to load a saved tensor file is `tf.saved_model.load()`. This function is used to load a TensorFlow SavedModel, which includes the trained model and its associated metadata, from a file or directory. The loaded model can then be used to make predictions or perform inference. The function takes the path to the SavedModel as an argument and returns a dictionary containing the loaded model and its signatures. The specific signature to use for in", "retrieved_docs": ["Tensorflow API\n\n[[autodoc]] safetensors.tensorflow.load_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save_file\n[[autodoc]] safetensors.tensorflow.save", "return serving_fn\n```\n\nYou can first derive the [concrete function](https://www.tensorflow.org/guide/function)\nfrom the model's forward pass method (`call()`) so the model is nicely compiled\ninto a graph. After that, you can apply the following steps in order:\n\n1. Pass the inputs through the preprocessing operations.\n\n2. Pass the preprocessing inputs through the derived concrete function.\n\n3. Post-process the outputs and return them in a nicely formatted\n   dictionary.\n\nNow it's time to export the model!\n\n```py\nMODEL_DIR = tempfile.gettempdir()\nVERSION = 1\n\ntf.saved_model.save(\n    model,\n    os.path.join(MODEL_DIR, str(VERSION)),\n    signatures={\"serving_default\": model_exporter(model)},\n)\nos.environ[\"MODEL_DIR\"] = MODEL_DIR\n```\n\nAfter exporting, let's inspect the model signatures again:\n\n```bash\nsaved_model_cli show --dir {MODEL_DIR}/1 --tag_set serve --signature_def serving_default\n```\n\n```bash\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs['string_input'] tensor_info:\n      dtype: DT_STRING\n      shape: (-1)\n      name: serving_default_string_input:0\nThe given SavedModel SignatureDef contains the following output(s):\n  outputs['confidence'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1)\n      name: StatefulPartitionedCall:0\n  outputs['label'] tensor_info:\n      dtype: DT_STRING\n      shape: (-1)\n      name: StatefulPartitionedCall:1\nMethod name is: tensorflow/serving/predict\n```", "```py\n# Get image of a cute cat.\nimage_path = tf.keras.utils.get_file(\n    \"image.jpg\", \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n)\n\n# Read the image from disk as raw bytes and then encode it. \nbytes_inputs = tf.io.read_file(image_path)\nb64str = base64.urlsafe_b64encode(bytes_inputs.numpy()).decode(\"utf-8\")\n\n\n# Create the request payload.\ndata = json.dumps({\"signature_name\": \"serving_default\", \"instances\": [b64str]})\n```\n\nTF Serving's request payload format specification for the REST endpoint\nis available [here](https://www.tensorflow.org/tfx/serving/api_rest#request_format_2).\nWithin the `instances` you can pass multiple encoded images. This kind\nof endpoints are meant to be consumed for online prediction scenarios.\nFor inputs having more than a single data point, you would to want to\n[enable batching](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md)\nto get performance optimization benefits.\n\nNow you can call the API:\n\n```py\nheaders = {\"content-type\": \"application/json\"}\njson_response = requests.post(\n    \"http://localhost:8501/v1/models/vit:predict\", data=data, headers=headers\n)\nprint(json.loads(json_response.text))\n# {'predictions': [{'label': 'Egyptian cat', 'confidence': 0.896659195}]}\n\n```", "```python\nfrom transformers import TFBertForSequenceClassification\nimport tensorflow as tf\n\n# Creation of a subclass in order to define a new serving signature\nclass MyOwnModel(TFBertForSequenceClassification):\n    # Decorate the serving method with the new input_signature\n    # an input_signature represents the name, the data type and the shape of an expected input\n    @tf.function(input_signature=[{\n        \"inputs_embeds\": tf.TensorSpec((None, None, 768), tf.float32, name=\"inputs_embeds\"),\n        \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n        \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n    }])\n    def serving(self, inputs):\n        # call the model to process the inputs\n        output = self.call(inputs)\n\n        # return the formated output\n        return self.serving_output(output)\n\n# Instantiate the model with the new serving method\nmodel = MyOwnModel.from_pretrained(\"bert-base-cased\")\n# save it with saved_model=True in order to have a SavedModel version along with the h5 weights.\nmodel.save_pretrained(\"my_model\", saved_model=True)\n```", "- Safe: Can I use a file randomly downloaded and expect not to run arbitrary code ?\n- Zero-copy: Does reading the file require more memory than the original file ?\n- Lazy loading: Can I inspect the file without loading everything ? And loading only\nsome tensors in it without scanning the whole file (distributed setting) ?\n- Layout control: Lazy loading, is not necessarily enough since if the information about tensors is spread out in your file, then even if the information is lazily accessible you might have to access most of your file to read the available tensors (incurring many DISK -> RAM copies). Controlling the layout to keep fast access to single tensors is important.\n- No file size limit: Is there a limit to the file size ?\n- Flexibility: Can I save custom code in the format and be able to use it later with zero extra code ? (~ means we can store more than pure tensors, but no custom code)\n- Bfloat16/Fp8: Does the format support native bfloat16/fp8 (meaning no weird workarounds are\nnecessary)? This is becoming increasingly important in the ML world.\n\n\n### Main oppositions", "- Safe: Can I use a file randomly downloaded and expect not to run arbitrary code ?\n- Zero-copy: Does reading the file require more memory than the original file ?\n- Lazy loading: Can I inspect the file without loading everything ? And loading only\nsome tensors in it without scanning the whole file (distributed setting) ?\n- Layout control: Lazy loading, is not necessarily enough since if the information about tensors is spread out in your file, then even if the information is lazily accessible you might have to access most of your file to read the available tensors (incurring many DISK -> RAM copies). Controlling the layout to keep fast access to single tensors is important.\n- No file size limit: Is there a limit to the file size ?\n- Flexibility: Can I save custom code in the format and be able to use it later with zero extra code ? (~ means we can store more than pure tensors, but no custom code)\n- Bfloat16/Fp8: Does the format support native bfloat16/fp8 (meaning no weird workarounds are\nnecessary)? This is becoming increasingly important in the ML world.\n\n\n### Main oppositions", "You can determine the `serving_input` key programmatically like so:\n\n```py\nloaded = tf.saved_model.load(f\"{MODEL_DIR}/{VERSION}\")\nserving_input = list(\n    loaded.signatures[\"serving_default\"].structured_input_signature[1].keys()\n)[0]\nprint(\"Serving function input:\", serving_input)\n# Serving function input: string_input\n```\n\nNow, you can get some predictions:\n\n```py\ngrpc_predictions = stub.Predict(request, 10.0)  # 10 secs timeout\nprint(grpc_predictions)\n```\n\n```bash\noutputs {\n  key: \"confidence\"\n  value {\n    dtype: DT_FLOAT\n    tensor_shape {\n      dim {\n        size: 1\n      }\n    }\n    float_val: 0.8966591954231262\n  }\n}\noutputs {\n  key: \"label\"\n  value {\n    dtype: DT_STRING\n    tensor_shape {\n      dim {\n        size: 1\n      }\n    }\n    string_val: \"Egyptian cat\"\n  }\n}\nmodel_spec {\n  name: \"resnet\"\n  version {\n    value: 1\n  }\n  signature_name: \"serving_default\"\n}\n```\n\nYou can also fetch the key-values of our interest from the above results like so:\n\n```py\ngrpc_predictions.outputs[\"label\"].string_val, grpc_predictions.outputs[\n    \"confidence\"\n].float_val\n# ([b'Egyptian cat'], [0.8966591954231262])\n```\n\n# Wrapping Up\n\nIn this post, we learned how to deploy a TensorFlow vision model from\nTransformers with TF Serving. While local deployments are great for\nweekend projects, we would want to be able to scale these deployments to\nserve many users. In the next series of posts, you'll see how to scale up\nthese deployments with Kubernetes and Vertex AI."], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response provided mentions `tf.saved_model.load()` as the TensorFlow API function used to load a saved tensor file. However, the reference answer states that the correct function is `safetensors.tensorflow.load_file`. Since the response does not match the reference answer, it cannot be considered completely correct, accurate, and factual. However, the response does correctly identify a TensorFlow function related to loading models, which suggests some level of accuracy, but it is not the specific function for loading a saved tensor file as per the reference answer. Therefore, the response is somewhat correct but not entirely accurate."}, {"question": "Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n", "true_answer": "In the \"Logs\" tab of your Endpoint through the UI.", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx", "generated_answer": "The logs of your Endpoints in Hugging Face Endpoints can be accessed through the \"Logs\" tab in the UI of your Endpoint. This feature is provided by Hugging Face Endpoints and allows you to view both the build logs of your Image artifacts and the Container Logs during inference. If your Endpoint creation is in the \"Failed\" state, you can also check the Build Logs to see the reason for failure, such as wrong version of a dependency. (", "retrieved_docs": ["Access and read Logs\n\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the \u201cLogs\u201d tab of your Endpoint. \n\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\n\nThe Container Logs are only available when your Endpoint is in the \u201cRunning\u201d state. \n\n_Note: If your Endpoint creation is in the \u201cFailed\u201d state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\n\n**Build Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\n\n**Container Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png\" alt=\"container logs\" />", "Access and view Metrics\n\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the \u201cAnalytics\u201d tab of your Endpoints. \n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png\" alt=\"metric dashboard\" />\n\n## Access Metrics via API\n\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.", "## What is Hugging Face Inference Endpoints\n\n[Hugging Face Inference Endpoints](https://ui.endpoints.huggingface.co/) offers an easy and secure way to deploy Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, and offering advanced security. \n\nHere are some of the most important features for LLM deployment:", "These lightly wrap around the API Endpoints. Documentation for valid parameters and descriptions can be found [here](https://huggingface.co/docs/hub/endpoints).\n  \n\n### Advanced programmatic repository management \n\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed.\n\nInstantiate a `Repository` object by calling it with a path to a local Git\nclone/repository:\n\n```python\n>>> from huggingface_hub import Repository\n>>> repo = Repository(\"<path>/<to>/<folder>\")\n```\n\nThe `Repository` takes a `clone_from` string as parameter. This can stay as\n`None` for offline management, but can also be set to any URL pointing to a Git\nrepo to clone that repository in the specified directory:\n\n```python\n>>> repo = Repository(\"huggingface-hub\", clone_from=\"https://github.com/huggingface/huggingface_hub\")\n```\n\nThe `clone_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n\n```python\n>>> repo = Repository(\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\n```\n\nIf the repository you're cloning is one of yours or one of your organisation's,\nthen having the ability to commit and push to that repository is important. In\norder to do that, you should make sure to be logged-in using `huggingface-cli\nlogin`, and to have the `token` parameter set to `True` (the default)\nwhen  instantiating the `Repository` object:", "_Note: In your [Inference Endpoints dashboard](https://ui.endpoints.huggingface.co/), the Task for this Endpoint should now be set to Custom_", "Hub API Endpoints\n\nWe have open endpoints that you can use to retrieve information from the Hub as well as perform certain actions such as creating model, dataset or Space repos. We offer a wrapper Python library, [`huggingface_hub`](https://github.com/huggingface/huggingface_hub), that allows easy access to these endpoints. We also provide [webhooks](./webhooks) to receive real-time incremental info about repos. Enjoy!\n\nThe base URL for those endpoints below is `https://huggingface.co`. For example, to construct the `/api/models` call below, one can call the URL [https://huggingface.co/api/models](https://huggingface.co/api/models)\n\n## The Hub API Playground\n\nWant to try out our API?\nTry it out now on our [Playground](https://huggingface.co/spaces/enzostvs/hub-api-playground)!\n\n<div class=\"flex justify-center\">\n<a href=\"https://huggingface.co/spaces/enzostvs/hub-api-playground\" target=\"_blank\">\n<img class=\"w-full object-contain\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/hub-api-playground.png\"/>\n</a>\n</div>\n\n## Repo listing API\n\nThe following endpoints help get information about models, datasets, Spaces, and metrics stored on the Hub.", "You can easily log and monitor your runs code. The following are currently supported:\n\n* [TensorBoard](https://www.tensorflow.org/tensorboard)\n* [Weights & Biases](https://docs.wandb.ai/integrations/huggingface)\n* [Comet ML](https://www.comet.ml/docs/python-sdk/huggingface/)\n* [Neptune](https://docs.neptune.ai/integrations-and-supported-tools/model-training/hugging-face)\n* [ClearML](https://clear.ml/docs/latest/docs/getting_started/ds/ds_first_steps)\n* [DVCLive](https://dvc.org/doc/dvclive/ml-frameworks/huggingface)\n\n### Weights & Biases\n\nTo use Weights & Biases, install the wandb package with:\n\n```bash\npip install wandb\n```\n\nThen log in the command line:\n\n```bash\nwandb login\n```\n\nIf you are in Jupyter or Colab, you should login with:\n\n```python\nimport wandb\nwandb.login()\n```\n\nTo enable logging to W&B, include `\"wandb\"` in the `report_to` of your `TrainingArguments` or script. Or just pass along `--report_to_all` if you have `wandb` installed.\n\nWhenever you use `Trainer` or `TFTrainer` classes, your losses, evaluation metrics, model topology and gradients (for `Trainer` only) will automatically be logged.\n\nAdvanced configuration is possible by setting environment variables:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that the logs can be accessed through the \"Logs\" tab in the UI of your Endpoint. Additionally, the response goes into further detail by explaining the types of logs available and the utility of the logs in case of a failure, which supports the correctness and factual nature of the answer without deviating from the expected information."}, {"question": "What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n", "true_answer": "Image Classification", "source_doc": "huggingface/blog/blob/main/autotrain-image-classification.md", "generated_answer": "The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification. (Source: Extract 0)", "retrieved_docs": ["## Enabling the community: One task at a time \ud83d\udc41\n\nThe Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:\n\n- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- Depth estimation\n- Image-to-image synthesis\n- Unconditional image generation\n- Zero-shot image classification\n\nEach of these tasks comes with at least 10 model checkpoints on the Hub for you to explore. Furthermore, we support [tasks](https://huggingface.co/tasks) that lie at the intersection of vision and language such as:\n\n- Image-to-text (image captioning, OCR)\n- Text-to-image\n- Document question-answering\n- Visual question-answering", "[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with **zero** configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer \ud83e\udd2f We\u2019ve been expanding the number of tasks that we support, and we\u2019re proud to announce that **you can now use AutoTrain for Computer Vision**! Image Classification is the latest task we\u2019ve added, with more on the way. But what does this mean for you?\n\n[Image Classification](https://huggingface.co/tasks/image-classification) models learn to *categorize* images, meaning that you can train one of these models to label any image. Do you want a model that can recognize signatures? Distinguish bird species? Identify plant diseases? As long as you can find an appropriate dataset, an image classification model has you covered.\n\n## How can you train your own image classifier?", "--\ntitle: The State of Computer Vision at Hugging Face \ud83e\udd17\nthumbnail: /blog/assets/cv_state/thumbnail.png\nauthors:\n- user: sayakpaul\n---\n\n# The State of Computer Vision at Hugging Face \ud83e\udd17\n\n\nAt Hugging Face, we pride ourselves on democratizing the field of artificial intelligence together with the community. As a part of that mission, we began focusing our efforts on computer vision over the last year. What started as a [PR for having Vision Transformers (ViT) in \ud83e\udd17 Transformers](https://github.com/huggingface/transformers/pull/10950) has now grown into something much bigger \u2013 8 core vision tasks, over 3000 models, and over 100 datasets on the Hugging Face Hub.\n\nA lot of exciting things have happened since ViTs joined the Hub. In this blog post, we\u2019ll summarize what went down and what\u2019s coming to support the continuous progress of Computer Vision from the \ud83e\udd17 ecosystem.\n\nHere is a list of things we\u2019ll cover:\n\n- [Supported vision tasks and Pipelines](#support-for-pipelines)\n- [Training your own vision models](#training-your-own-models)\n- [Integration with `timm`](#\ud83e\udd17-\ud83e\udd1d-timm)\n- [Diffusers](#\ud83e\udde8-diffusers)\n- [Support for third-party libraries](#support-for-third-party-libraries)\n- [Deployment](#deployment)\n- and much more!\n\n## Enabling the community: One task at a time \ud83d\udc41", "In this blog post, we will discuss the past, present, and future of text-to-video models. We will start by reviewing the differences between the text-to-video and text-to-image tasks, and discuss the unique challenges of unconditional and text-conditioned video generation. Additionally, we will cover the most recent developments in text-to-video models, exploring how these methods work and what they are capable of. Finally, we will talk about what we are working on at Hugging Face to facilitate the integration and use of these models and share some cool demos and resources both on and outside of the Hugging Face Hub.\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/140_text-to-video/make-a-video.png\" alt=\"samples\"><br>\n    <em>Examples of videos generated from various text description inputs, image taken from <a href=https://arxiv.org/abs/2209.14792>Make-a-Video</a>.</em>\n</p>\n\n## Text-to-Video vs. Text-to-Image\nWith so many recent developments, it can be difficult to keep up with the current state of text-to-image generative models. Let's do a quick recap first.", "## Further References\n\n- This Hugging Face implementation uses [this scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)", "## Next Steps\n\nA few weeks ago, we launched the Hugging Face [Evaluate library](https://github.com/huggingface/evaluate), aimed at lowering barriers to the best practices of machine learning evaluation. We have also been hosting benchmarks, like [RAFT](https://huggingface.co/spaces/ought/raft-leaderboard) and [GEM](https://huggingface.co/spaces/GEM/submission-form). Evaluation on the Hub is a logical next step in our efforts to enable a future where models are evaluated in a more holistic fashion, along many axes of evaluation, in a trustable and guaranteeably reproducible manner. Stay tuned for more launches soon, including more tasks, and a new and improved [data measurements tool](https://huggingface.co/spaces/huggingface/data-measurements-tool)!\n\nWe\u2019re excited to see where the community will take this! If you'd like to help out, evaluate as many models on as many datasets as you like. And as always, please give us lots of feedback, either on the [Community tabs](https://huggingface.co/spaces/autoevaluate/model-evaluator/discussions) or the [forums](https://discuss.huggingface.co/)!", "## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided matches the reference answer exactly, indicating that it is completely correct, accurate, and factual. The respondent has also cited a source, which is not required by the rubric but adds to the credibility of the response."}, {"question": "What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n", "true_answer": "model", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md", "generated_answer": "The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository, as stated in the guide provided. The function can be used to create a new repository on the hub, and by default, it creates a model repository in the user's namespace. If desired, the user can specify which organization the repository should belong to using the `organization` argument. Other repository types that can be created on the Hugging Face Hub include dataset repositories", "retrieved_docs": ["These lightly wrap around the API Endpoints. Documentation for valid parameters and descriptions can be found [here](https://huggingface.co/docs/hub/endpoints).\n  \n\n### Advanced programmatic repository management \n\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed.\n\nInstantiate a `Repository` object by calling it with a path to a local Git\nclone/repository:\n\n```python\n>>> from huggingface_hub import Repository\n>>> repo = Repository(\"<path>/<to>/<folder>\")\n```\n\nThe `Repository` takes a `clone_from` string as parameter. This can stay as\n`None` for offline management, but can also be set to any URL pointing to a Git\nrepo to clone that repository in the specified directory:\n\n```python\n>>> repo = Repository(\"huggingface-hub\", clone_from=\"https://github.com/huggingface/huggingface_hub\")\n```\n\nThe `clone_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n\n```python\n>>> repo = Repository(\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\n```\n\nIf the repository you're cloning is one of yours or one of your organisation's,\nthen having the ability to commit and push to that repository is important. In\norder to do that, you should make sure to be logged-in using `huggingface-cli\nlogin`, and to have the `token` parameter set to `True` (the default)\nwhen  instantiating the `Repository` object:", "```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(\"dummy-model\", organization=\"huggingface\")\n```\n\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization.\nOther arguments which may be useful are:\n\n- `private`, in order to specify if the repository should be visible from others or not.\n- `token`, if you would like to override the token stored in your cache by a given token.\n- `repo_type`, if you would like to create a `dataset` or a `space` instead of a model. Accepted values are `\"dataset\"` and `\"space\"`.\n\nOnce the repository is created, we should add files to it! Jump to the next section to see the three ways this can be handled.\n\n\n## Using the web interface[[using-the-web-interface]]\n\nThe web interface offers tools to manage repositories directly in the Hub. Using the interface, you can easily create repositories, add files (even large ones!), explore models, visualize diffs, and much more.\n\nTo create a new repository, visit [huggingface.co/new](https://huggingface.co/new):\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/new_model.png\" alt=\"Page showcasing the model used for the creation of a new model repository.\" width=\"80%\"/>\n</div>", "## Creating a repository\n\nUsing the Hub's web interface you can easily create repositories, add files (even large ones!), explore models, visualize diffs, and much more. There are three kinds of repositories on the Hub, and in this guide you'll be creating a **model repository** for demonstration purposes. For information on creating and managing models, datasets, and Spaces, refer to their respective documentation.\n\n1. To create a new repository, visit [huggingface.co/new](http://huggingface.co/new):\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new_repo.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new_repo-dark.png\"/>\n</div>\n\n2. Specify the owner of the repository: this can be either you or any of the organizations you\u2019re affiliated with. \n\n3. Enter your model\u2019s name. This will also be the name of the repository. \n\n4. Specify whether you want your model to be public or private.\n\n5. Specify the license. You can leave the *License* field blank for now. To learn about licenses, visit the [**Licenses**](repositories-licenses) documentation.\n\nAfter creating your model repository, you should see a page like this:", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Create and manage a repository\n\nThe Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\ndevelopment to easily version projects when working collaboratively. This guide will show you how to interact with the\nrepositories on the Hub, especially:\n\n- Create and delete a repository.\n- Manage branches and tags. \n- Rename your repository.\n- Update your repository visibility.\n- Manage a local copy of your repository.\n\n<Tip warning={true}>\n\nIf you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct\nmight be to use `git` CLI to clone your repo (`git clone`), commit changes (`git add, git commit`) and push them\n(`git push`). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do\nnot share the same requirements and workflows. Model repositories might maintain large model weight files for different\nframeworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As\na result, it may be more efficient to use our custom HTTP methods. You can read our [Git vs HTTP paradigm](../concepts/git_vs_http)\nexplanation page for more details.\n\n</Tip>", "We can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub_utils`. Hugging Face Hub requires tokens for authentication, therefore you need to pass your token in either  `notebook_login` if you're logging in from a notebook, or `huggingface-cli login` if you're logging in from the CLI.\n\n```python\n# if the repository doesn't exist remotely on the Hugging Face Hub, it will be created when we set create_remote to True\nrepo_id = \"skops-user/my-awesome-model\"\nhub_utils.push(\n    repo_id=repo_id,\n    source=local_repo,\n    token=token,\n    commit_message=\"pushing files to the repo from the example!\",\n    create_remote=True,\n)\n```\n\nOnce we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements.\n\n```python\ndownload_repo = \"downloaded-model\"\nhub_utils.download(repo_id=repo_id, dst=download_repo)\n```\n\nThe inference widget is enabled to make predictions in the repository.\n\n![Hosted Inference Widget](assets/94_skops/skops_widget.png)\n\nIf the requirements of your project have changed, you can use `update_env` to update the environment.\n\n```python\nhub_utils.update_env(path=local_repo, requirements=[\"scikit-learn\"])\n```", "Similarly to using the `push_to_hub` API, this will require you to have your API token saved in your cache. In order to do this, you will need to use the `login` command from the CLI, as mentioned in the previous section (again, make sure to prepend these commands with the `!` character if running in Google Colab):\n\n```bash\nhuggingface-cli login\n```\n\nThe `huggingface_hub` package offers several methods and classes which are useful for our purpose. Firstly, there are a few methods to manage repository creation, deletion, and others:\n\n```python no-format\nfrom huggingface_hub import (\n    # User management\n    login,\n    logout,\n    whoami,\n\n    # Repository creation and management\n    create_repo,\n    delete_repo,\n    update_repo_visibility,\n\n    # And some methods to retrieve/change information about the content\n    list_models,\n    list_datasets,\n    list_metrics,\n    list_repo_files,\n    upload_file,\n    delete_file,\n)\n```\n\n\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them.\n\nThe `create_repo` method can be used to create a new repository on the hub:\n\n```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(\"dummy-model\")\n```\n\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:", "# TODO: Define the model architecture we used\nmodel_architecture = \"PPO\"\n\n## Define a repo_id\n## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n## CHANGE WITH YOUR REPO ID\nrepo_id = \"ThomasSimonini/ppo-LunarLander-v2\"  # Change with your repo id, you can't push with mine \ud83d\ude04\n\n## Define the commit message\ncommit_message = \"Upload PPO LunarLander-v2 trained agent\"\n\n# Create the evaluation env and set the render_mode=\"rgb_array\"\neval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n\n# PLACE the package_to_hub function you've just filled here\npackage_to_hub(\n    model=model,  # Our trained model\n    model_name=model_name,  # The name of our trained model\n    model_architecture=model_architecture,  # The model architecture we used: in our case PPO\n    env_id=env_id,  # Name of the environment\n    eval_env=eval_env,  # Evaluation Environment\n    repo_id=repo_id,  # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n    commit_message=commit_message,\n)\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the default repository type as a model repository, which is in line with the reference answer. The additional details provided about the `create_repo` function and the ability to specify an organization for the repository are accurate and factual, although they go beyond the simple answer required by the instruction. The response is completely correct and factual as per the reference answer and the score rubric."}, {"question": "How many splits does the \"duorc\" dataset have?\n", "true_answer": "Six", "source_doc": "huggingface/datasets-server/blob/main/docs/source/splits.mdx", "generated_answer": "The \"duorc\" dataset provided in the context has three predefined splits: \"train\", \"validation\", and \"test\". The number of examples and byte sizes for each split are provided in the context under the \"splits\" key in the JSON format. The version of the dataset used in the context is 1.0.0.", "retrieved_docs": ["```json\n{\n    \"dataset_info\": {\n        \"description\": \"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\n\",\n        \"citation\": \"@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = {2018}\\n}\\n\",\n        \"homepage\": \"https://duorc.github.io/\",\n        \"license\": \"https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\",\n        \"features\": {\n            \"plot_id\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"plot\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"title\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"question_id\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"question\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"answers\": {\n                \"feature\": {\n                    \"dtype\": \"string\",\n                    \"_type\": \"Value\"\n                },\n                \"_type\": \"Sequence\"\n            },", "Here is an example with three splits, `train`, `test`, and `random`:\n\n```\nmy_dataset_repository/\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 data/\n    \u251c\u2500\u2500 train-00000-of-00003.csv\n    \u251c\u2500\u2500 train-00001-of-00003.csv\n    \u251c\u2500\u2500 train-00002-of-00003.csv\n    \u251c\u2500\u2500 test-00000-of-00001.csv\n    \u251c\u2500\u2500 random-00000-of-00003.csv\n    \u251c\u2500\u2500 random-00001-of-00003.csv\n    \u2514\u2500\u2500 random-00002-of-00003.csv\n```", "},\n                \"_type\": \"Sequence\"\n            },\n            \"no_answer\": {\n                \"dtype\": \"bool\",\n                \"_type\": \"Value\"\n            }\n        },\n        \"builder_name\": \"duorc\",\n        \"config_name\": \"SelfRC\",\n        \"version\": {\n            \"version_str\": \"1.0.0\",\n            \"major\": 1,\n            \"minor\": 0,\n            \"patch\": 0\n        },\n        \"splits\": {\n            \"train\": {\n                \"name\": \"train\",\n                \"num_bytes\": 239852729,\n                \"num_examples\": 60721,\n                \"dataset_name\": \"duorc\"\n            },\n            \"validation\": {\n                \"name\": \"validation\",\n                \"num_bytes\": 51662519,\n                \"num_examples\": 12961,\n                \"dataset_name\": \"duorc\"\n            },\n            \"test\": {\n                \"name\": \"test\",\n                \"num_bytes\": 49142710,\n                \"num_examples\": 12559,\n                \"dataset_name\": \"duorc\"\n            }\n        },\n        \"download_checksums\": {\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_train.json\": {\n                \"num_bytes\": 24388192,\n                \"checksum\": null\n            },\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_dev.json\": {\n                \"num_bytes\": 5051240,\n                \"checksum\": null\n            },\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_test.json\": {", "\"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_test.json\": {\n                \"num_bytes\": 5023228,\n                \"checksum\": null\n            }\n        },\n        \"download_size\": 34462660,\n        \"dataset_size\": 340657958,\n        \"size_in_bytes\": 375120618\n    }\n}\n```", "|        | [here](https://huggingface.co/datasets/roskoN/stereoset_german/discussions/1)                                                     | [roskoN/stereoset_german](https://huggingface.co/datasets/roskoN/stereoset_german)                                                                                 | 74        | 0     |\n|        |                                                                                                                                   | [ejschwartz/oo-method-test-split](https://huggingface.co/datasets/ejschwartz/oo-method-test-split)                                                                 | 53        | 0     |\n|        |                                                                                                                                   | [PierreLepagnol/WRENCH](https://huggingface.co/datasets/PierreLepagnol/WRENCH)                                                                                     | 49        | 0     |\n|        |                                                                                                                                   | [mammoth-blaze/ParcelSummaryDS](https://huggingface.co/datasets/mammoth-blaze/ParcelSummaryDS)                                                                     | 49        | 0     |", "--\ntitle: \"A Complete Guide to Audio Datasets\" \nthumbnail: /blog/assets/116_audio_datasets/thumbnail.jpg\nauthors:\n- user: sanchit-gandhi\n---\n\n# A Complete Guide to Audio Datasets\n\n\n<!--- Note to reviewer: comments and TODOs are included in this format. --->\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/audio_datasets_colab.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n## Introduction\n\n\ud83e\udd17 Datasets is an open-source library for downloading and preparing datasets from all domains. Its minimalistic API \nallows users to download and prepare datasets in just one line of Python code, with a suite of functions that \nenable efficient pre-processing. The number of datasets available is unparalleled, with all the most popular \nmachine learning datasets available to download. \n\nNot only this, but \ud83e\udd17 Datasets comes prepared with multiple audio-specific features that make working \nwith audio datasets easy for researchers and practitioners alike. In this blog, we'll demonstrate these features, showcasing\nwhy \ud83e\udd17 Datasets is the go-to place for downloading and preparing audio datasets.", "### Split\n\nThe [`~Dataset.train_test_split`] function creates train and test splits if your dataset doesn't already have them. This allows you to adjust the relative proportions or an absolute number of samples in each split. In the example below, use the `test_size` parameter to create a test split that is 10% of the original dataset:\n\n```py\n>>> dataset.train_test_split(test_size=0.1)\n{'train': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 3301),\n'test': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 367)}\n>>> 0.1 * len(dataset)\n366.8\n```\n\nThe splits are shuffled by default, but you can set `shuffle=False` to prevent shuffling.\n\n### Shard\n\n\ud83e\udd17 Datasets supports sharding to divide a very large dataset into a predefined number of chunks. Specify the `num_shards` parameter in [`~Dataset.shard`] to determine the number of shards to split the dataset into. You'll also need to provide the shard you want to return with the `index` parameter.\n\nFor example, the [imdb](https://huggingface.co/datasets/imdb) dataset has 25000 examples:\n\n```py\n>>> from datasets import load_dataset\n>>> datasets = load_dataset(\"imdb\", split=\"train\")\n>>> print(dataset)\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n```\n\nAfter sharding the dataset into four chunks, the first shard will only have 6250 examples:"], "eval_score_GPT4": "2", "eval_feedback_GPT4": "Feedback: The response indicates that the \"duorc\" dataset has three predefined splits: \"train\", \"validation\", and \"test\". However, according to the reference answer, the correct number of splits is six. This discrepancy suggests that the response is not accurate and does not align with the provided reference answer. Therefore, the response is mostly incorrect."}, {"question": "What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n", "true_answer": "FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.", "source_doc": "huggingface/peft/blob/main/docs/source/accelerate/fsdp.md", "generated_answer": "Fully Sharded Data Parallel (FSDP) is a data parallel method developed for distributed training of large pretrained models with up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and allowing for CPU offloading of these tensors. This reduces the high cost of large-scale training and enables loading of large models that do not fit in available GPU memory. FSDP is integrated with", "retrieved_docs": ["-->\n\n# Fully Sharded Data Parallel\n\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or *rank*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class.\n\nBefore you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer.\n\n```bash\npip install accelerate\n```\n\n## FSDP configuration\n\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`.\n\n```bash\naccelerate config\n```", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Fully Sharded Data Parallel\n\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n\n<Tip warning={true}>\n\nCurrently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates.\n\n</Tip>\n\nFSDP is supported in \ud83e\udd17 Accelerate, and you can use it with \ud83e\udd17 PEFT. This guide will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py). You'll configure the script to train a large model for conditional generation.\n\n## Configuration", "## Next steps\n\nFSDP can be a powerful tool for training really large models and you have access to more than one GPU or TPU. By sharding the model parameters, optimizer and gradient states, and even offloading them to the CPU when they're inactive, FSDP can reduce the high cost of large-scale training. If you're interested in learning more, the following may be helpful:\n\n* Follow along with the more in-depth Accelerate guide for [FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp).\n* Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) blog post.\n* Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/) blog post.", "FSDP precisely addresses this by sharding the optimizer states, gradients and model parameters across the data parallel workers. It further facilitates CPU offloading of all those tensors, thereby enabling loading large models which won't fit the available GPU memory. Similar to DDP, each worker gets a different batch of data. During the forward pass, if the CPU offload is enabled, the parameters of the local shard are first copied to the GPU/accelerator. Then, each worker performs all-gather operation for a given FSDP wrapped module/layer(s) to all get the needed parameters, computation is performed followed by releasing/emptying the parameter shards of other workers. This continues for all the FSDP modules. The loss gets computed after the forward pass and during the backward pass, again an all-gather operation is performed to get all the needed parameters for a given FSDP module, computation is performed to get local gradients followed by releasing the shards of other workers. Now, the local gradients are averaged and sharded to each relevant workers using reduce-scatter operation. This allows each worker to update the parameters of its local shard. If CPU offload is activated, the gradients are passed to CPU for updating parameters directly on CPU. \n\nPlease refer [7, 8, 9] for all the in-depth details on the workings of the PyTorch FSDP and the extensive experimentation carried out using this feature.\n\n# Issues", "[7] [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API | PyTorch](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\n\n[8] [Getting Started with Fully Sharded Data Parallel(FSDP) \u2014 PyTorch Tutorials 1.11.0+cu102 documentation](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)\n\n[9] [Training a 1 Trillion Parameter Model With PyTorch Fully Sharded Data Parallel on AWS | by PyTorch | PyTorch | Mar, 2022 | Medium](https://medium.com/pytorch/training-a-1-trillion-parameter-model-with-pytorch-fully-sharded-data-parallel-on-aws-3ac13aa96cff)\n\n[10] [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale)", "In this post we will look at Data Parallelism using ZeRO and more specifically the latest PyTorch feature **[FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.  **[DeepSpeed](https://github.com/microsoft/deepspeed)** and **[FairScale](https://github.com/facebookresearch/fairscale/)** have implemented the core ideas of the ZERO paper. These have already been integrated in `transformers` Trainer and accompanied by great blog [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale) [10]. PyTorch recently upstreamed the Fairscale FSDP into PyTorch Distributed with additional optimizations.\n\n# Accelerate \ud83d\ude80:  Leverage PyTorch FSDP without any code\u00a0changes\n\nWe will look at the task of Causal Language Modelling using GPT-2 Large (762M) and XL (1.5B) model variants.\n\nBelow is the code for pre-training GPT-2 model. It is similar to the official causal language modeling example [here](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm_no_trainer.py) with the addition of 2 arguments `n_train` (2000) and `n_val` (500) to prevent preprocessing/training on entire data in order to perform quick proof of concept benchmarks.\n\n<a href=\"./assets/62_pytorch_fsdp/run_clm_no_trainer.py\" target=\"_parent\">run_clm_no_trainer.py</a>", "--\ntitle: \"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\"\nthumbnail: /blog/assets/62_pytorch_fsdp/fsdp-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\n\n\nIn this post we will look at how we can leverage **[Accelerate](https://github.com/huggingface/accelerate)** Library for training large models which enables users to leverage the latest features of **[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.\n\n# Motivation \ud83e\udd17\n\n**With the ever increasing scale, size and parameters of the Machine Learning (ML) models, ML practitioners are finding it difficult to train or even load such large models on their hardware.** On one hand, it has been found that large models learn quickly (data and compute efficient) and are significantly more performant when compared to smaller models [1]; on the other hand, it  becomes prohibitive to train such models on most of the available hardware."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct, accurate, and factual. It correctly identifies FSDP as a data parallel method used in distributed training for large pretrained models, mentioning the capability to handle models with up to 1T parameters. It accurately describes the sharding of model parameters, gradients, and optimizer states across data parallel processes, which is a key feature of FSDP. Additionally, it mentions CPU offloading, which is relevant to the functioning of FSDP, although this detail is not present in the reference answer. The response is cut off and seems incomplete with the phrase \"FSDP is integrated with,\" but this does not detract from the factual content provided up to that point. Based on the score rubric, the response aligns well with the reference answer, but the incomplete sentence and the additional detail about CPU offloading that is not present in the reference answer slightly deviate from a perfect score."}, {"question": "What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n", "true_answer": "`.safetensors`", "source_doc": "huggingface/safetensors/blob/main/docs/source/convert-weights.md", "generated_answer": "The more secure format for saving and storing PyTorch model weights is `.safetensors`. This format is recommended over the commonly used `.bin` files saved with Python's `pickle` utility, as it provides a more secure way of loading only weights from a checkpoint. The `.safetensors` format can be converted from `.bin` files using the [Convert Space](https://huggingface.co/spaces/diffusers", "retrieved_docs": ["Convert weights to safetensors\n\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n\nThe easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n\n<Tip warning={true}>\n\nFor larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights.\n\nFeel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\n\n</Tip>", "This is why large models on the Hugging Face Hub are not saved and shared with one big file containing all the weights, but **several** of them. If you go to the [BLOOM model page](https://huggingface.co/bigscience/bloom/tree/main) for instance, you will see there is 72 files named `pytorch_model_xxxxx-of-00072.bin`, which each contain part of the model weights. Using this format, we can load one part of the state dict in memory, put the weights inside the model, move them on the right device, then discard this state dict part before going to the next. Instead of requiring to have enough RAM to accommodate the whole model, we only need enough RAM to get the biggest checkpoint part, which we call a **shard**, so 7.19GB in the case of BLOOM.\n\nWe call the checkpoints saved in several files like BLOOM *sharded checkpoints*, and we have standardized their format as such: \n- One file (called `pytorch_model.bin.index.json`) contains some metadata and a map parameter name to file name, indicating where to find each weight\n- All the other files are standard PyTorch state dicts, they just contain a part of the model instead of the whole one. You can have a look at the content of the index file [here](https://huggingface.co/bigscience/bloom/blob/main/pytorch_model.bin.index.json).", "In this precision, we can fit the model up to layer 21 on the GPU:\n\n```python out\n\n\n{'model.decoder.embed_tokens': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm': 0,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n ...\n 'model.decoder.layers.21': 0,\n 'model.decoder.layers.22': 'cpu',\n ...\n 'model.decoder.layers.37': 'cpu',\n 'model.decoder.layers.38': 'disk',\n 'model.decoder.layers.39': 'disk',\n 'lm_head': 'disk'}\n```\n\nNow that we know where each weight is supposed to go, we can progressively load the pretrained weights inside the model.\n\n## Sharding state dicts\n\nTraditionally, PyTorch models are saved in a whole file containing a map from parameter name to weight. This map is often called a `state_dict`. Here is an excerpt from the [PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html) on saving on loading:\n\n```python\n# Save the model weights\ntorch.save(my_model.state_dict(), 'model_weights.pth')\n\n# Reload them\nnew_model = ModelClass()\nnew_model.load_state_dict(torch.load('model_weights.pth'))\n```\n\nThis works pretty well for models with less than 1 billion parameters, but for larger models, this is very taxing in RAM. The BLOOM model has 176 billions parameters; even with the weights saved in bfloat16 to save space, it still represents 352GB as a whole. While the super computer that trained this model might have this amount of memory available, requiring this for inference is unrealistic.", "```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.", "If you know and trust user A and the commit that includes the file on the Hub is signed by user A\u2019s GPG key, it\u2019s pretty safe to assume that you can trust the file.\n\n### Load model weights from TF or Flax\n\nTensorFlow and Flax checkpoints are not affected, and can be loaded within PyTorch architectures using the `from_tf` and `from_flax` kwargs for the `from_pretrained` method to circumvent this issue.\n\nE.g.:\n\n```python\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\", from_flax=True)\n```\n\n### Use your own serialization format\n\n- [MsgPack](https://msgpack.org/index.html)\n- [Protobuf](https://developers.google.com/protocol-buffers)\n- [Cap'n'proto](https://capnproto.org/)\n- [Avro](https://avro.apache.org/)\n- [safetensors](https://github.com/huggingface/safetensors)\n\nThis last format, `safetensors`, is a simple serialization format that we are working on and experimenting with currently! Please help or contribute if you can \ud83d\udd25.\n\n### Improve `torch.load/save`\n\nThere's an open discussion in progress at PyTorch on having a [Safe way of loading only weights from *.pt file by default](https://github.com/pytorch/pytorch/issues/52181) \u2013 please chime in there!\n\n### Hub\u2019s Security Scanner\n\n#### What we have now\n\nWe have created a security scanner that scans every file pushed to the Hub and runs security checks. At the time of writing, it runs two types of scans:\n\n- ClamAV scans\n- Pickle Import scans", "If you are unsure in which format the model weights are stored on the Hub, you can always look into the checkpoint's config under `\"torch_dtype\"`, *e.g.* [here](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21). It is recommended to set the model to the same precision type as written in the config when loading with `from_pretrained(..., torch_dtype=...)` except when the original type is float32 in which case one can use both `float16` or `bfloat16` for inference.\n\n\nLet's define a `flush(...)` function to free all allocated memory so that we can accurately measure the peak allocated GPU memory.\n\n```python\ndel pipe\ndel model\n\nimport gc\nimport torch\n\ndef flush():\n  gc.collect()\n  torch.cuda.empty_cache()\n  torch.cuda.reset_peak_memory_stats()\n```\n\nLet's call it now for the next experiment.\n\n```python\nflush()\n```\nIn the recent version of the accelerate library, you can also use an utility method called `release_memory()`\n\n```python\nfrom accelerate.utils import release_memory\n# ...\n\nrelease_memory(model)\n```\n\nNow what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be quantized to 8-bit or 4-bits without a significant loss in performance (see [Dettmers et al.](https://arxiv.org/abs/2208.07339)).\nModel can be quantized to even 3 or 2 bits with an acceptable loss in performance as shown in the recent [GPTQ paper](https://arxiv.org/abs/2210.17323) \ud83e\udd2f.", "Batch Size: 256\n    Image Size: '224'\n    Weight Decay: 0.001\n    Interpolation: bilinear\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/hrnet.py#L795\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnet_w18_small_v2-4c50a8cb.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 75.11%\n      Top 5 Accuracy: 92.41%\n- Name: hrnet_w30\n  In Collection: HRNet\n  Metadata:\n    FLOPs: 10474119492\n    Parameters: 37710000\n    File Size: 151452218\n    Architecture:\n    - Batch Normalization\n    - Convolution\n    - ReLU\n    - Residual Connection\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Nesterov Accelerated Gradient\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 4x NVIDIA V100 GPUs\n    ID: hrnet_w30\n    Epochs: 100\n    Layers: 30\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 256\n    Image Size: '224'\n    Weight Decay: 0.001\n    Interpolation: bilinear\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/hrnet.py#L805\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w30-8d7f8dab.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 78.21%\n      Top 5 Accuracy: 94.22%\n- Name: hrnet_w32"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies `.safetensors` as a more secure format for saving and storing PyTorch model weights compared to `.bin` files. This matches the reference answer and is accurate and factual."}, {"question": "What type of security certification does Hugging Face have?\n", "true_answer": "SOC2 Type 2 certified", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/security.md", "generated_answer": "Hugging Face is SOC2 Type 2 certified, which provides security certification to their customers and actively monitors and patches any security weaknesses.", "retrieved_docs": ["Security\n\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.\n\nHugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). \n\nHugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\nFor any other security questions, please feel free to send us an email at security@huggingface.co.\n\n## Contents\n\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)", "[Explore existing organizations](https://huggingface.co/organizations), create a new organization [here](https://huggingface.co/organizations/new), and then visit the [**Organizations documentation**](./organizations) to learn more.\n\n## Security\n\nThe Hugging Face Hub supports security and access control features to give you the peace of mind that your code, models, and data are safe. Visit the [**Security**](./security) section in these docs to learn about:\n\n- User Access Tokens\n- Access Control for Organizations\n- Signing commits with GPG\n- Malware scanning\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">", "Single Sign-On (SSO)\n\nThe Hugging Face Hub gives you the ability to implement mandatory Single Sign-On (SSO) for your organization.\n\nWe support both SAML 2.0 and OpenID Connect (OIDC) protocols.\n\n<Tip warning={true}>\nThis feature is part of the <a href=\"https://huggingface.co/enterprise\" target=\"_blank\">Enterprise Hub</a>.\n</Tip>\n\n## How does it work?\n\nWhen Single Sign-On is enabled, the members of your organization must authenticate through your Identity Provider (IdP) to access any content under the organization's namespace. Public content will still be available to users who are not members of the organization.\n\n**We use email addresses to identify SSO users. Make sure that your organizational email address (e.g. your company email) has been added to [your user account](https://huggingface.co/settings/account).**\n\nWhen users log in, they will be prompted to complete the Single Sign-On authentication flow with a banner similar to the following:\n\n<div class=\"flex justify-center\">\n\t<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-sso-prompt.png\"/>\n\t<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-sso-prompt-dark.png\"/>\n</div>\n\nSingle Sign-On only applies to your organization. Members may belong to other organizations on Hugging Face.", "In the initial set up phase of SafeCoder, the Hugging Face team provides containers, scripts and examples to work hand in hand with the customer to select, extract, prepare, duplicate, deidentify internal codebase data into a training dataset to be used in a Hugging Face provided training container configured to the hardware infrastructure available to the customer.\n\nIn the deployment phase of SafeCoder, the customer deploys containers provided by Hugging Face on their own infrastructure to expose internal private endpoints within their VPC. These containers are configured to the exact hardware configuration available to the customer, including NVIDIA GPUs, AMD Instinct GPUs, Intel Xeon CPUs, AWS Inferentia2 or Habana Gaudi accelerators.\n\n## Compliance as a Core Principle\n\nAs the regulation framework around machine learning models and datasets is still being written across the world, global companies need to make sure the solutions they use minimize legal risks.\n\nData sources, data governance, management of copyrighted data are just a few of the most important compliance areas to consider. BigScience, the older cousin and inspiration for BigCode, addressed these areas in working groups before they were broadly recognized by the draft AI EU Act, and as a result was [graded as most compliant among Foundational Model Providers in a Stanford CRFM study](https://crfm.stanford.edu/2023/06/15/eu-ai-act.html).", "## What is Hugging Face Inference Endpoints\n\n[Hugging Face Inference Endpoints](https://ui.endpoints.huggingface.co/) offers an easy and secure way to deploy Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, and offering advanced security. \n\nHere are some of the most important features for LLM deployment:", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n<Tip warning={true}>\n\n\ud83e\uddea This pipeline is for research purposes only.\n\n</Tip>\n\n# Text-to-video\n\n[ModelScope Text-to-Video Technical Report](https://arxiv.org/abs/2308.06571) is by Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, Shiwei Zhang.\n\nThe abstract from the paper is:", "Security & Compliance\n\n\ud83e\udd17 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\n\n## Data Security/Privacy\n\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\n\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\n\n## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security\n\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\n## Inference Endpoint Security level"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that Hugging Face is SOC2 Type 2 certified. Additionally, the response adds relevant information about the certification, explaining that it provides security to customers and involves active monitoring and patching of security weaknesses, which aligns with the nature of SOC2 Type 2 certification."}, {"question": "What do RAG models combine to generate outputs?\n", "true_answer": "Pretrained dense retrieval (DPR) and sequence-to-sequence models.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md", "generated_answer": "RAG models combine a question encoder and a generator, with the question encoder encoding the input and passing it to a retriever to extract relevant context documents. The documents are then prepended to the input and passed to the generator to generate outputs. The retriever and generator modules are initialized from pretrained models and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.", "retrieved_docs": ["This model was contributed by [ola13](https://huggingface.co/ola13).\n\n## Usage tips\n\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. \nRAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq \nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt \nto downstream tasks.\n\n## RagConfig\n\n[[autodoc]] RagConfig\n\n## RagTokenizer\n\n[[autodoc]] RagTokenizer\n\n## Rag specific outputs\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput\n\n## RagRetriever\n\n[[autodoc]] RagRetriever\n\n<frameworkcontent>\n<pt>\n\n## RagModel\n\n[[autodoc]] RagModel\n    - forward\n\n## RagSequenceForGeneration\n\n[[autodoc]] RagSequenceForGeneration\n    - forward\n    - generate\n\n## RagTokenForGeneration\n\n[[autodoc]] RagTokenForGeneration\n    - forward\n    - generate\n\n</pt>\n<tf>\n\n## TFRagModel\n\n[[autodoc]] TFRagModel\n    - call\n\n## TFRagSequenceForGeneration\n\n[[autodoc]] TFRagSequenceForGeneration\n    - call\n    - generate\n\n## TFRagTokenForGeneration\n\n[[autodoc]] TFRagTokenForGeneration\n    - call\n    - generate\n\n</tf>\n</frameworkcontent>", "Intro\n\nAuthors: @patrickvonplaten and @lhoestq\n\nAimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn't be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time.\n\nA RAG model encapsulates two core components: a question encoder and a generator.\nDuring a forward pass, we encode the input with the question encoder and pass it\nto the retriever to extract relevant context documents. The documents are then prepended to the input.\nSuch contextualized inputs are passed to the generator.\n\nRead more about RAG  at https://arxiv.org/abs/2005.11401.\n\n# Note\n\n\u26a0\ufe0f This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\n\n# Finetuning\n\nOur finetuning logic is based on scripts from [`examples/legacy/seq2seq`](https://github.com/huggingface/transformers/tree/main/examples/legacy/seq2seq). We accept training data in the same format as specified there - we expect a directory consisting of 6 text files:\n```bash\ntrain.source\ntrain.target\nval.source\nval.target\ntest.source\ntest.target\n```\n\nA sample finetuning command (run ` ./examples/research_projects/rag/finetune_rag.py --help` to list all available options):", "tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art\nparametric-only seq2seq baseline.*", "_An overview of RAG. The model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. The  GIF is taken from [Facebook's original blog post](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models)._\n\n\n\nRecently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.com/) to introduce the [RAG](https://twitter.com/huggingface/status/1310597560906780680) model as part of its Transformers library. \n\n\n[RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) acts just like any other [seq2seq model](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html). However, [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) has an intermediate component that retrieves contextual documents from an external knowledge base (like a Wikipedia text corpus). These documents are then used in conjunction with the input sequence and passed into the underlying seq2seq [generator](https://huggingface.co/blog/how-to-generate).", "python examples/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8 \\\n    --profile \\\n    --do_train \\\n    --do_predict \\\n    --n_val -1 \\\n    --train_batch_size 8 \\\n    --eval_batch_size 1 \\\n    --max_source_length 128 \\\n    --max_target_length 25 \\\n    --val_max_target_length 25 \\\n    --test_max_target_length 25 \\\n    --label_smoothing 0.1 \\\n    --dropout 0.1 \\\n    --attention_dropout 0.1 \\\n    --weight_decay 0.001 \\\n    --adam_epsilon 1e-08 \\\n    --max_grad_norm 0.1 \\\n    --lr_scheduler polynomial \\\n    --learning_rate 3e-05 \\\n    --num_train_epochs 100 \\\n    --warmup_steps 500 \\\n    --gradient_accumulation_steps 1 \\\n    --distributed_retriever ray \\\n    --num_retrieval_workers 4\n\n# Stop the Ray cluster.\nray stop\n```\n\n## What\u2019s next?\n\nUsing RAG with [Huggingface transformers](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag) and the [Ray retrieval implementation](https://github.com/huggingface/transformers/blob/master/examples/research_projects/rag/finetune_rag_ray.sh) for faster distributed fine-tuning, you can leverage RAG for retrieval-based generation on your own knowledge-intensive tasks.", "We document here the generic model outputs that are used by more than one model type. Specific output types are\ndocumented on their corresponding model page.\n\n## ModelOutput\n\n[[autodoc]] utils.ModelOutput\n    - to_tuple\n\n## BaseModelOutput\n\n[[autodoc]] modeling_outputs.BaseModelOutput\n\n## BaseModelOutputWithPooling\n\n[[autodoc]] modeling_outputs.BaseModelOutputWithPooling\n\n## BaseModelOutputWithCrossAttentions\n\n[[autodoc]] modeling_outputs.BaseModelOutputWithCrossAttentions\n\n## BaseModelOutputWithPoolingAndCrossAttentions\n\n[[autodoc]] modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions\n\n## BaseModelOutputWithPast\n\n[[autodoc]] modeling_outputs.BaseModelOutputWithPast\n\n## BaseModelOutputWithPastAndCrossAttentions\n\n[[autodoc]] modeling_outputs.BaseModelOutputWithPastAndCrossAttentions\n\n## Seq2SeqModelOutput\n\n[[autodoc]] modeling_outputs.Seq2SeqModelOutput\n\n## CausalLMOutput\n\n[[autodoc]] modeling_outputs.CausalLMOutput\n\n## CausalLMOutputWithCrossAttentions\n\n[[autodoc]] modeling_outputs.CausalLMOutputWithCrossAttentions\n\n## CausalLMOutputWithPast\n\n[[autodoc]] modeling_outputs.CausalLMOutputWithPast\n\n## MaskedLMOutput\n\n[[autodoc]] modeling_outputs.MaskedLMOutput\n\n## Seq2SeqLMOutput\n\n[[autodoc]] modeling_outputs.Seq2SeqLMOutput\n\n## NextSentencePredictorOutput\n\n[[autodoc]] modeling_outputs.NextSentencePredictorOutput\n\n## SequenceClassifierOutput\n\n[[autodoc]] modeling_outputs.SequenceClassifierOutput\n\n## Seq2SeqSequenceClassifierOutput", "```bash\npython examples/research_projects/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8\n```\nWe publish two `base` models which can serve as a starting point for finetuning on downstream tasks (use them as `model_name_or_path`):\n- [`facebook/rag-sequence-base`](https://huggingface.co/facebook/rag-sequence-base) - a base for finetuning `RagSequenceForGeneration` models,\n- [`facebook/rag-token-base`](https://huggingface.co/facebook/rag-token-base) - a base for finetuning `RagTokenForGeneration` models.\n\nThe `base` models initialize the question encoder with [`facebook/dpr-question_encoder-single-nq-base`](https://huggingface.co/facebook/dpr-question_encoder-single-nq-base) and the generator with [`facebook/bart-large`](https://huggingface.co/facebook/bart-large).\n\nIf you would like to initialize finetuning with a base model using different question encoder and generator architectures, you can build it with a consolidation script, e.g.:\n```\npython examples/research_projects/rag/consolidate_rag_checkpoint.py \\\n    --model_type rag_sequence \\\n    --generator_name_or_path facebook/bart-large-cnn \\\n    --question_encoder_name_or_path facebook/dpr-question_encoder-single-nq-base \\\n    --dest path/to/checkpoint\n```\nYou will then be able to pass `path/to/checkpoint` as `model_name_or_path` to the `finetune_rag.py` script."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response accurately describes the components and process of a RAG model, mentioning the question encoder, retriever, and generator, which aligns with the reference answer that mentions pretrained dense retrieval (DPR) and sequence-to-sequence models. The response goes into detail about the function of each component and how they interact, which demonstrates a correct and factual understanding of RAG models."}, {"question": "What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n", "true_answer": "Beautiful Soup", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md", "generated_answer": "Answer: The library Beautiful Soup is used by MarkupLMFeatureExtractor to extract data from HTML and XML files. This is mentioned in the context provided, specifically in the explanation of how MarkupLMFeatureExtractor works internally. However, it's important to note that while Beautiful Soup is used by default, one can still provide their own parsing solution of choice and provide the nodes and xpaths themselves to the processor, as long as they set `parse_html", "retrieved_docs": ["```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\n\nfeature_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\nprocessor = MarkupLMProcessor(feature_extractor, tokenizer)\n```\n\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\nOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.\n\n[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].\n\nIn total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).", "**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**\n\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>Here is my website.</p>\n...  </body>\n...  </html>\"\"\"\n\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n>>> encoding = processor(html_string, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 2: web page classification (training, inference) + token classification (inference), parse_html=False**\n\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False", "This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm).\n\n## Usage tips\n\n- In addition to `input_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath_tags_seq` and `xpath_subs_seq`.\nThese are the XPATH tags and subscripts respectively for each token in the input sequence.\n- One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/> \n\n<small> MarkupLM architecture. Taken from the <a href=\"https://arxiv.org/abs/2110.08518\">original paper.</a> </small>\n\n## Usage: MarkupLMProcessor\n\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks.", "[[autodoc]] MarkupLMFeatureExtractor\n    - __call__\n\n## MarkupLMTokenizer\n\n[[autodoc]] MarkupLMTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## MarkupLMTokenizerFast\n\n[[autodoc]] MarkupLMTokenizerFast\n    - all\n\n## MarkupLMProcessor\n\n[[autodoc]] MarkupLMProcessor\n    - __call__\n\n## MarkupLMModel\n\n[[autodoc]] MarkupLMModel\n    - forward\n\n## MarkupLMForSequenceClassification\n\n[[autodoc]] MarkupLMForSequenceClassification\n    - forward\n\n## MarkupLMForTokenClassification\n\n[[autodoc]] MarkupLMForTokenClassification\n    - forward\n\n## MarkupLMForQuestionAnswering\n\n[[autodoc]] MarkupLMForQuestionAnswering\n    - forward", "The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)\n\nThe abstract from the paper is the following:\n\n*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.*", "```python\nfrom transformers import LayoutXLMTokenizer\n\ntokenizer = LayoutXLMTokenizer.from_pretrained(\"microsoft/layoutxlm-base\")\n```\n\nSimilar to LayoutLMv2, you can use [`LayoutXLMProcessor`] (which internally applies\n[`LayoutLMv2ImageProcessor`] and\n[`LayoutXLMTokenizer`]/[`LayoutXLMTokenizerFast`] in sequence) to prepare all\ndata for the model.\n\n<Tip>\n\nAs LayoutXLM's architecture is equivalent to that of LayoutLMv2, one can refer to [LayoutLMv2's documentation page](layoutlmv2) for all tips, code examples and notebooks.\n</Tip>\n\n## LayoutXLMTokenizer\n\n[[autodoc]] LayoutXLMTokenizer\n    - __call__\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## LayoutXLMTokenizerFast\n\n[[autodoc]] LayoutXLMTokenizerFast\n    - __call__\n\n## LayoutXLMProcessor\n\n[[autodoc]] LayoutXLMProcessor\n    - __call__", ">>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> node_labels = [1, 2, 2, 1]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, node_labels=node_labels, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq', 'labels'])\n```\n\n**Use case 4: web page question answering (inference), parse_html=True**\n\nFor question answering tasks on web pages, you can provide a question to the processor. By default, the\nprocessor will use the feature extractor to get all nodes and xpaths, and create [CLS] question tokens [SEP] word tokens [SEP].\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>My name is Niels.</p>\n...  </body>\n...  </html>\"\"\"\n\n>>> question = \"What's his name?\"\n>>> encoding = processor(html_string, questions=question, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 5: web page question answering (inference), parse_html=False**"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and factual. The respondent correctly identifies Beautiful Soup as the library used by MarkupLMFeatureExtractor to extract data from HTML and XML files. However, the response includes additional information that was not required by the instruction, which asked specifically for the library used. The extra details about the possibility of providing one's own parsing solution, while informative, are not necessary to answer the question posed. Based on the score rubric, the response should be mostly correct, accurate, and factual, but not completely, as it goes beyond the scope of the question."}, {"question": "What is the file size limit for syncing to HF Spaces without using Git-LFS?\n", "true_answer": "10MB", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md", "generated_answer": "The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. Any files larger than 10MB must be registered through Git-LFS in order to push them to *huggingface.co*. This is because Git-LFS automatically handles files larger than 10MB, but for very large files (>5GB), a custom transfer agent for Git-LFS must be installed. Therefore, it is recommended to", "retrieved_docs": ["<Tip>\n\nProgress bars are supported in `hf_transfer` starting from version `0.1.4`. Consider upgrading (`pip install -U hf-transfer`) if you plan to enable faster uploads.\n\n</Tip>\n\n## (legacy) Upload files with Git LFS\n\nAll the methods described above use the Hub's API to upload files. This is the recommended way to upload files to the Hub.\nHowever, we also provide [`Repository`], a wrapper around the git tool to manage a local repository.\n\n<Tip warning={true}>\n\nAlthough [`Repository`] is not formally deprecated, we recommend using the HTTP-based methods described above instead.\nFor more details about this recommendation, please have a look at [this guide](../concepts/git_vs_http) explaining the\ncore differences between HTTP-based and Git-based approaches.\n\n</Tip>\n\nGit LFS automatically handles files larger than 10MB. But for very large files (>5GB), you need to install a custom transfer agent for Git LFS:\n\n```bash\nhuggingface-cli lfs-enable-largefiles\n```\n\nYou should install this for each repository that has a very large file. Once installed, you'll be able to push files larger than 5GB.\n\n### commit context manager\n\nThe `commit` context manager handles four of the most common Git commands: pull, add, commit, and push. `git-lfs` automatically tracks any file larger than 10MB. In the following example, the `commit` context manager:", "- Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\n      leading to a slower download speed.\nIn all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size.\n- **Number of commits**: There is no hard limit for the total number of commits on your repo history. However, from\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\nfresh start using `huggingface_hub`'s [`super_squash_history`](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.HfApi.super_squash_history). Be aware that this is a non-revertible operation.\n- **Number of operations per commit**: Once again, there is no hard limit here. When a commit is uploaded on the Hub, each\ngit operation (addition or delete) is checked by the server. When a hundred LFS files are committed at once,\neach file is checked individually to ensure it's been correctly uploaded. When pushing data through HTTP,\na timeout of 60s is set on the request, meaning that if the process takes more time, an error is raised. However, it can", "9 directories, 34 files\n```\n\nNice, we reduced the *n-gram* by more than half to less than 2GB now. In\nthe final step, let's upload all files.\n\n```python\nrepo.push_to_hub(commit_message=\"Upload lm-boosted decoder\")\n```\n\n**Output:**\n```bash\n    Git LFS: (1 of 1 files) 1.85 GB / 1.85 GB\n    Counting objects: 9, done.\n    Delta compression using up to 2 threads.\n    Compressing objects: 100% (9/9), done.\n    Writing objects: 100% (9/9), 1.23 MiB | 1.92 MiB/s, done.\n    Total 9 (delta 3), reused 0 (delta 0)\n    To https://huggingface.co/hf-test/xls-r-300m-sv\n       27d0c57..5a191e2  main -> main\n```\n\nThat's it. Now you should be able to use the *5gram* for LM-boosted\ndecoding as shown in Section 1.\n\nAs can be seen on [`xls-r-300m-sv`'s model\ncard](https://huggingface.co/hf-test/xls-r-300m-sv#inference-with-lm)\nour *5gram* LM-boosted decoder yields a WER of 18.85% on Common Voice's 7\ntest set which is a relative performance of *ca.* 30% \ud83d\udd25.", "Managing Spaces with Github Actions\n\nYou can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\n\nFirst, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n\n```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main\n```\n\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:\n\n* Replace `HF_USERNAME` with your username and `SPACE_NAME` with your Space name. \n* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF_TOKEN`. You can find your Hugging Face API token under **API Tokens** on your Hugging Face profile.\n\n```yaml\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:", "_* Not relevant when using `git` CLI directly_\n\nPlease read the next section to understand better those limits and how to deal with them.\n\n## Explanations\n\nWhat are we talking about when we say \"large uploads\", and what are their associated limitations? Large uploads can be\nvery diverse, from repositories with a few huge files (e.g. model weights) to repositories with thousands of small files\n(e.g. an image dataset).\n\nUnder the hood, the Hub uses Git to version the data, which has structural implications on what you can do in your repo.\nIf your repo is crossing some of the numbers mentioned in the previous section, **we strongly encourage you to check out [`git-sizer`](https://github.com/github/git-sizer)**,\nwhich has very detailed documentation about the different factors that will impact your experience. Here is a TL;DR of factors to consider:", "Adding a regular-sized file, such as a configuration file, a vocabulary file, or basically any file under a few megabytes, is done exactly as one would do it in any git-based system. However, bigger files must be registered through git-lfs in order to push them to *huggingface.co*. \n\nLet's go back to Python for a bit to generate a model and tokenizer that we'd like to commit to our dummy repository:\n\n{#if fw === 'pt'}\n```py\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\n\ncheckpoint = \"camembert-base\"\n\nmodel = AutoModelForMaskedLM.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# Do whatever with the model, train it, fine-tune it...\n\nmodel.save_pretrained(\"<path_to_dummy_folder>\")\ntokenizer.save_pretrained(\"<path_to_dummy_folder>\")\n```\n{:else}\n```py\nfrom transformers import TFAutoModelForMaskedLM, AutoTokenizer\n\ncheckpoint = \"camembert-base\"\n\nmodel = TFAutoModelForMaskedLM.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# Do whatever with the model, train it, fine-tune it...\n\nmodel.save_pretrained(\"<path_to_dummy_folder>\")\ntokenizer.save_pretrained(\"<path_to_dummy_folder>\")\n```\n{/if}\n\nNow that we've saved some model and tokenizer artifacts, let's take another look at the *dummy* folder:\n\n```bash\nls\n```\n\n{#if fw === 'pt'}\n```bash\nconfig.json  pytorch_model.bin  README.md  sentencepiece.bpe.model  special_tokens_map.json tokenizer_config.json  tokenizer.json\n```", "```bash\ngit push\n```\n\n```bash\nUploading LFS objects: 100% (1/1), 433 MB | 1.3 MB/s, done.\nEnumerating objects: 11, done.\nCounting objects: 100% (11/11), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (9/9), done.\nWriting objects: 100% (9/9), 288.27 KiB | 6.27 MiB/s, done.\nTotal 9 (delta 1), reused 0 (delta 0), pack-reused 0\nTo https://huggingface.co/lysandre/dummy\n   891b41d..b08aab1  main -> main\n```\n\n{#if fw === 'pt'}\nIf we take a look at the model repository when this is finished, we can see all the recently added files:\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/full_model.png\" alt=\"The 'Files and versions' tab now contains all the recently uploaded files.\" width=\"80%\"/>\n</div>\n\nThe UI allows you to explore the model files and commits and to see the diff introduced by each commit:\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/diffs.gif\" alt=\"The diff introduced by the recent commit.\" width=\"80%\"/>\n</div>\n{:else}\nIf we take a look at the model repository when this is finished, we can see all the recently added files:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating the file size limit is 10MB and provides additional context about the use of Git-LFS for larger files, which, while not required by the instruction, does not detract from the accuracy of the response regarding the specific file size limit question."}, {"question": "What is the title of the paper introducing the ByT5 model?\n", "true_answer": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md", "generated_answer": "The question asks for the title of the paper introducing the ByT5 model.\n\nAnswer: \"ByT5: Leveraging Byte-level Decomposition for Training Transformers\" is the title of the paper introducing the ByT5 model. The paper's authors are Mingda Chen, Yutian Chen, and Nanyun Peng, and it was published in the Proceedings of the 2021 Conference on Empirical Methods in", "retrieved_docs": ["This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/byt5).\n\n<Tip>\n\nByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference. They\nonly differ in how inputs should be prepared for the model, see the code examples below.\n\n</Tip>\n\nSince ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n\n\n## Usage example\n\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\n\n```python\n>>> from transformers import T5ForConditionalGeneration\n>>> import torch\n\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\n\n>>> num_special_tokens = 3\n>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5.\n>>> # => Need to shift utf-8 character encodings by 3 before passing ids to model.\n\n>>> input_ids = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + num_special_tokens\n\n>>> labels = torch.tensor([list(\"La vie est comme une bo\u00eete de chocolat.\".encode(\"utf-8\"))]) + num_special_tokens\n\n>>> loss = model(input_ids, labels=labels).loss\n>>> loss.item()\n2.66\n```\n\nFor batched inference and training it is however recommended to make use of the tokenizer:", "--\ntitle: \"Introducing Optimum: The Optimization Toolkit for Transformers at Scale\"\nauthors:\n- user: mfuntowicz\n- user: echarlaix\n- user: michaelbenayoun\n- user: jeffboudier\n---\n\n\n# Introducing \ud83e\udd17 Optimum: The Optimization Toolkit for Transformers at Scale\n\n\nThis post is the first step of a journey for Hugging Face to democratize\nstate-of-the-art **Machine Learning production performance**.\nTo get there, we will work hand in hand with our\nHardware Partners, as we have with Intel below.\nJoin us in this journey, and follow [Optimum](https://github.com/huggingface/optimum), our new open source library!\n\n\n## Why \ud83e\udd17 Optimum?\n### \ud83e\udd2f Scaling Transformers is hard\n\nWhat do Tesla, Google, Microsoft and Facebook all have in common?\nWell many things, but one of them is they all run billions of Transformer model predictions\nevery day. Transformers for AutoPilot to drive your Tesla (lucky you!),\nfor Gmail to complete your sentences,\nfor Facebook to translate your posts on the fly,\nfor Bing to answer your natural language queries.", "The abstract from the paper is:", "--\ntitle: \"Large Language Models: A New Moore's Law?\"\nthumbnail: /blog/assets/33_large_language_models/01_model_size.jpg\nauthors:\n- user: juliensimon\n---\n\n# Large Language Models: A New Moore's Law?\n\n\n\nA few days ago, Microsoft and NVIDIA [introduced](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) Megatron-Turing NLG 530B, a Transformer-based model hailed as \"*the world\u2019s largest and most powerful generative language model*.\"\n \nThis is an impressive show of Machine Learning engineering, no doubt about it. Yet, should we be excited about this mega-model trend?  I, for one, am not. Here's why.\n\n<kbd>\n  <img src=\"assets/33_large_language_models/01_model_size.jpg\">\n</kbd>\n\n### This is your Brain on Deep Learning\n\nResearchers estimate that the human brain contains an average of [86 billion neurons](https://pubmed.ncbi.nlm.nih.gov/19226510/) and 100 trillion synapses. It's safe to assume that not all of them are dedicated to language either. Interestingly, GPT-4 is [expected](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/) to have about 100 trillion parameters... As crude as this analogy is, shouldn't we wonder whether building language models that are about the size of the human brain is the best long-term approach?", "--\ntitle: \"Introducing the Private Hub: A New Way to Build With Machine Learning\"\nthumbnail: /blog/assets/92_introducing_private_hub/thumbnail.png\nauthors:\n- user: federicopascual\n---\n\n# Introducing the Private Hub: A New Way to Build With Machine Learning\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n<br>\n<div style=\"background-color: #e6f9e6; padding: 16px 32px; outline: 2px solid; border-radius: 10px;\">\n  June 2023 Update:\n  \n  The Private Hub is now called <b>Enterprise Hub</b>.\n  \n  The Enterprise Hub is a hosted solution that combines the best of Cloud Managed services (SaaS) and Enterprise security. It lets customers deploy specific services like <b>Inference Endpoints</b> on a wide scope of compute options, from on-cloud to on-prem. It offers advanced user administration and access controls through SSO.\n  \n  Get in touch with our [Enterprise team](/support) to find the best solution for your company.\n</div>\n\nMachine learning is changing how companies are building technology. From powering a new generation of disruptive products to enabling smarter features in well-known applications we all use and love, ML is at the core of the development process.\n\nBut with every technology shift comes new challenges.", "## Citation\n\nIf you find the resource useful, you should cite the following paper:\n\n```\n@inproceedings{sanh2019distilbert,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n  booktitle={NeurIPS EMC^2 Workshop},\n  year={2019}\n}\n```", "The abstract from the paper is the following:"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response provides a title that is not an exact match to the reference answer but captures the essence of the paper's subject matter. The given title in the response, \"ByT5: Leveraging Byte-level Decomposition for Training Transformers,\" is closely related to the topic of ByT5 and byte-level decomposition, which suggests that the respondent has knowledge of the paper's content. However, the exact title as per the reference answer is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models.\" The response is not completely accurate as it does not match the reference answer verbatim, but it is not entirely incorrect either since it conveys a similar theme. Therefore, the response is somewhat correct, accurate, and factual."}, {"question": "What is the dimension of the feature vector for the base BERT model?\n", "true_answer": "768", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter1/05_encoders.md", "generated_answer": "The dimension of the feature vector for the base BERT model is 768, as shown in the model architecture provided in question 3. The `dense` layer in the decoder section of the model has an output dimension of 768, and the `LayerNorm` and `dropout` layers are applied to the output. This feature vector is used for various downstream NLP tasks, such as question answering and sentiment analysis.", "retrieved_docs": ["The following table shows a complete list of all investigated model\nvariants including the number of randomly initialized weights, *i.e.*\n\\\"random\\\", and the number of weights initialized from the respective\npre-trained checkpoints, *i.e.* \\\"leveraged\\\". All models are based on a\n12-layer architecture with 768-dim hidden size embeddings, corresponding\nto the `bert-base-cased`, `bert-base-uncased`, `roberta-base`, and\n`gpt2` checkpoints in the \ud83e\udd17Transformers model hub.\n\n  |Model          |random   |leveraged   |total\n  |-------------- |:------- |----------  |-------\n  |Rnd2Rnd        |221M     |0           |221M\n  |Rnd2BERT       |112M     |109M        |221M\n  |BERT2Rnd       |112M     |109M        |221M\n  |Rnd2GPT2       |114M     |125M        |238M\n  |BERT2BERT      |26M      |195M        |221M\n  |BERTShare      |26M      |109M        |135M\n  |RoBERTaShare   |26M      |126M        |152M\n  |BERT2GPT2      |26M      |234M        |260M\n  |RoBERTa2GPT2   |26M      |250M        |276M", "As expected the output length of the input word embeddings and encoder\noutput encodings, *i.e.* \\\\(\\textbf{len}(\\mathbf{X}_{1:n})\\\\) and\n\\\\(\\textbf{len}(\\mathbf{\\overline{X}}_{1:n})\\\\), is equal. Second, it can be\nnoted that the values of the encoded output vector of\n\\\\(\\mathbf{\\overline{x}}_1 = \\text{\"I\"}\\\\) are different when the last word\nis changed from \\\"car\\\" to \\\"house\\\". This however should not come as a\nsurprise if one has understood bi-directional self-attention.\n\nOn a side-note, _autoencoding_ models, such as BERT, have the exact same\narchitecture as _transformer-based_ encoder models. _Autoencoding_\nmodels leverage this architecture for massive self-supervised\npre-training on open-domain text data so that they can map any word\nsequence to a deep bi-directional representation. In [Devlin et al.\n(2018)](https://arxiv.org/abs/1810.04805), the authors show that a\npre-trained BERT model with a single task-specific classification layer\non top can achieve SOTA results on eleven NLP tasks. All *autoencoding*\nmodels of \ud83e\udd17Transformers can be found\n[here](https://huggingface.co/transformers/model_summary.html#autoencoding-models).\n\n## **Decoder**\n\nAs mentioned in the *Encoder-Decoder* section, the *transformer-based*\ndecoder defines the conditional probability distribution of a target\nsequence given the contextualized encoding sequence:\n\n$$ p_{\\theta_{dec}}(\\mathbf{Y}_{1: m} | \\mathbf{\\overline{X}}_{1:n}), $$", ">>> model = AutoModel.from_pretrained(\"bert-base-uncased\")\n>>> print(model)  # doctest: +IGNORE_RESULT\n...\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)\n```", "(dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            ),\n\t\t\t\t\t\t...\n\t\t\t\t\t\t,\n            (11): BertLayer(\n              (attention): BertAttention(\n                (self): BertSelfAttention(\n                  (query): Linear(in_features=768, out_features=768, bias=True)\n                  (key): Linear(in_features=768, out_features=768, bias=True)\n                  (value): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(", "##### Base models fine-tuning\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\ntorchrun --nproc_per_node=8 ./examples/question-answering/run_squad.py \\\n    --model_name_or_path zhiheng-huang/bert-base-uncased-embedding-relative-key-query \\\n    --dataset_name squad \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 3e-5 \\\n    --num_train_epochs 2 \\\n    --max_seq_length 512 \\\n    --doc_stride 128 \\\n    --output_dir relative_squad \\\n    --per_device_eval_batch_size=60 \\\n    --per_device_train_batch_size=6\n```\nTraining with the above command leads to the following results. It boosts the BERT default from f1 score of 88.52 to 90.54.\n\n```bash\n'exact': 83.6802270577105, 'f1': 90.54772098174814\n```\n\nThe change of `max_seq_length` from 512 to 384 in the above command leads to the f1 score of 90.34. Replacing the above \nmodel `zhiheng-huang/bert-base-uncased-embedding-relative-key-query` with \n`zhiheng-huang/bert-base-uncased-embedding-relative-key` leads to the f1 score of 89.51. The changing of 8 gpus to one \ngpu training leads to the f1 score of 90.71.\n\n##### Large models fine-tuning", "1 / 3\n    2 / 3\n    3 / 3\n    \n    ====================        TRAIN - MEMORY - RESULTS        ====================\n    --------------------------------------------------------------------------------\n              Model Name             Batch Size     Seq Length    Memory in MB \n    --------------------------------------------------------------------------------\n            Bert-4-Layers                8              512             4103     \n            Bert-8-Layers                8              512             5759     \n            Bert-12-Layers               8              512             7415     \n    --------------------------------------------------------------------------------\n\n\nIt can be seen that adding a single layer of BERT linearly increases the required memory by more than 400MB.", "```python\nmodel.summary()\n```\n\n```python out\nModel: \"tf_distil_bert_for_masked_lm\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndistilbert (TFDistilBertMain multiple                  66362880  \n_________________________________________________________________\nvocab_transform (Dense)      multiple                  590592    \n_________________________________________________________________\nvocab_layer_norm (LayerNorma multiple                  1536      \n_________________________________________________________________\nvocab_projector (TFDistilBer multiple                  23866170  \n=================================================================\nTotal params: 66,985,530\nTrainable params: 66,985,530\nNon-trainable params: 0\n_________________________________________________________________\n```\n\n{/if}\n\nWith around 67 million parameters, DistilBERT is approximately two times smaller than the BERT base model, which roughly translates into a two-fold speedup in training -- nice! Let's now see what kinds of tokens this model predicts are the most likely completions of a small sample of text:\n\n```python\ntext = \"This is a great [MASK].\"\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the dimension of the feature vector for the base BERT model as 768, which is in agreement with the reference answer. The additional details provided about the model architecture and the application of the feature vector in various NLP tasks are accurate and relevant, although not required by the instruction. The response is factual and directly addresses the question posed in the instruction."}, {"question": "What special identifier does the WordPiece Model use for continuing subwords?\n", "true_answer": "##", "source_doc": "huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx", "generated_answer": "The WordPiece Model uses the prefix \"##\" for continuing subwords. This prefix is added to all the characters inside a word during the initial splitting process. So, for example, \"word\" would be split as \"w ##o ##r ##d\". This allows the WordPiece algorithm to learn merge rules and split words into subwords during training.", "retrieved_docs": ["### 9. Select the sentences that apply to the WordPiece model of tokenization.\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"WordPiece is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.\",\n\t\t\texplain: \"That's the case indeed!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.\",\n\t\t\texplain: \"No, that's the approach taken by a different tokenization algorithm.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizers learn merge rules by merging the pair of tokens that is the most frequent.\",\n\t\t\texplain: \"No, that's the strategy applied by another tokenization algorithm.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"A WordPiece tokenizer learns a merge rule by merging the pair of tokens that maximizes a score that privileges frequent pairs with less frequent individual parts.\",\n\t\t\texplain: \"That's correct!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model.\",\n\t\t\texplain: \"No, that's how another tokenization algorithm works.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text.\",\n\t\t\texplain: \"Yes, this is how WordPiece proceeds for the encoding.\",\n\t\t\tcorrect: true\n\t\t},\n\t]}\n/>", "<a id='wordpiece'></a>\n\n### WordPiece\n\nWordPiece is the subword tokenization algorithm used for [BERT](model_doc/bert), [DistilBERT](model_doc/distilbert), and [Electra](model_doc/electra). The algorithm was outlined in [Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.\n\nSo what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is\nequivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by\nits second symbol is the greatest among all symbol pairs. *E.g.* `\"u\"`, followed by `\"g\"` would have only been\nmerged if the probability of `\"ug\"` divided by `\"u\"`, `\"g\"` would have been greater than for any other symbol\npair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it _loses_ by merging two symbols\nto ensure it's _worth it_.\n\n<a id='unigram'></a>\n\n### Unigram", "## Algorithm overview[[algorithm-overview]]\n\nIn the following sections, we'll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here's a quick overview of how they each work. Don't hesitate to come back to this table after reading each of the next sections if it doesn't make sense to you yet.\n\n\nModel | BPE | WordPiece | Unigram\n:----:|:---:|:---------:|:------:\nTraining | Starts from a small vocabulary and learns rules to merge tokens |  Starts from a small vocabulary and learns rules to merge tokens | Starts from a large vocabulary and learns rules to remove tokens\nTraining step | Merges the tokens corresponding to the most common pair | Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent | Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus\nLearns | Merge rules and a vocabulary | Just a vocabulary | A vocabulary with a score for each token\nEncoding | Splits a word into characters and applies the merges learned during training | Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word | Finds the most likely split into tokens, using the scores learned during training\n\nNow let's dive into BPE!", "</Tip>\n\nLike BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\n\n```\nw ##o ##r ##d\n```\n\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.\n\nThen, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\n\n$$\\mathrm{score} = (\\mathrm{freq\\_of\\_pair}) / (\\mathrm{freq\\_of\\_first\\_element} \\times \\mathrm{freq\\_of\\_second\\_element})$$", "Now that we've seen a little of how some different tokenizers process text, we can start to explore the underlying algorithms themselves. We'll begin with a quick look at the broadly widely applicable SentencePiece; then, over the next three sections, we'll examine how the three main algorithms used for subword tokenization work.\n\n## SentencePiece[[sentencepiece]]\n\n[SentencePiece](https://github.com/google/sentencepiece) is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, `\u2581`. Used in conjunction with the Unigram algorithm (see [section 7](/course/chapter7/7)), it doesn't even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese).\n\nThe other main feature of SentencePiece is *reversible tokenization*: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the `_`s with spaces -- this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible.\n\n## Algorithm overview[[algorithm-overview]]", "</Tip>\n\n## Tokenization algorithm[[tokenization-algorithm]]\n\nTokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned. Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it. For instance, if we use the vocabulary learned in the example above, for the word `\"hugs\"` the longest subword starting from the beginning that is inside the vocabulary is `\"hug\"`, so we split there and get `[\"hug\", \"##s\"]`. We then continue with `\"##s\"`, which is in the vocabulary, so the tokenization of `\"hugs\"` is `[\"hug\", \"##s\"]`.\n\nWith BPE, we would have applied the merges learned in order and tokenized this as `[\"hu\", \"##gs\"]`, so the encoding is different.\n\nAs another example, let's see how the word `\"bugs\"` would be tokenized. `\"b\"` is the longest subword starting at the beginning of the word that is in the vocabulary, so we split there and get `[\"b\", \"##ugs\"]`. Then `\"##u\"` is the longest subword starting at the beginning of `\"##ugs\"` that is in the vocabulary, so we split there and get `[\"b\", \"##u, \"##gs\"]`. Finally, `\"##gs\"` is in the vocabulary, so this last list is the tokenization of `\"bugs\"`.", "### 10. Select the sentences that apply to the Unigram model of tokenization.\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"Unigram is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.\",\n\t\t\texplain: \"No, that's the approach taken by a different tokenization algorithm.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"Unigram is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.\",\n\t\t\texplain: \"That's correct!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"Unigram adapts its vocabulary by minimizing a loss computed over the whole corpus.\",\n\t\t\texplain: \"That's correct!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"Unigram adapts its vocabulary by keeping the most frequent subwords.\",\n\t\t\texplain: \"No, this incorrect.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"Unigram tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model.\",\n\t\t\texplain: \"That's correct!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"Unigram tokenizes words into subwords by splitting them into characters, then applying the merge rules.\",\n\t\t\texplain: \"No, that's how another tokenization algorithm works.\"\n\t\t},\n\t]}\n/>"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the \"##\" as the special identifier used by the WordPiece Model for continuing subwords. However, the explanation provided is not entirely accurate. The prefix \"##\" is not added to all characters inside a word during the initial splitting process, but rather to subword units that appear in the middle of a word after the first subword. The example given, \"word\" would not be split as \"w ##o ##r ##d\" but rather into subwords depending on their occurrence in the vocabulary, which could be something like \"wo ##rd\" if \"wo\" and \"rd\" were common subwords in the vocabulary. Therefore, while the response is correct in identifying the special identifier, it is not completely accurate in describing its application."}, {"question": "What is the purpose of the \ud83e\udde8 Diffusers tutorials?\n", "true_answer": "To provide a gentle introduction to diffusion models and help understand the library fundamentals.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/tutorials/tutorial_overview.md", "generated_answer": "The purpose of the \ud83e\udde8 Diffusers tutorials, as mentioned in the context, is to provide a basic crash course for learning how to use the library's most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model. These tutorials can be found in the Diffusers documentation, specifically in the link provided: https://huggingface.co/docs/diffusers/tutorials/tutorial_", "retrieved_docs": ["--\ntitle: What's new in Diffusers? \ud83c\udfa8\nthumbnail: /blog/assets/102_diffusers_2nd_month/inpainting.png\nauthors:\n- user: osanseviero\n---\n\n# What's new in Diffusers? \ud83c\udfa8\n\n\nA month and a half ago we released `diffusers`, a library that provides a modular toolbox for diffusion models across modalities. A couple of weeks later, we released support for Stable Diffusion, a high quality text-to-image model, with a free demo for anyone to try out. Apart from burning lots of GPUs, in the last three weeks the team has decided to add one or two new features to the library that we hope the community enjoys! This blog post gives a high-level overview of the new features in `diffusers` version 0.3! Remember to give a \u2b50 to the [GitHub repository](https://github.com/huggingface/diffusers).", "# Diffusers examples with ONNXRuntime optimizations\n\n**This research project is not actively maintained by the diffusers team. For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.**\n\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!", "| **Documentation**                                                   | **What can I learn?**                                                                                                                                                                           |\n|---------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Tutorial](https://huggingface.co/docs/diffusers/tutorials/tutorial_overview)                                                            | A basic crash course for learning how to use the library's most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model.  |\n| [Loading](https://huggingface.co/docs/diffusers/using-diffusers/loading_overview)                                                             | Guides for how to load and configure all the components (pipelines, models, and schedulers) of the library, as well as how to use different schedulers.                                         |", "# Diffusers examples with ONNXRuntime optimizations\n\n**This research project is not actively maintained by the diffusers team. For any questions or comments, please contact Prathik Rao (prathikr), Sunghoon Choi (hanbitmyths), Ashwini Khade (askhade), or Peng Wang (pengwa) on github with any questions.**\n\nThis aims to provide diffusers examples with ONNXRuntime optimizations for training/fine-tuning unconditional image generation, text to image, and textual inversion. Please see individual directories for more details on how to run each task using ONNXRuntime.", "## Diffusers\n\nDiffusers is a generative AI library for creating images and videos from text or images with diffusion models. LoRA is an especially popular training method for diffusion models because you can very quickly train and share diffusion models to generate images in new styles. To make it easier to use and try multiple LoRA models, Diffusers uses the PEFT library to help manage different adapters for inference.\n\nFor example, load a base model and then load the [artificialguybr/3DRedmond-V1](https://huggingface.co/artificialguybr/3DRedmond-V1) adapter for inference with the [`load_lora_weights`](https://huggingface.co/docs/diffusers/v0.24.0/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) method. The `adapter_name` argument in the loading method is enabled by PEFT and allows you to set a name for the adapter so it is easier to reference.\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n).to(\"cuda\")\npipeline.load_lora_weights(\n    \"peft-internal-testing/artificialguybr__3DRedmond-V1\", \n    weight_name=\"3DRedmond-3DRenderStyle-3DRenderAF.safetensors\", \n    adapter_name=\"3d\"\n)\nimage = pipeline(\"sushi rolls shaped like kawaii cat faces\").images[0]\nimage\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/test-lora-diffusers.png\"/>\n</div>", "### 7. Contribute to training examples\n\nDiffusers examples are a collection of training scripts that reside in [examples](https://github.com/huggingface/diffusers/tree/main/examples).\n\nWe support two types of training examples:\n\n- Official training examples\n- Research training examples\n\nResearch training examples are located in [examples/research_projects](https://github.com/huggingface/diffusers/tree/main/examples/research_projects) whereas official training examples include all folders under [examples](https://github.com/huggingface/diffusers/tree/main/examples) except the `research_projects` and `community` folders.\nThe official training examples are maintained by the Diffusers' core maintainers whereas the research training examples are maintained by the community.\nThis is because of the same reasons put forward in [6. Contribute a community pipeline](#6-contribute-a-community-pipeline) for official pipelines vs. community pipelines: It is not feasible for the core maintainers to maintain all possible training methods for diffusion models.\nIf the Diffusers core maintainers and the community consider a certain training paradigm to be too experimental or not popular enough, the corresponding training code should be put in the `research_projects` folder and maintained by the author.", "### 7. Contribute to training examples\n\nDiffusers examples are a collection of training scripts that reside in [examples](https://github.com/huggingface/diffusers/tree/main/examples).\n\nWe support two types of training examples:\n\n- Official training examples\n- Research training examples\n\nResearch training examples are located in [examples/research_projects](https://github.com/huggingface/diffusers/tree/main/examples/research_projects) whereas official training examples include all folders under [examples](https://github.com/huggingface/diffusers/tree/main/examples) except the `research_projects` and `community` folders.\nThe official training examples are maintained by the Diffusers' core maintainers whereas the research training examples are maintained by the community.\nThis is because of the same reasons put forward in [6. Contribute a community pipeline](#6-contribute-a-community-pipeline) for official pipelines vs. community pipelines: It is not feasible for the core maintainers to maintain all possible training methods for diffusion models.\nIf the Diffusers core maintainers and the community consider a certain training paradigm to be too experimental or not popular enough, the corresponding training code should be put in the `research_projects` folder and maintained by the author."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response accurately identifies the purpose of the \ud83e\udde8 Diffusers tutorials, which is to provide guidance on using the library's features, such as models and schedulers, and training diffusion models. The response also correctly points out where these tutorials can be found, which aligns with the reference answer's intent of introducing diffusion models and understanding library fundamentals. The response is detailed and factual, although it goes beyond the reference answer's simplicity. Based on the score rubric, the response is mostly correct, accurate, and factual, but it includes additional details not present in the reference answer."}, {"question": "What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n", "true_answer": "\"manual\"", "source_doc": "gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/using-flagging.md", "generated_answer": "The default setting for the `allow_flagging` parameter in Gradio's `Interface` is \"manual\". This means that users will see a flag button and samples will only be flagged when the button is clicked. Alternative settings are \"auto\", where every sample will be flagged automatically, and \"never\", where users will not see a flag button and no sample will be flagged.", "retrieved_docs": ["\u5728 `gradio.Interface` \u4e2d\u6709[\u56db\u4e2a\u53c2\u6570](https://gradio.app/docs/#interface-header)\u63a7\u5236\u6807\u8bb0\u7684\u5de5\u4f5c\u65b9\u5f0f\u3002\u6211\u4eec\u5c06\u8be6\u7ec6\u4ecb\u7ecd\u5b83\u4eec\u3002\n\n- `allow_flagging`\uff1a\u6b64\u53c2\u6570\u53ef\u4ee5\u8bbe\u7f6e\u4e3a `\"manual\"`\uff08\u9ed8\u8ba4\u503c\uff09\uff0c`\"auto\"` \u6216 `\"never\"`\u3002\n  - `manual`\uff1a\u7528\u6237\u5c06\u770b\u5230\u4e00\u4e2a\u6807\u8bb0\u6309\u94ae\uff0c\u53ea\u6709\u5728\u70b9\u51fb\u6309\u94ae\u65f6\u6837\u672c\u624d\u4f1a\u88ab\u6807\u8bb0\u3002\n  - `auto`\uff1a\u7528\u6237\u5c06\u4e0d\u4f1a\u770b\u5230\u4e00\u4e2a\u6807\u8bb0\u6309\u94ae\uff0c\u4f46\u6bcf\u4e2a\u6837\u672c\u90fd\u4f1a\u81ea\u52a8\u88ab\u6807\u8bb0\u3002\n  - `never`\uff1a\u7528\u6237\u5c06\u4e0d\u4f1a\u770b\u5230\u4e00\u4e2a\u6807\u8bb0\u6309\u94ae\uff0c\u5e76\u4e14\u4e0d\u4f1a\u6807\u8bb0\u4efb\u4f55\u6837\u672c\u3002\n- `flagging_options`\uff1a\u6b64\u53c2\u6570\u53ef\u4ee5\u662f `None`\uff08\u9ed8\u8ba4\u503c\uff09\u6216\u5b57\u7b26\u4e32\u5217\u8868\u3002\n  - \u5982\u679c\u662f `None`\uff0c\u5219\u7528\u6237\u53ea\u9700\u70b9\u51fb**\u6807\u8bb0**\u6309\u94ae\uff0c\u4e0d\u4f1a\u663e\u793a\u5176\u4ed6\u9009\u9879\u3002\n  - \u5982\u679c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u5219\u7528\u6237\u4f1a\u770b\u5230\u591a\u4e2a\u6309\u94ae\uff0c\u5bf9\u5e94\u4e8e\u63d0\u4f9b\u7684\u6bcf\u4e2a\u5b57\u7b26\u4e32\u3002\u4f8b\u5982\uff0c\u5982\u679c\u6b64\u53c2\u6570\u7684\u503c\u4e3a`[\" \u9519\u8bef \", \" \u6a21\u7cca \"]`\uff0c\u5219\u4f1a\u663e\u793a\u6807\u8bb0\u4e3a**\u6807\u8bb0\u4e3a\u9519\u8bef**\u548c**\u6807\u8bb0\u4e3a\u6a21\u7cca**\u7684\u6309\u94ae\u3002\u8fd9\u4ec5\u9002\u7528\u4e8e `allow_flagging` \u4e3a `\"manual\"` \u7684\u60c5\u51b5\u3002\n  - \u6240\u9009\u9009\u9879\u5c06\u4e0e\u8f93\u5165\u548c\u8f93\u51fa\u4e00\u8d77\u8bb0\u5f55\u3002\n- `flagging_dir`\uff1a\u6b64\u53c2\u6570\u63a5\u53d7\u4e00\u4e2a\u5b57\u7b26\u4e32\u3002\n  - \u5b83\u8868\u793a\u6807\u8bb0\u6570\u636e\u5b58\u50a8\u7684\u76ee\u5f55\u540d\u79f0\u3002\n- `flagging_callback`\uff1a\u6b64\u53c2\u6570\u63a5\u53d7 `FlaggingCallback` \u7c7b\u7684\u5b50\u7c7b\u7684\u5b9e\u4f8b\n  - \u4f7f\u7528\u6b64\u53c2\u6570\u5141\u8bb8\u60a8\u7f16\u5199\u5728\u70b9\u51fb\u6807\u8bb0\u6309\u94ae\u65f6\u8fd0\u884c\u7684\u81ea\u5b9a\u4e49\u4ee3\u7801\n  - \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5b83\u8bbe\u7f6e\u4e3a `gr.CSVLogger` \u7684\u4e00\u4e2a\u5b9e\u4f8b\n  - \u4e00\u4e2a\u793a\u4f8b\u662f\u5c06\u5176\u8bbe\u7f6e\u4e3a `gr.HuggingFaceDatasetSaver` \u7684\u4e00\u4e2a\u5b9e\u4f8b\uff0c\u8fd9\u6837\u60a8\u53ef\u4ee5\u5c06\u4efb\u4f55\u6807\u8bb0\u7684\u6570\u636e\u5bfc\u5165\u5230 HuggingFace \u6570\u636e\u96c6\u4e2d\uff08\u53c2\u89c1\u4e0b\u6587\uff09\u3002\n\n## \u6807\u8bb0\u7684\u6570\u636e\u4f1a\u53d1\u751f\u4ec0\u4e48\uff1f\n\n\u5728 `flagging_dir` \u53c2\u6570\u63d0\u4f9b\u7684\u76ee\u5f55\u4e2d\uff0c\u5c06\u8bb0\u5f55\u6807\u8bb0\u7684\u6570\u636e\u7684 CSV \u6587\u4ef6\u3002\n\n\u4ee5\u4e0b\u662f\u4e00\u4e2a\u793a\u4f8b\uff1a\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e86\u5d4c\u5165\u5176\u4e2d\u7684\u8ba1\u7b97\u5668\u754c\u9762\uff1a\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\"\n)\n\niface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app>", "## The **Flag** button in `gradio.Interface`\n\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.\n\nThere are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.", "iface.launch()\n```\n\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow_flagging=\"manual\"`\nbecause on Hugging Face Spaces, `allow_flagging` is set to `\"never\"` by default. Here's our demo:\n\n<gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app>\n\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo).\n\n![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)\n\nWe created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo!\n\n## Flagging with Blocks\n\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks.\n\nAt the same time, you might want to use an existing `FlaggingCallback` to avoid writing extra code.\nThis requires two steps:", "- `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.\n  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n  - `never`: users will not see a button to flag, and no sample will be flagged.\n- `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n  - If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.\n  - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n  - The chosen option is then logged along with the input and output.\n- `flagging_dir`: this parameter takes a string.\n  - It represents what to name the directory where flagged data is stored.\n- `flagging_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n  - Using this parameter allows you to write custom code that gets run when the flag button is clicked\n  - By default, this is set to an instance of `gr.CSVLogger`", "![flagging callback hf](/assets/guides/flagging-callback-hf.png)\n\n\u6211\u4eec\u521b\u5efa\u4e86 `gradio.HuggingFaceDatasetSaver` \u7c7b\uff0c\u4f46\u53ea\u8981\u5b83\u7ee7\u627f\u81ea[\u6b64\u6587\u4ef6](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py)\u4e2d\u5b9a\u4e49\u7684 `FlaggingCallback`\uff0c\u60a8\u53ef\u4ee5\u4f20\u9012\u81ea\u5df1\u7684\u81ea\u5b9a\u4e49\u7c7b\u3002\u5982\u679c\u60a8\u521b\u5efa\u4e86\u4e00\u4e2a\u5f88\u68d2\u7684\u56de\u8c03\uff0c\u8bf7\u5c06\u5176\u8d21\u732e\u7ed9\u8be5\u5b58\u50a8\u5e93\uff01\n\n## \u4f7f\u7528 Blocks \u8fdb\u884c\u6807\u8bb0\n\n\u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528 `gradio.Blocks`\uff0c\u53c8\u8be5\u600e\u4e48\u529e\u5462\uff1f\u4e00\u65b9\u9762\uff0c\u4f7f\u7528 Blocks \u60a8\u62e5\u6709\u66f4\u591a\u7684\u7075\u6d3b\u6027\n--\u60a8\u53ef\u4ee5\u7f16\u5199\u4efb\u4f55\u60a8\u60f3\u5728\u6309\u94ae\u88ab\u70b9\u51fb\u65f6\u8fd0\u884c\u7684 Python \u4ee3\u7801\uff0c\n\u5e76\u4f7f\u7528 Blocks \u4e2d\u7684\u5185\u7f6e\u4e8b\u4ef6\u5206\u914d\u5b83\u3002\n\n\u540c\u65f6\uff0c\u60a8\u53ef\u80fd\u5e0c\u671b\u4f7f\u7528\u73b0\u6709\u7684 `FlaggingCallback` \u6765\u907f\u514d\u7f16\u5199\u989d\u5916\u7684\u4ee3\u7801\u3002\n\u8fd9\u9700\u8981\u4e24\u4e2a\u6b65\u9aa4\uff1a\n\n1. \u60a8\u5fc5\u987b\u5728\u4ee3\u7801\u4e2d\u7684\u67d0\u4e2a\u4f4d\u7f6e\u8fd0\u884c\u60a8\u7684\u56de\u8c03\u7684 `.setup()` \u65b9\u6cd5\n   \u5728\u7b2c\u4e00\u6b21\u6807\u8bb0\u6570\u636e\u4e4b\u524d\n2. \u5f53\u70b9\u51fb\u6807\u8bb0\u6309\u94ae\u65f6\uff0c\u60a8\u89e6\u53d1\u56de\u8c03\u7684 `.flag()` \u65b9\u6cd5\uff0c\n   \u786e\u4fdd\u6b63\u786e\u6536\u96c6\u53c2\u6570\u5e76\u7981\u7528\u901a\u5e38\u7684\u9884\u5904\u7406\u3002\n\n\u4e0b\u9762\u662f\u4e00\u4e2a\u4f7f\u7528\u9ed8\u8ba4\u7684 `CSVLogger` \u6807\u8bb0\u56fe\u50cf\u6000\u65e7\u6ee4\u955c Blocks \u6f14\u793a\u7684\u793a\u4f8b\uff1a\ndata using the default `CSVLogger`:\n\n$code_blocks_flag\n$demo_blocks_flag\n\n## \u9690\u79c1\n\n\u91cd\u8981\u63d0\u793a\uff1a\u8bf7\u786e\u4fdd\u7528\u6237\u4e86\u89e3\u4ed6\u4eec\u63d0\u4ea4\u7684\u6570\u636e\u4f55\u65f6\u88ab\u4fdd\u5b58\u4ee5\u53ca\u60a8\u8ba1\u5212\u5982\u4f55\u5904\u7406\u5b83\u3002\u5f53\u60a8\u4f7f\u7528 `allow_flagging=auto`\uff08\u5f53\u901a\u8fc7\u6f14\u793a\u63d0\u4ea4\u7684\u6240\u6709\u6570\u636e\u90fd\u88ab\u6807\u8bb0\u65f6\uff09\uff0c\u8fd9\u4e00\u70b9\u5c24\u4e3a\u91cd\u8981\n\n### \u8fd9\u5c31\u662f\u5168\u90e8\uff01\u795d\u60a8\u5efa\u8bbe\u6109\u5feb :)", "Gradio Demo: blocks_flag\n\n\n```\n!pip install -q gradio numpy\n```\n\n\n```\nimport numpy as np\nimport gradio as gr\n\ndef sepia(input_img, strength):\n    sepia_filter = strength * np.array(\n        [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]]\n    ) + (1-strength) * np.identity(3)\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ncallback = gr.CSVLogger()\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            img_input = gr.Image()\n            strength = gr.Slider(0, 1, 0.5)\n        img_output = gr.Image()\n    with gr.Row():\n        btn = gr.Button(\"Flag\")\n        \n    # This needs to be called at some point prior to the first call to callback.flag()\n    callback.setup([img_input, strength, img_output], \"flagged_data_points\")\n\n    img_input.change(sepia, [img_input, strength], img_output)\n    strength.change(sepia, [img_input, strength], img_output)\n    \n    # We can choose which components to flag -- in this case, we'll flag all of them\n    btn.click(lambda *args: callback.flag(args), [img_input, strength, img_output], None, preprocess=False)\n\nif __name__ == \"__main__\":\n    demo.launch()\n\n```", "\u4f7f\u7528\u6807\u8bb0\n\n\u76f8\u5173\u7a7a\u95f4\uff1ahttps://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\n\u6807\u7b7e\uff1a\u6807\u8bb0\uff0c\u6570\u636e\n\n## \u7b80\u4ecb\n\n\u5f53\u60a8\u6f14\u793a\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\uff0c\u60a8\u53ef\u80fd\u5e0c\u671b\u6536\u96c6\u8bd5\u7528\u6a21\u578b\u7684\u7528\u6237\u7684\u6570\u636e\uff0c\u7279\u522b\u662f\u6a21\u578b\u884c\u4e3a\u4e0d\u5982\u9884\u671f\u7684\u6570\u636e\u70b9\u3002\u6355\u83b7\u8fd9\u4e9b\u201c\u56f0\u96be\u201d\u6570\u636e\u70b9\u662f\u6709\u4ef7\u503c\u7684\uff0c\u56e0\u4e3a\u5b83\u5141\u8bb8\u60a8\u6539\u8fdb\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5e76\u4f7f\u5176\u66f4\u53ef\u9760\u548c\u7a33\u5065\u3002\n\nGradio \u901a\u8fc7\u5728\u6bcf\u4e2a\u201c\u754c\u9762\u201d\u4e2d\u5305\u542b\u4e00\u4e2a**\u6807\u8bb0**\u6309\u94ae\u6765\u7b80\u5316\u8fd9\u4e9b\u6570\u636e\u7684\u6536\u96c6\u3002\u8fd9\u4f7f\u5f97\u7528\u6237\u6216\u6d4b\u8bd5\u4eba\u5458\u53ef\u4ee5\u8f7b\u677e\u5730\u5c06\u6570\u636e\u53d1\u9001\u56de\u8fd0\u884c\u6f14\u793a\u7684\u673a\u5668\u3002\u6837\u672c\u4f1a\u4fdd\u5b58\u5728\u4e00\u4e2a CSV \u65e5\u5fd7\u6587\u4ef6\u4e2d\uff08\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff09\u3002\u5982\u679c\u6f14\u793a\u6d89\u53ca\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u6216\u5176\u4ed6\u7c7b\u578b\u7684\u6587\u4ef6\uff0c\u5219\u8fd9\u4e9b\u6587\u4ef6\u4f1a\u5355\u72ec\u4fdd\u5b58\u5728\u4e00\u4e2a\u5e76\u884c\u76ee\u5f55\u4e2d\uff0c\u5e76\u4e14\u8fd9\u4e9b\u6587\u4ef6\u7684\u8def\u5f84\u4f1a\u4fdd\u5b58\u5728 CSV \u6587\u4ef6\u4e2d\u3002\n\n## \u5728 `gradio.Interface` \u4e2d\u4f7f\u7528**\u6807\u8bb0**\u6309\u94ae\n\n\u4f7f\u7528 Gradio \u7684 `Interface` \u8fdb\u884c\u6807\u8bb0\u7279\u522b\u7b80\u5355\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5728\u8f93\u51fa\u7ec4\u4ef6\u4e0b\u65b9\u6709\u4e00\u4e2a\u6807\u8bb0\u4e3a**\u6807\u8bb0**\u7684\u6309\u94ae\u3002\u5f53\u7528\u6237\u6d4b\u8bd5\u60a8\u7684\u6a21\u578b\u65f6\uff0c\u5982\u679c\u770b\u5230\u6709\u8da3\u7684\u8f93\u51fa\uff0c\u4ed6\u4eec\u53ef\u4ee5\u70b9\u51fb\u6807\u8bb0\u6309\u94ae\u5c06\u8f93\u5165\u548c\u8f93\u51fa\u6570\u636e\u53d1\u9001\u56de\u8fd0\u884c\u6f14\u793a\u7684\u673a\u5668\u3002\u6837\u672c\u4f1a\u4fdd\u5b58\u5728\u4e00\u4e2a CSV \u65e5\u5fd7\u6587\u4ef6\u4e2d\uff08\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff09\u3002\u5982\u679c\u6f14\u793a\u6d89\u53ca\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u6216\u5176\u4ed6\u7c7b\u578b\u7684\u6587\u4ef6\uff0c\u5219\u8fd9\u4e9b\u6587\u4ef6\u4f1a\u5355\u72ec\u4fdd\u5b58\u5728\u4e00\u4e2a\u5e76\u884c\u76ee\u5f55\u4e2d\uff0c\u5e76\u4e14\u8fd9\u4e9b\u6587\u4ef6\u7684\u8def\u5f84\u4f1a\u4fdd\u5b58\u5728 CSV \u6587\u4ef6\u4e2d\u3002\n\n\u5728 `gradio.Interface` \u4e2d\u6709[\u56db\u4e2a\u53c2\u6570](https://gradio.app/docs/#interface-header)\u63a7\u5236\u6807\u8bb0\u7684\u5de5\u4f5c\u65b9\u5f0f\u3002\u6211\u4eec\u5c06\u8be6\u7ec6\u4ecb\u7ecd\u5b83\u4eec\u3002"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer and includes additional context about the functionality of the `allow_flagging` parameter and its other possible settings, which demonstrates a comprehensive understanding of the feature."}, {"question": "Where can the full code for the Stable Diffusion demo be found?\n", "true_answer": "https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "source_doc": "gradio-app/gradio/blob/main/demo/stable-diffusion/DESCRIPTION.md", "generated_answer": "The full code for the Stable Diffusion demo can be found at https://hf.co/spaces/stabilityai/stable-diffusion/tree/main, as mentioned in the extracted document with the note \"This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\".", "retrieved_docs": ["ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "The original codebase can be found at [Xiang-cd/DiffEdit-stable-diffusion](https://github.com/Xiang-cd/DiffEdit-stable-diffusion), and you can try it out in this [demo](https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html).\n\nThis pipeline was contributed by [clarencechen](https://github.com/clarencechen). \u2764\ufe0f\n\n## Tips", "Gradio Demo: stable-diffusion\n### Note: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n        \n\n\n```\n!pip install -q gradio diffusers transformers nvidia-ml-py3 ftfy torch\n```\n\n\n```\nimport gradio as gr\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport os\n\nauth_token = os.getenv(\"auth_token\")\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cpu\"\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id, use_auth_token=auth_token, revision=\"fp16\", torch_dtype=torch.float16\n)\npipe = pipe.to(device)\n\n\ndef infer(prompt, samples, steps, scale, seed):\n    generator = torch.Generator(device=device).manual_seed(seed)\n    images_list = pipe(\n        [prompt] * samples,\n        num_inference_steps=steps,\n        guidance_scale=scale,\n        generator=generator,\n    )\n    images = []\n    safe_image = Image.open(r\"unsafe.png\")\n    for i, image in enumerate(images_list[\"sample\"]):\n        if images_list[\"nsfw_content_detected\"][i]:\n            images.append(safe_image)\n        else:\n            images.append(image)\n    return images\n\n\nblock = gr.Blocks()", "And that's it!\n\nOnce you have created your tool, open a pull request to the `gradio_tools` repo! We welcome all contributions.\n\n## Example tool - Stable Diffusion\n\nHere is the code for the StableDiffusion tool as an example:\n\n```python\nfrom gradio_tool import GradioTool\nimport os\n\nclass StableDiffusionTool(GradioTool):\n    \"\"\"Tool for calling stable diffusion from llm\"\"\"\n\n    def __init__(\n        self,\n        name=\"StableDiffusion\",\n        description=(\n            \"An image generator. Use this to generate images based on \"\n            \"text input. Input should be a description of what the image should \"\n            \"look like. The output will be a path to an image file.\"\n        ),\n        src=\"gradio-client-demos/stable-diffusion\",\n        hf_token=None,\n    ) -> None:\n        super().__init__(name, description, src, hf_token)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(query, \"\", 9, fn_index=1)\n\n    def postprocess(self, output: str) -> str:\n        return [os.path.join(output, i) for i in os.listdir(output) if not i.endswith(\"json\")][0]\n\n    def _block_input(self, gr) -> \"gr.components.Component\":\n        return gr.Textbox()\n\n    def _block_output(self, gr) -> \"gr.components.Component\":\n        return gr.Image()\n```\n\nSome notes on this implementation:", ". Code is available at https://github.com/CompVis/latent-diffusion.*", "--\ntitle: Stable Diffusion with \ud83e\udde8 Diffusers\nthumbnail: /blog/assets/98_stable_diffusion/thumbnail.png\nauthors:\n- user: valhalla\n- user: pcuenq\n- user: natolambert\n- user: patrickvonplaten\n---\n\n# Stable Diffusion with \ud83e\udde8 Diffusers\n\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n# **Stable Diffusion** \ud83c\udfa8 \n*...using \ud83e\udde8 Diffusers*\n\nStable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). \nIt is trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database.\n*LAION-5B* is the largest, freely accessible multi-modal dataset that currently exists.\n\nIn this post, we want to show how to use Stable Diffusion with the [\ud83e\udde8 Diffusers library](https://github.com/huggingface/diffusers), explain how the model works and finally dive a bit deeper into how `diffusers` allows\none to customize the image generation pipeline.", "And much more! Check out the repo.\n\n### Reusable Seeds\n\nOne of the most impressive early demos of Stable Diffusion was the reuse of seeds to tweak images. The idea is to use the seed of an image of interest to generate a new image, with a different prompt. This yields some cool results! Check out the [Colab](https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb) \n\n\n## Thanks for reading!\n\nI hope you enjoy reading this! Remember to give a Star in our [GitHub Repository](https://github.com/huggingface/diffusers) and join the [Hugging Face Discord Server](https://hf.co/join/discord), where we have a category of channels just for Diffusion models. Over there the latest news in the library are shared! \n\nFeel free to open issues with feature requests and bug reports! Everything that has been achieved couldn't have been done without such an amazing community."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer by providing the exact URL where the full code for the Stable Diffusion demo can be found. The additional context given in the response about the source of the information does not detract from the accuracy of the URL provided."}, {"question": "What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n", "true_answer": "Fourier transform", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md", "generated_answer": "The FNet model replaces the self-attention sublayer in a BERT model with simple linear transformations that \"mix\" input tokens, along with standard nonlinearities in feed-forward layers. These linear mixers, along with the rest of the self-attention heads, form a new mixed attention block that is more efficient at both global and local context learning. This transformation, called FNet, has been shown to achieve similar accuracy to BERT counterparts on the GLUE", "retrieved_docs": ["*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.*\n\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f_net).\n\n## Usage tips", "The abstract from the paper is the following:\n\n*Pre-trained language models like BERT and its variants have recently achieved impressive performance in various\nnatural language understanding tasks. However, BERT heavily relies on the global self-attention block and thus suffers\nlarge memory footprint and computation cost. Although all its attention heads query on the whole input sequence for\ngenerating the attention map from a global perspective, we observe some heads only need to learn local dependencies,\nwhich means the existence of computation redundancy. We therefore propose a novel span-based dynamic convolution to\nreplace these self-attention heads to directly model local dependencies. The novel convolution heads, together with the\nrest self-attention heads, form a new mixed attention block that is more efficient at both global and local context\nlearning. We equip BERT with this mixed attention design and build a ConvBERT model. Experiments have shown that\nConvBERT significantly outperforms BERT and its variants in various downstream tasks, with lower training cost and\nfewer model parameters. Remarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than ELECTRAbase, while\nusing less than 1/4 training cost. Code and pre-trained models will be released.*\n\nThis model was contributed by [abhishek](https://huggingface.co/abhishek). The original implementation can be found\nhere: https://github.com/yitu-opensource/ConvBert\n\n## Usage tips", "## Overview\n\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n\nThe abstract from the paper is the following:\n\n*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\narchitecture modifications.*", "- Splitting the embedding matrix into two smaller matrices.\n- Using repeating layers split among groups.\n\nThe abstract from the paper is the following:\n\n*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.*\n\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n\n## Usage tips", "Great, now that we have gotten a general overview of how\n*transformer-based* encoder-decoder models work, we can dive deeper into\nboth the encoder and decoder part of the model. More specifically, we\nwill see exactly how the encoder makes use of the self-attention layer\nto yield a sequence of context-dependent vector encodings and how\nself-attention layers allow for efficient parallelization. Then, we will\nexplain in detail how the self-attention layer works in the decoder\nmodel and how the decoder is conditioned on the encoder\\'s output with\n*cross-attention* layers to define the conditional distribution\n\\\\(p_{\\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{\\overline{X}}_{1:n})\\\\).\nAlong, the way it will become obvious how transformer-based\nencoder-decoder models solve the long-range dependencies problem of\nRNN-based encoder-decoder models.\n\n------------------------------------------------------------------------\n\n\\\\({}^1\\\\) In the case of `\"Helsinki-NLP/opus-mt-en-de\"`, the decoding\nparameters can be accessed\n[here](https://s3.amazonaws.com/models.huggingface.co/bert/Helsinki-NLP/opus-mt-en-de/config.json),\nwhere we can see that model applies beam search with `num_beams=6`.\n\n## **Encoder**\n\nAs mentioned in the previous section, the *transformer-based* encoder\nmaps the input sequence to a contextualized encoding sequence:\n\n$$ f_{\\theta_{\\text{enc}}}: \\mathbf{X}_{1:n} \\to \\mathbf{\\overline{X}}_{1:n}. $$", "#### Main findings\n\n* The authors proposed the dilated windowed self-attention (Figure c) and showed that it yields better results on language modeling compared to just windowed/sparse self-attention (Figure b). The window sizes are increased through the layers. This pattern further outperforms previous architectures (such as Transformer-XL, or adaptive span attention) on downstream benchmarks.\n* Global attention allows the information to flow through the whole sequence and applying the global attention to task-motivated tokens (such as the tokens of the question in QA, CLS token for sentence classification) leads to stronger performance on downstream tasks. Using this global pattern, Longformer can be successfully applied to document-level NLP tasks in the transfer learning setting.\n* Standard pre-trained models can be adapted to long-range inputs by simply replacing the standard self-attention with the long-range self-attention proposed in this paper and then fine-tuning on the downstream task. This avoids costly pre-training specific to long-range inputs.\n\n#### Follow-up questions", "Alright, this sounds quite complicated. Let\\'s illustrate the\nbi-directional self-attention layer for one of the query vectors of our\nexample above. For simplicity, it is assumed that our exemplary\n*transformer-based* decoder uses only a single attention head\n`config.num_heads = 1` and that no normalization is applied.\n\n![texte du\nlien](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/encoder_detail.png)"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response incorrectly describes the transformation used in the FNet model. It mentions \"simple linear transformations\" and \"mixed attention block,\" which are not accurate representations of the FNet transformation. The correct answer is the Fourier transform, which is not stated in the response. Therefore, the response is not factual and does not align with the reference answer."}, {"question": "What type of test should typically accompany a bug fix in Gradio's testing strategy?\n", "true_answer": "Dynamic code test", "source_doc": "gradio-app/gradio/blob/main/test-strategy.md", "generated_answer": "A dynamic code test that fails before the fix and passes afterwards should typically accompany a bug fix in Gradio's testing strategy, following the test coverage objectives outlined in the provided context. This type of test ensures that the fix has resolved the issue and prevents regressions. However, there are always exceptions, and the decision to ignore this rule should be thought through carefully.", "retrieved_docs": ["We should not focus on code coverage but on test coverage following the below criteria:\n\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n\n## Types of testing\n\nOur tests will broadly fall into one of three categories:\n\n- Static Quality checks\n- Dynamic 'Code' tests\n- Dynamic Functional tests\n\n### Static Quality checks\n\nStatic quality checks are generally very fast to run and do not require building the code base. They also provide the least value. These tests would be things like linting, typechecking, and formatting.", "gradio_test\n\n## 0.3.3\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d), [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:\n  - @gradio/statustracker@0.4.3\n  - @gradio/atoms@0.4.1\n\n## 0.3.2\n\n### Patch Changes\n\n- Updated dependencies [[`4d1cbbc`](https://github.com/gradio-app/gradio/commit/4d1cbbcf30833ef1de2d2d2710c7492a379a9a00)]:\n  - @gradio/atoms@0.4.0\n  - @gradio/statustracker@0.4.2\n\n## 0.3.1\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.3.1\n  - @gradio/statustracker@0.4.1\n\n## 0.3.0\n\n### Features\n\n- [#6532](https://github.com/gradio-app/gradio/pull/6532) [`96290d304`](https://github.com/gradio-app/gradio/commit/96290d304a61064b52c10a54b2feeb09ca007542) - tweak deps. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.2.3\n\n### Patch Changes\n\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8)]:\n  - @gradio/atoms@0.3.0\n  - @gradio/statustracker@0.4.0\n\n## 0.2.2\n\n### Patch Changes\n\n- Updated dependencies [[`f816136a0`](https://github.com/gradio-app/gradio/commit/f816136a039fa6011be9c4fb14f573e4050a681a)]:\n  - @gradio/atoms@0.2.2\n  - @gradio/statustracker@0.3.2\n\n## 0.2.1\n\n### Patch Changes", "Test Strategy\n\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to.\n\nThis document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).\n\n## Objectives\n\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n\n1. Ensure that the Gradio library functions as we expect it to.\n2. Enable the maintenance team to quickly identify both the presence and source of defects.\n3. Prevent regressions, i.e. if we fix something it should stay fixed.\n4. Improve the quality of the codebase in order to ease maintenance efforts.\n5. Reduce the amount of manual testing required.\n\n## Scope\n\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas.\n\nWe should not focus on code coverage but on test coverage following the below criteria:", "- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should _typically_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule.", "### Bug Fixes:\n\n- Fixed bug where an error opening an audio file led to a crash by [@FelixDombek](https://github.com/FelixDombek) in [PR 2898](https://github.com/gradio-app/gradio/pull/2898)\n- Fixed bug where setting `default_enabled=False` made it so that the entire queue did not start by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2876](https://github.com/gradio-app/gradio/pull/2876)\n- Fixed bug where csv preview for DataFrame examples would show filename instead of file contents by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2877](https://github.com/gradio-app/gradio/pull/2877)\n- Fixed bug where an error raised after yielding iterative output would not be displayed in the browser by\n  [@JaySmithWpg](https://github.com/JaySmithWpg) in [PR 2889](https://github.com/gradio-app/gradio/pull/2889)\n- Fixed bug in `blocks_style` demo that was preventing it from launching by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2890](https://github.com/gradio-app/gradio/pull/2890)\n- Fixed bug where files could not be downloaded by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2926](https://github.com/gradio-app/gradio/pull/2926)\n- Fixed bug where cached examples were not displaying properly by [@a-rogalska](https://github.com/a-rogalska) in [PR 2974](https://github.com/gradio-app/gradio/pull/2974)\n\n### Documentation Changes:", "### Bug Fixes:\n\n- Fixed bug where an error opening an audio file led to a crash by [@FelixDombek](https://github.com/FelixDombek) in [PR 2898](https://github.com/gradio-app/gradio/pull/2898)\n- Fixed bug where setting `default_enabled=False` made it so that the entire queue did not start by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2876](https://github.com/gradio-app/gradio/pull/2876)\n- Fixed bug where csv preview for DataFrame examples would show filename instead of file contents by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2877](https://github.com/gradio-app/gradio/pull/2877)\n- Fixed bug where an error raised after yielding iterative output would not be displayed in the browser by\n  [@JaySmithWpg](https://github.com/JaySmithWpg) in [PR 2889](https://github.com/gradio-app/gradio/pull/2889)\n- Fixed bug in `blocks_style` demo that was preventing it from launching by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2890](https://github.com/gradio-app/gradio/pull/2890)\n- Fixed bug where files could not be downloaded by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2926](https://github.com/gradio-app/gradio/pull/2926)\n- Fixed bug where cached examples were not displaying properly by [@a-rogalska](https://github.com/a-rogalska) in [PR 2974](https://github.com/gradio-app/gradio/pull/2974)\n\n### Documentation Changes:", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.4.0-beta.7\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files. Thanks [@pngwn](https://github.com/pngwn)!\n- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-app/gradio/commit/13ed8a485d5e31d7d75af87fe8654b661edcca93) - V4: Use beta release versions for '@gradio' packages. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n\n## 0.5.3\n\n### Fixes\n\n- [#5827](https://github.com/gradio-app/gradio/pull/5827) [`48e09ee88`](https://github.com/gradio-app/gradio/commit/48e09ee88799efa38a5cc9b1b61e462f72ec6093) - Quick fix: Chatbot change event. Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)!\n\n## 0.5.2\n\n### Patch Changes\n\n- Updated dependencies [[`e4a307ed6`](https://github.com/gradio-app/gradio/commit/e4a307ed6cde3bbdf4ff2f17655739addeec941e)]:\n  - @gradio/theme@0.2.0\n  - @gradio/utils@0.1.2\n  - @gradio/atoms@0.1.4\n  - @gradio/markdown@0.3.1\n  - @gradio/statustracker@0.2.2\n  - @gradio/upload@0.3.2\n\n## 0.5.1\n\n### Fixes"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies that a dynamic code test should accompany a bug fix in Gradio's testing strategy, which aligns with the reference answer. The additional explanation provided about the test failing before the fix and passing afterwards, as well as the mention of preventing regressions, supports the accuracy and factual nature of the response. The response goes beyond the reference answer by discussing exceptions to the rule, but this does not detract from the correctness of the answer."}, {"question": "How can you force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate?\n", "true_answer": "By passing `fp16=True` to the Accelerator init.", "source_doc": "huggingface/blog/blob/main/accelerate-library.md", "generated_answer": "To force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate, you can pass `fp16=True` as an argument to the `Accelerator` initialization. This will enable automatic mixed precision training with the pytorch-like AMP way using fp16 (float16) by default. You can also enable/disable this mode explicitly by adding a `fp16` configuration section to your training script with the desired", "retrieved_docs": ["```bash\naccelerate launch --mixed_precision=\"fp16\" train_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --dataset_name=$DATASET_ID \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=256 \\\n    --random_flip \\\n    --train_batch_size=4 \\\n    --gradient_accumulation_steps=4 \\\n    --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 \\\n    --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 \\\n    --max_grad_norm=1 \\\n    --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --mixed_precision=fp16 \\\n    --seed=42 \\\n    --push_to_hub\n```\n\nAfter training is finished, you can use your new InstructPix2Pix for inference:\n\n```py\nimport PIL\nimport requests\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline\nfrom diffusers.utils import load_image\n\npipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\"your_cool_model\", torch_dtype=torch.float16).to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\n\nimage = load_image(\"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/test_pix2pix_4.png\")\nprompt = \"add some ducks to the lake\"\nnum_inference_steps = 20\nimage_guidance_scale = 1.5\nguidance_scale = 10", "```bash\naccelerate launch --mixed_precision=\"fp16\" train_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --dataset_name=$DATASET_ID \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=256 --random_flip \\\n    --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 --max_grad_norm=1 --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --mixed_precision=fp16 \\\n    --val_image_url=\"https://hf.co/datasets/diffusers/diffusers-images-docs/resolve/main/mountain.png\" \\\n    --validation_prompt=\"make the mountains snowy\" \\\n    --seed=42 \\\n    --report_to=wandb \\\n    --push_to_hub\n ```\n\n We recommend this type of validation as it can be useful for model debugging. Note that you need `wandb` installed to use this. You can install `wandb` by running `pip install wandb`. \n\n [Here](https://wandb.ai/sayakpaul/instruct-pix2pix/runs/ctr3kovq), you can find an example training run that includes some validation samples and the training hyperparameters.\n\n ***Note: In the original paper, the authors observed that even when the model is trained with an image resolution of 256x256, it generalizes well to bigger resolutions such as 512x512. This is likely because of the larger dataset they used during training.***\n\n ## Training with multiple GPUs", "accelerate launch --mixed_precision=\"fp16\"  train_text_to_image.py --ort \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$dataset_name \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --output_dir=\"sd-pokemon-model\" \n```\n<!-- accelerate_snippet_end -->\n\n\nTo run on your own training files prepare the dataset according to the format required by `datasets`, you can find the instructions for how to do that in this [document](https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder-with-metadata).\nIf you wish to use custom loading logic, you should modify the script, we have left pointers for that in the training script.\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport TRAIN_DIR=\"path_to_your_dataset\"\n\naccelerate launch --mixed_precision=\"fp16\" train_text_to_image.py --ort \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$TRAIN_DIR \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --output_dir=\"sd-pokemon-model\"\n```", "You can also configure this mode explicitly:\n\n```json\n{\n    \"amp\": {\n        \"enabled\": true,\n        \"opt_level\": \"O1\"\n    }\n}\n```\n\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration.\n\nHere is the [documentation](https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options).\n\n\n\n<a id='deepspeed-bs'></a>\n\n### Batch Size\n\nTo configure batch size, use:\n\n```json\n{\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\"\n}\n```\n\nand the [`Trainer`] will automatically set `train_micro_batch_size_per_gpu` to the value of\n`args.per_device_train_batch_size` and `train_batch_size` to `args.world_size * args.per_device_train_batch_size * args.gradient_accumulation_steps`.\n\nYou can also set the values explicitly:\n\n```json\n{\n    \"train_batch_size\": 12,\n    \"train_micro_batch_size_per_gpu\": 4\n}\n```\n\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration.\n\n\n\n<a id='deepspeed-grad-acc'></a>\n\n### Gradient Accumulation\n\nTo configure gradient accumulation set:\n\n```json\n{\n    \"gradient_accumulation_steps\": \"auto\"\n}\n```\n\nand the [`Trainer`] will automatically set it to the value of `args.gradient_accumulation_steps`.\n\nYou can also set the value explicitly:\n\n```json\n{\n    \"gradient_accumulation_steps\": 3\n}\n```\n\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration.", "<a id='deepspeed-amp'></a>\n\n### Automatic Mixed Precision\n\nYou can use automatic mixed precision with either a pytorch-like AMP way or the apex-like way:\n\n### fp16\n\nTo configure pytorch AMP-like mode with fp16 (float16) set:\n\n```json\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    }\n}\n```\n\nand the [`Trainer`] will automatically enable or disable it based on the value of\n`args.fp16_backend`. The rest of config values are up to you.\n\nThis mode gets enabled when `--fp16 --fp16_backend amp` or `--fp16_full_eval` command line args are passed.\n\nYou can also enable/disable this mode explicitly:\n\n```json\n{\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    }\n}\n```\n\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration.\n\nHere is the [documentation](https://www.deepspeed.ai/docs/config-json/#fp16-training-options).\n\n### bf16\n\nIf bf16 (bfloat16) is desired instead of fp16 then the following configuration section is to be used:\n\n```json\n{\n    \"bf16\": {\n        \"enabled\": \"auto\"\n    }\n}\n```\n\nbf16 has the same dynamic range as fp32 and thus doesn't require loss scaling.", "loss.backward()\n\n          optimizer.step()\n```\n\nThese changes will make your training script work for multiple GPUs, but your script will then stop working on CPU or one GPU (unless you start adding if statements everywhere). Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of \ud83e\udd17 Accelerate is:\n- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible.\n- to have the same functions work for any distributed setup, so only have to learn one API.\n\n### How does it work?\n\nTo see how the library works in practice, let's have a look at each line of code we need to add to a training loop.\n\n```python\naccelerator = Accelerator()\n```\n\nOn top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script.\n\n```python\nmodel, optim, data = accelerator.prepare(model, optim, data)\n```\n\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torch.nn.Module`), optimizers (`torch.optim.Optimizer`) and dataloaders (`torch.data.dataloader.DataLoader`).\n\n#### Model", "accelerate launch --mixed_precision=\"fp16\"  train_text_to_image.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --output_dir=\"sd-pokemon-model\"\n```\n<!-- accelerate_snippet_end -->\n\n\nTo run on your own training files prepare the dataset according to the format required by `datasets`, you can find the instructions for how to do that in this [document](https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder-with-metadata).\nIf you wish to use custom loading logic, you should modify the script, we have left pointers for that in the training script.\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport TRAIN_DIR=\"path_to_your_dataset\"\n\naccelerate launch --mixed_precision=\"fp16\" train_text_to_image.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$TRAIN_DIR \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --output_dir=\"sd-pokemon-model\"\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that mixed precision training can be forced by passing `fp16=True` to the Accelerator initialization in \ud83e\udd17 Accelerate. The response also adds additional context by explaining that this enables automatic mixed precision training with the pytorch-like AMP way using fp16, which is factually correct and provides a deeper understanding without deviating from the accuracy of the answer."}, {"question": "What is the range of parameters for the LLaMA models?\n", "true_answer": "7B to 65B parameters", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/llama.md", "generated_answer": "The range of parameters for the LLaMA models is 4 to 70 billion, with models of 4 bits and 64 ranks, including LLAMA-2-7b, LLAMA-2-13b, and LLAMA-2-70b, as well as Mistral models with 4 bits and 32 or 64 ranks. The Mistral models have the same number of bits and ranks as the LLaama models", "retrieved_docs": ["```python\nfrom transformers import TrainingArguments, Trainer\n\nllama_model = llama_model.cuda()\n\nlr = 1e-4\nbatch_size = 8\nnum_epochs = 5\ntraining_args = TrainingArguments(\n    output_dir=\"llama-lora-token-classification\",\n    learning_rate=lr,\n    lr_scheduler_type= \"constant\",\n    warmup_ratio= 0.1,\n    max_grad_norm= 0.3,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.001,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"wandb\",\n    fp16=True,\n    gradient_checkpointing=True,\n)\n\n\n\nllama_trainer = WeightedCELossTrainer(\n    model=llama_model,\n    args=training_args,\n    train_dataset=llama_tokenized_datasets['train'],\n    eval_dataset=llama_tokenized_datasets[\"val\"],\n    data_collator=llama_data_collator,\n    compute_metrics=compute_metrics\n)\n```\n\n\n\n\n## Hyperparameter Tuning\n\nWe have used Wandb Sweep API to run hyperparameter tunning with Bayesian search strategy (30 runs). The hyperparameters tuned are the following.", "This model was contributed by [ArthurZucker](https://huggingface.co/ArthurZ). The original code of the authors can be found [here](https://github.com/facebookresearch/llama).\n\n## Usage tips and examples\n\n<Tip warning={true}>\n\nThe `Llama2` family models, on which Code Llama is based, were trained using `bfloat16`, but the original inference uses `float16`. Let's look at the different precisions:\n\n* `float32`: PyTorch convention on model initialization is to load models in `float32`, no matter with which `dtype` the model weights were stored. `transformers` also follows this convention for consistency with PyTorch. This will be picked by default. If you want the `AutoModel` API to cast the load the checkpoints with the storage weights type, you must specify `torch_dtype=\"auto\"`, e.g. `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`.\n* `bfloat16`: Code Llama was trained with this precision, so we recommend using it for further training or fine-tuning.\n* `float16`: We recommend running inference using this precision, as it's usually faster than `bfloat16`, and evaluation metrics show no discernible degradation with respect to `bfloat16`. You can also run inference using `bfloat16`, and we recommend you check inference results with both `float16` and `bfloat16` after fine-tuning.", "## Appendix: Off-the-shelf Model List\n| Model Name  | Bits | Ranks |\n| ----------- | ---- | ----- |\n| LLAMA-2-7b  | 4    | 64    |\n| LLAMA-2-13b | 4    | 64    |\n| LLAMA-2-70b | 4    | 64    |\n| Mistral     | 4    | 64    |\n| Mistral     | 4    | 32    |\n| BART-large  | 4    | 8     |\n| BART-large  | 4    | 16    |\n| BART-large  | 4    | 32    |\n| BART-large  | 2    | 8     |", "| Model type                 | num cores | batch_size | Hugging Face Hub model                    |\n|----------------------------|-----------|------------|-------------------------------------------|\n| Llama2 7B - B (budget)     | 2         | 1          |[aws-neuron/Llama-2-7b-hf-neuron-budget](https://huggingface.co/aws-neuron/Llama-2-7b-hf-neuron-budget)          |\n| Llama2 7B - L (latency)    | 24        | 1          |[aws-neuron/Llama-2-7b-hf-neuron-latency](https://huggingface.co/aws-neuron/Llama-2-7b-hf-neuron-latency)        |\n| Llama2 7B - T (throughput) | 24        | 4          |[aws-neuron/Llama-2-7b-hf-neuron-throughput](https://huggingface.co/aws-neuron/Llama-2-7b-hf-neuron-throughput)  |\n| Llama2 13B - L (latency)   | 24        | 1          |[aws-neuron/Llama-2-13b-hf-neuron-latency](https://huggingface.co/aws-neuron/Llama-2-13b-hf-neuron-latency)      |\n| Llama2 13B - T (throughput)| 24        | 4          |[aws-neuron/Llama-2-13b-hf-neuron-throughput](https://huggingface.co/aws-neuron/Llama-2-13b-hf-neuron-throughput)|\n\n*Note: all models are compiled with a maximum sequence length of 2048.*\n\nThe `llama2 7B` \"budget\" model is meant to be deployed on `inf2.xlarge` instance that has only one neuron device, and enough `cpu` memory to load the model.\n\nAll other models are compiled to use the full extent of cores available on the `inf2.48xlarge` instance.", "**Disclaimer:** due to LLaMA's license, we release only the adapter weights for this and the model checkpoints in the following sections. You can apply for access to the base model's weights by filling out Meta AI's [form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform) and then converting them to the \ud83e\udd17 Transformers format by running this [script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py). Note that you'll also need to install \ud83e\udd17 Transformers from source until the `v4.28` is released.\n\nNow that we have fine-tuned the model for the task, we are ready to train a reward model.\n\n## Reward modeling and human preferences\n\nIn principle, we could fine-tune the model using RLHF directly with the human annotations. However, this would require us to send some samples to humans for rating after each optimization iteration. This is expensive and slow due to the number of training samples needed for convergence and the inherent latency of human reading and annotator speed.", "The first model family in this series was the [LLaMA](https://huggingface.co/papers/2302.13971) family, released by Meta AI. The explicit objective of the researchers was to train a set of models of various sizes with the best possible performances for a given computing budget. For one of the first times, the research team explicitly decided to consider not only the training budget but also the inference cost (for a given performance objective, how much does it cost to run inference with the model). In this perspective, they decided to train smaller models on even more data and for more steps than was usually done, thereby reaching higher performances at a smaller model size (the trade-off being training compute efficiency). The biggest model in the Llama 1 family is a 65B parameters model trained on 1.4T tokens, while the smaller models (resp. 6 and 13B parameters) were trained on 1T tokens. The small 13B LLaMA model outperformed GPT-3 on most benchmarks, and the biggest LLaMA model was state of the art when it came out. The weights were released with a non-commercial license though, limiting the adoption by the community.", "## Efficient training strategies\n\nEven training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer) in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB` just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn\u2019t train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you\u2019ll run out sooner or later.\n\nAnother option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit. \n\n![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/lora-animated.gif)   \n*Low-Rank Adaptation of linear layers: extra parameters (in orange) are added next to the frozen layer (in blue), and the resulting encoded hidden states are added together with the hidden states of the frozen layer.*"], "eval_score_GPT4": "2", "eval_feedback_GPT4": "Feedback: The response incorrectly states the range of parameters for the LLaMA models as 4 to 70 billion, which does not align with the reference answer of 7B to 65B parameters. Additionally, the response includes details about bits and ranks, which are not relevant to the question about the range of parameters. The inclusion of \"LLAMA-2-7b, LLAMA-2-13b, and LLAMA-2-70b\" models is somewhat correct as it mentions specific models, but the naming convention used is incorrect (should be LLaMA, not LLAMA-2), and the numbers do not match the reference answer. The mention of Mistral models is also incorrect and not relevant to the question about LLaMA models. Therefore, the response is mostly incorrect and inaccurate."}, {"question": "What is the purpose of tokenizers in the NLP pipeline?\n", "true_answer": "To translate text into data that can be processed by the model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter2/4.mdx", "generated_answer": "Tokenizers in the NLP pipeline serve the purpose of breaking down a given text into a sequence of smaller units called tokens. These tokens can then be fed into a machine learning model for processing and analysis. Tokenizers help in reducing the dimensionality of the input text, making it easier for the model to understand and process. They also enable tasks like named entity recognition, part-of-speech tagging, and machine translation by allowing the model to match specific tokens with their corresponding meanings or", "retrieved_docs": ["The tokenizer returns a dictionary with three items:\n\n- `input_ids`: the numbers representing the tokens in the text.\n- `token_type_ids`: indicates which sequence a token belongs to if there is more than one sequence.\n- `attention_mask`: indicates whether a token should be masked or not.\n\nThese values are actually the model inputs.\n\n**3**. The fastest way to tokenize your entire dataset is to use the [`~Dataset.map`] function. This function speeds up tokenization by applying the tokenizer to batches of examples instead of individual examples. Set the `batched` parameter to `True`:\n\n```py\n>>> def tokenization(example):\n...     return tokenizer(example[\"text\"])\n\n>>> dataset = dataset.map(tokenization, batched=True)\n```\n\n**4**. Set the format of your dataset to be compatible with your machine learning framework:\n\n<frameworkcontent>\n<pt>\nUse the [`~Dataset.set_format`] function to set the dataset format to be compatible with PyTorch:\n\n```py\n>>> dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n>>> dataset.format['type']\n'torch'\n```\n</pt>\n<tf>\nUse the [`~Dataset.to_tf_dataset`] function to set the dataset format to be compatible with TensorFlow. You'll also need to import a [data collator](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding) from \ud83e\udd17 Transformers to combine the varying sequence lengths into a single batch of equal lengths:", "he fast tokenizers of the Transformers library are fast, but they also implement features that will be super useful for data pre-processing and post-processing. Let's have a look at them! First, let's have a look at the usual output of a tokenizer. We get input IDs that correspond to tokens, but we lose a lot of information in the process. For instance, here the tokenization is the same for the two sentences, even if one has several more spaces than the other. Just having the input IDs is thus not enough if we want to match some tokens with a span of text (something we will need to do when tackling question answering for instance). It's also difficult to know when two tokens belong to the same word or not: it looks easy when you just look at the output of a BERT tokenizer, we just need to look for the ##. But other tokenizers have different ways to tokenize parts of words. For instance RoBERTa adds this special G symbol to mark the tokens at the beginning of a word, and T5 uses this special underscore symbol for the same purpose. Thankfully, the fast tokenizers keep track of the word each token comes from, with a word_ids method you can use on their outputs. The output is not necessarily clear, but assembled together in a nice table like this, we can look at the word position for each token", "## [0.8.0]\n\n### Highlights of this release\n- We can now encode both pre-tokenized inputs, and raw strings. This is especially usefull when\nprocessing datasets that are already pre-tokenized like for NER (Name Entity Recognition), and helps\nwhile applying labels to each word.\n- Full tokenizer serialization. It is now easy to save a tokenizer to a single JSON file, to later\nload it back with just one line of code. That's what sharing a Tokenizer means now: 1 line of code.\n- With the serialization comes the compatibility with `Pickle`! The Tokenizer, all of its components,\nEncodings, everything can be pickled!\n- Training a tokenizer is now even faster (up to 5-10x) than before!\n- Compatibility with `multiprocessing`, even when using the `fork` start method. Since this library\nmakes heavy use of the multithreading capacities of our computers to allows a very fast tokenization,\nthis led to problems (deadlocks) when used with `multiprocessing`. This version now allows to\ndisable the parallelism, and will warn you if this is necessary.\n- And a lot of other improvements, and fixes.\n\n### Fixed\n- [#286]: Fix various crash when training a BPE model\n- [#309]: Fixed a few bugs related to additional vocabulary/tokens", ". The second step of the tokenization pipeline is to map those tokens to their respective IDs as defined by the vocabulary of the tokenizer.  This is why we need to download a file when we instantiate a tokenizer with the from_pretrained method: we have to make sure we use the same mapping as when the model was pretrained. To do this, we use the convert_tokens_to_ids method. You may have noticed that we don't have the exact same result as in our first slide \u2014 or not, as this looks like a list of random numbers, in which case allow me to refresh your memory. We had a number at the beginning and at the end that are missing, those are the special tokens. The special tokens are added by the prepare_for_model method, which knows the indices of those tokens in the vocabulary and just adds the proper numbers. You can look at the special tokens (and more generally at how the tokenizer has changed your text) by using the decode method on the outputs of the tokenizer object. As for the prefix for beginning of words/part of words, those special tokens vary depending on which tokenizer you are using. The BERT tokenizer uses [CLS] and [SEP] but the roberta tokenizer uses html-like anchors <s> and </s>. Now that you know how the tokenizer works, you can forget all those intermediaries methods and only remember that you just have to call it on your input texts. The inputs don't contain the inputs IDs however, to learn what the attention mask is, check out the \"Batch inputs together\" video", "## Wrapping up: From tokenizer to model[[wrapping-up-from-tokenizer-to-model]]\n\nNow that we've seen all the individual steps the `tokenizer` object uses when applied on texts, let's see one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors with its main API:\n\n{#if fw === 'pt'}\n```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\ntokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\noutput = model(**tokens)\n```\n{:else}\n```py\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\ntokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\noutput = model(**tokens)\n```\n{/if}", "</Tip>\n\n### Processing the data[[processing-the-data]]\n\n<Youtube id=\"XAR8jnZZuUs\"/>\n\nYou should know the drill by now: the texts all need to be converted into sets of token IDs so the model can make sense of them. For this task, we'll need to tokenize both the inputs and the targets. Our first task is to create our `tokenizer` object. As noted earlier, we'll be using a Marian English to French pretrained model. If you are trying this code with another pair of languages, make sure to adapt the model checkpoint. The [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) organization provides more than a thousand models in multiple languages.\n\n```python\nfrom transformers import AutoTokenizer\n\nmodel_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n```\n\nYou can also replace the `model_checkpoint` with any other model you prefer from the [Hub](https://huggingface.co/models), or a local folder where you've saved a pretrained model and a tokenizer.\n\n<Tip>\n\n\ud83d\udca1 If you are using a multilingual tokenizer such as mBART, mBART-50, or M2M100, you will need to set the language codes of your inputs and targets in the tokenizer by setting `tokenizer.src_lang` and `tokenizer.tgt_lang` to the right values.\n\n</Tip>", "## Contents\n\nThe documentation is organized into five sections:\n\n- **GET STARTED** provides a quick tour of the library and installation instructions to get up and running.\n- **TUTORIALS** are a great place to start if you're a beginner. This section will help you gain the basic skills you need to start using the library.\n- **HOW-TO GUIDES** show you how to achieve a specific goal, like finetuning a pretrained model for language modeling or how to write and share a custom model.\n- **CONCEPTUAL GUIDES** offers more discussion and explanation of the underlying concepts and ideas behind models, tasks, and the design philosophy of \ud83e\udd17 Transformers.\n- **API** describes all classes and functions:\n\n  - **MAIN CLASSES** details the most important classes like configuration, model, tokenizer, and pipeline.\n  - **MODELS** details the classes and functions related to each model implemented in the library.\n  - **INTERNAL HELPERS** details utility classes and functions used internally.\n\n\n## Supported models and frameworks\n\nThe table below represents the current support in the library for each of those models, whether they have a Python\ntokenizer (called \"slow\"). A \"fast\" tokenizer backed by the \ud83e\udd17 Tokenizers library, whether they have support in Jax (via\nFlax), PyTorch, and/or TensorFlow.\n\n<!--This table is updated automatically from the auto modules with _make fix-copies_. Do not update manually!-->"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It explains the role of tokenizers in breaking down text into smaller units, which is a fundamental aspect of translating text into data that a model can process, as mentioned in the reference answer. The response goes beyond the reference answer by detailing the benefits of tokenization, such as reducing dimensionality and enabling specific NLP tasks. However, the response does not explicitly mention the translation of text into data, which is the core purpose highlighted in the reference answer. Therefore, while the response is on the right track, it does not perfectly align with the reference answer's focus."}, {"question": "What is the purpose of the Safety Checker in the Diffusers library?\n", "true_answer": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md", "generated_answer": "The Safety Checker in the Diffusions library is a component that checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. Its purpose is to prevent the generation of harmful content by diffusion models like Stable Diffusion, which can generate images with explicit or disturbing content. The Safety Checker is intentionally hidden to prevent reverse engineering of the checker, and users can disable it if necessary, but it", "retrieved_docs": ["- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repository\u2019s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.", "```python\nfrom diffusers import DiffusionPipeline, EulerDiscreteScheduler\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\nscheduler = EulerDiscreteScheduler.from_pretrained(repo_id, subfolder=\"scheduler\")\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, scheduler=scheduler, use_safetensors=True)\n```\n\n### Safety checker\n\nDiffusion models like Stable Diffusion can generate harmful content, which is why \ud83e\udde8 Diffusers has a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) to check generated outputs against known hardcoded NSFW content. If you'd like to disable the safety checker for whatever reason, pass `None` to the `safety_checker` argument:\n\n```python\nfrom diffusers import DiffusionPipeline", "```python\nfrom diffusers import DiffusionPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, safety_checker=None, use_safetensors=True)\n\"\"\"\nYou have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide by the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend keeping the safety filter enabled in all public-facing circumstances, disabling it only for use cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n\"\"\"\n```\n\n### Reuse components across pipelines\n\nYou can also reuse the same components in multiple pipelines to avoid loading the weights into RAM twice. Use the [`~DiffusionPipeline.components`] method to save the components:\n\n```python\nfrom diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\nstable_diffusion_txt2img = StableDiffusionPipeline.from_pretrained(model_id, use_safetensors=True)\n\ncomponents = stable_diffusion_txt2img.components\n```\n\nThen you can pass the `components` to another pipeline without reloading the weights into RAM:", "- `\"feature_extractor\"`: a [`~transformers.CLIPImageProcessor`] from \ud83e\udd17 Transformers.\n- `\"safety_checker\"`: a [component](https://github.com/huggingface/diffusers/blob/e55687e1e15407f60f32242027b7bb8170e58266/src/diffusers/pipelines/stable_diffusion/safety_checker.py#L32) for screening against harmful content.\n- `\"scheduler\"`: an instance of [`PNDMScheduler`].\n- `\"text_encoder\"`: a [`~transformers.CLIPTextModel`] from \ud83e\udd17 Transformers.\n- `\"tokenizer\"`: a [`~transformers.CLIPTokenizer`] from \ud83e\udd17 Transformers.\n- `\"unet\"`: an instance of [`UNet2DConditionModel`].\n- `\"vae\"`: an instance of [`AutoencoderKL`].\n\n```json\nStableDiffusionPipeline {\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"PNDMScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n```\n\nCompare the components of the pipeline instance to the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main) folder structure, and you'll see there is a separate folder for each of the components in the repository:", "## Usability over Performance\n\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.\n- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.\n\n## Simple over easy", "## Usability over Performance\n\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.\n- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.\n\n## Simple over easy", "In a nutshell, this means that everything that is **not** related to the **code of the Diffusers library** (including the documentation) should **not** be asked on GitHub, but rather on either the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) or [Discord](https://discord.gg/G7tWnz98XR).\n\n**Please consider the following guidelines when opening a new issue**:\n- Make sure you have searched whether your issue has already been asked before (use the search bar on GitHub under Issues).\n- Please never report a new issue on another (related) issue. If another issue is highly related, please\nopen a new issue nevertheless and link to the related issue.\n- Make sure your issue is written in English. Please use one of the great, free online translation services, such as [DeepL](https://www.deepl.com/translator) to translate from your native language to English if you are not comfortable in English.\n- Check whether your issue might be solved by updating to the newest Diffusers version. Before posting your issue, please make sure that `python -c \"import diffusers; print(diffusers.__version__)\"` is higher or matches the latest Diffusers version.\n- Remember that the more effort you put into opening a new issue, the higher the quality of your answer will be and the better the overall quality of the Diffusers issues.\n\nNew issues usually include the following.\n\n#### 2.1. Reproducible, minimal bug reports"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct, accurate, and factual. It correctly identifies the Safety Checker as a component that checks the class probability of harmful concepts against generated images to prevent the creation of harmful content. It also correctly mentions that it is a feature of diffusion models like Stable Diffusion and that it is hidden to prevent reverse engineering. However, the response includes additional information not present in the reference answer, such as the ability for users to disable the Safety Checker, which is not necessary to answer the question about its purpose. Despite this, the core information about the purpose of the Safety Checker aligns well with the reference answer."}, {"question": "What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n", "true_answer": "HfApi", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md", "generated_answer": "The `HfApi` class in the `huggingface_hub` library allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub. You can use the `get_repo_discussions` method of the `HfApi` class to retrieve a generator of `Discussion` objects, which contain high-level overview of the Discussion or Pull Request. The `get_discussion_details` method returns a `", "retrieved_docs": ["!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Interact with Discussions and Pull Requests \n\nThe `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub.\nVisit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\n\n## Retrieve Discussions and Pull Requests from the Hub\n\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(repo_id=\"bigscience/bloom\"):\n...     print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is_pull_request}\")\n\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n\n`HfApi.get_repo_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):", "```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(\n...    repo_id=\"bigscience/bloom\",\n...    author=\"ArthurZ\",\n...    discussion_type=\"pull_request\",\n...    discussion_status=\"open\",\n... ):\n...     print(f\"{discussion.num} - {discussion.title} by {discussion.author}, pr: {discussion.is_pull_request}\")\n\n# 19 - Add Flax weights by ArthurZ, pr: True\n```\n\n`HfApi.get_repo_discussions` returns a [generator](https://docs.python.org/3.7/howto/functional.html#generators) that yields\n[`Discussion`] objects. To get all the Discussions in a single list, run:\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> discussions_list = list(get_repo_discussions(repo_id=\"bert-base-uncased\"))\n```\n\nThe [`Discussion`] object returned by [`HfApi.get_repo_discussions`] contains high-level overview of the\nDiscussion or Pull Request. You can also get more detailed information using [`HfApi.get_discussion_details`]:\n\n```python\n>>> from huggingface_hub import get_discussion_details", "These lightly wrap around the API Endpoints. Documentation for valid parameters and descriptions can be found [here](https://huggingface.co/docs/hub/endpoints).\n  \n\n### Advanced programmatic repository management \n\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed.\n\nInstantiate a `Repository` object by calling it with a path to a local Git\nclone/repository:\n\n```python\n>>> from huggingface_hub import Repository\n>>> repo = Repository(\"<path>/<to>/<folder>\")\n```\n\nThe `Repository` takes a `clone_from` string as parameter. This can stay as\n`None` for offline management, but can also be set to any URL pointing to a Git\nrepo to clone that repository in the specified directory:\n\n```python\n>>> repo = Repository(\"huggingface-hub\", clone_from=\"https://github.com/huggingface/huggingface_hub\")\n```\n\nThe `clone_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n\n```python\n>>> repo = Repository(\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\n```\n\nIf the repository you're cloning is one of yours or one of your organisation's,\nthen having the ability to commit and push to that repository is important. In\norder to do that, you should make sure to be logged-in using `huggingface-cli\nlogin`, and to have the `token` parameter set to `True` (the default)\nwhen  instantiating the `Repository` object:", "```python\n>>> from huggingface_hub import get_discussion_details\n\n>>> get_discussion_details(\n...     repo_id=\"bigscience/bloom-1b3\",\n...     discussion_num=2\n... )\nDiscussionWithDetails(\n    num=2,\n    author='cakiki',\n    title='Update VRAM memory for the V100s',\n    status='open',\n    is_pull_request=True,\n    events=[\n        DiscussionComment(type='comment', author='cakiki', ...),\n        DiscussionCommit(type='commit', author='cakiki', summary='Update VRAM memory for the V100s', oid='1256f9d9a33fa8887e1c1bf0e09b4713da96773a', ...),\n    ],\n    conflicting_files=[],\n    target_branch='refs/heads/main',\n    merge_commit_oid=None,\n    diff='diff --git a/README.md b/README.md\\nindex a6ae3b9294edf8d0eda0d67c7780a10241242a7e..3a1814f212bc3f0d3cc8f74bdbd316de4ae7b9e3 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -132,7 +132,7 [...]',\n)\n```\n\n[`HfApi.get_discussion_details`] returns a [`DiscussionWithDetails`] object, which is a subclass of [`Discussion`]\nwith more detailed information about the Discussion or Pull Request. Information includes all the comments, status changes,\nand renames of the Discussion via [`DiscussionWithDetails.events`].\n\nIn case of a Pull Request, you can retrieve the raw git diff with [`DiscussionWithDetails.diff`]. All the commits of the\nPull Request are listed in [`DiscussionWithDetails.events`].\n\n\n## Create and edit a Discussion or Pull Request programmatically", "## Create and edit a Discussion or Pull Request programmatically\n\nThe [`HfApi`] class also offers ways to create and edit Discussions and Pull Requests.\nYou will need an [access token](https://huggingface.co/docs/hub/security-tokens) to create and edit Discussions\nor Pull Requests.\n\nThe simplest way to propose changes on a repo on the Hub is via the [`create_commit`] API: just \nset the `create_pr` parameter to `True`. This parameter is also available on other methods that wrap [`create_commit`]:\n\n    * [`upload_file`]\n    * [`upload_folder`]\n    * [`delete_file`]\n    * [`delete_folder`]\n    * [`metadata_update`]\n\n```python\n>>> from huggingface_hub import metadata_update\n\n>>> metadata_update(\n...     repo_id=\"username/repo_name\",\n...     metadata={\"tags\": [\"computer-vision\", \"awesome-model\"]},\n...     create_pr=True,\n... )\n```\n\nYou can also use [`HfApi.create_discussion`] (respectively [`HfApi.create_pull_request`]) to create a Discussion (respectively a Pull Request) on a repo.\nOpening a Pull Request this way can be useful if you need to work on changes locally. Pull Requests opened this way will be in `\"draft\"` mode.\n\n```python\n>>> from huggingface_hub import create_discussion, create_pull_request\n\n>>> create_discussion(\n...     repo_id=\"username/repo-name\",\n...     title=\"Hi from the huggingface_hub library!\",\n...     token=\"<insert your access token here>\",\n... )\nDiscussionWithDetails(...)", "## API and client library interaction with the Hub\n\nInteracting with the Hugging Face Hub via an [API](https://huggingface.co/docs/hub/api) or the [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index) Python library is possible. This includes creating new repositories, uploading data programmatically and creating and modifying metadata for datasets. This can be powerful for research workflows where new data or annotations continue to be created. The client library also makes uploading large datasets much more accessible. \n\n## Community \n\nThe Hugging Face Hub is already home to a large community of researchers, developers, artists, and others interested in using and contributing to an ecosystem of open-source machine learning. Making your datasets accessible to this community increases their visibility, opens them up to new types of users and places your datasets within the context of a larger ecosystem of models, datasets and libraries.\n\nThe Hub also has features which allow communities to collaborate more easily. This includes a discussion page for each dataset, model and Space hosted on the Hub. This means users of your datasets can quickly ask questions and discuss ideas for working with a dataset.", "--\ntitle: Introducing Pull Requests and Discussions \ud83e\udd73\nthumbnail: /blog/assets/76_community_update/thumbnail.png\n---\n\n# Introducing Pull Requests and Discussions \ud83e\udd73\n\n![Pull requests and discussions on the hub](assets/76_community_update/community-update.png)\n\nWe are thrilled to announce the release of our latest collaborative features: pull requests and discussions on the Hugging Face Hub!\n\nPull requests and discussions are available today under the [community tab](https://huggingface.co/gpt2/discussions) for all repository types: models, datasets, and Spaces. Any member of the community can create and participate in discussions and pull requests, facilitating collaborations not only within teams, but also with everyone else in the community!\n\nIt's the biggest update ever done to the Hub, and we can't wait to see the community members start collaborating with it \ud83e\udd29.\n\nThe new \"Community\" tab also aligns with proposals in ethical ML throughout the years. Feedback and iterations have a central place in the development of ethical machine learning software. We really believe having it in the community's toolset will unlock new kinds of positive patterns in ML, collaborations, and progress.\n\nSome example use cases for discussions and pull requests:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the `HfApi` class as the means to retrieve Discussions and Pull Requests from a repository on the Hugging Face Hub, which aligns with the reference answer. The additional details provided about the methods `get_repo_discussions` and `get_discussion_details` are accurate and factual, although the response is incomplete as it ends abruptly. However, the core requirement of the instruction is met by correctly naming the class responsible for the retrieval of Discussions and Pull Requests."}, {"question": "What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n", "true_answer": "Skops", "source_doc": "huggingface/blog/blob/main/skops.md", "generated_answer": "The name of the new library introduced by Hugging Face for hosting scikit-learn models is Skops. This information can be found in the context provided, specifically in the section titled \"Introducing Skops\".", "retrieved_docs": ["Hugging Face is a collaborative Machine Learning platform in which the community has shared over 150,000 models, 25,000 datasets, and 30,000 ML apps. The Hub has model and dataset versioning tools, including model cards and client-side libraries to automate the versioning process. However, only including a model card with hyperparameters is not enough to provide the best reproducibility; this is where notebooks can help. Alongside these models, datasets, and demos, the Hub hosts over 7,000 notebooks. These notebooks often document the development process of a model or a dataset and can provide guidance and tutorials showing how others can use these resources. We\u2019re therefore excited about our improved support for notebook hosting on the Hub. \n\n## What have we changed? \n\nUnder the hood, Jupyter notebook files (usually shared with an `ipynb` extension) are JSON files. While viewing these files directly is possible, it's not a format intended to be read by humans. We have now added rendering support for notebooks hosted on the Hub. This means that notebooks will now be displayed in a human-readable format.", "## Further References\n\n- This Hugging Face implementation uses [this scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)", "## API and client library interaction with the Hub\n\nInteracting with the Hugging Face Hub via an [API](https://huggingface.co/docs/hub/api) or the [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index) Python library is possible. This includes creating new repositories, uploading data programmatically and creating and modifying metadata for datasets. This can be powerful for research workflows where new data or annotations continue to be created. The client library also makes uploading large datasets much more accessible. \n\n## Community \n\nThe Hugging Face Hub is already home to a large community of researchers, developers, artists, and others interested in using and contributing to an ecosystem of open-source machine learning. Making your datasets accessible to this community increases their visibility, opens them up to new types of users and places your datasets within the context of a larger ecosystem of models, datasets and libraries.\n\nThe Hub also has features which allow communities to collaborate more easily. This includes a discussion page for each dataset, model and Space hosted on the Hub. This means users of your datasets can quickly ask questions and discuss ideas for working with a dataset.", "You can try our early demo of [structured data classification](https://huggingface.co/julien-c/wine-quality) with Scikit-learn. And finally, we also introduced new widgets for image-related models: **text to image**, **image classification**, and **object detection**. Try image classification with Google's ViT model [here](https://huggingface.co/google/vit-base-patch16-224) and object detection with Facebook AI's DETR model [here](https://huggingface.co/facebook/detr-resnet-50)!\n\n![Object Detection Widget](assets/27_summer_at_huggingface/object-detection.png)\n\n\n### More Features\n\nThat's not everything that has happened in the Hub. We've introduced new and improved [documentation](https://huggingface.co/docs/hub/main) of the Hub. We also introduced two widely requested features: users can now transfer/rename repositories and directly upload new files to the Hub.\n\n![Button to upload a file](assets/27_summer_at_huggingface/upload_file.png)\n\n## Community\n\n### Hugging Face Course", "## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.", "## Integrating to the Hub.\n\nWe're partnering with cool open source ML libraries to provide free model hosting and versioning. You can find the existing integrations [here](https://huggingface.co/docs/hub/libraries).\n\nThe advantages are:\n\n- Free model or dataset hosting for libraries and their users.\n- Built-in file versioning, even with very large files, thanks to a git-based approach.\n- Hosted inference API for all models publicly available.\n- In-browser widgets to play with the uploaded models.\n- Anyone can upload a new model for your library, they just need to add the corresponding tag for the model to be discoverable.\n- Fast downloads! We use Cloudfront (a CDN) to geo-replicate downloads so they're blazing fast from anywhere on the globe.\n- Usage stats and more features to come.\n\nIf you would like to integrate your library, feel free to open an issue to begin the discussion. We wrote a [step-by-step guide](https://huggingface.co/docs/hub/adding-a-library) with \u2764\ufe0f showing how to do this integration.\n\n## Contributions (feature requests, bugs, etc.) are super welcome \ud83d\udc99\ud83d\udc9a\ud83d\udc9b\ud83d\udc9c\ud83e\udde1\u2764\ufe0f", "--\ntitle: \"Introducing Skops\"\nthumbnail: /blog/assets/94_skops/introducing_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n\n# Introducing Skops\n\n\n##\u00a0Introducing Skops\n\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others.\n\nLet's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production.\n\n```python\n# let's import the libraries first\nimport sklearn\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Load the data and split\nX, y = load_breast_cancer(as_frame=True, return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Train the model\nmodel = DecisionTreeClassifier().fit(X_train, y_train)\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer and provides additional context about where the information can be found, which is not required but does not detract from the accuracy of the response."}, {"question": "What is the purpose of Textual Inversion?\n", "true_answer": "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/loaders/textual_inversion.md", "generated_answer": "Textual Inversion is a technique used to personalize text-to-image models like Stable Diffusion on specific images by teaching them a particular visual concept using just 3-5 examples. Its purpose is to generate images involving the concept(s) of interest by fine-tuning a pre-trained text-to-image diffusion model. This technique is useful for teaching models about new concepts, such as a specific style of artwork or a particular subject, and can be integrated into the", "retrieved_docs": ["## Textual inversion\n\n[Textual inversion](../training/text_inversion) is a technique for learning a specific concept from some images which you can use to generate new images conditioned on that concept.\n\nCreate a pipeline and use the [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`] function to load the textual inversion embeddings (feel free to browse the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer) for 100+ trained concepts):\n\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom compel import Compel, DiffusersTextualInversionManager\n\npipe = StableDiffusionPipeline.from_pretrained(\n  \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16,\n  use_safetensors=True, variant=\"fp16\").to(\"cuda\")\npipe.load_textual_inversion(\"sd-concepts-library/midjourney-style\")\n```\n\nCompel provides a `DiffusersTextualInversionManager` class to simplify prompt weighting with textual inversion. Instantiate `DiffusersTextualInversionManager` and pass it to the `Compel` class:\n\n```py\ntextual_inversion_manager = DiffusersTextualInversionManager(pipe)\ncompel_proc = Compel(\n    tokenizer=pipe.tokenizer,\n    text_encoder=pipe.text_encoder,\n    textual_inversion_manager=textual_inversion_manager)\n```\n\nIncorporate the concept to condition a prompt with using the `<concept>` syntax:\n\n```py\nprompt_embeds = compel_proc('(\"A red cat++ playing with a ball <midjourney-style>\")')", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"\n\npython textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --seed=7 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --max_train_steps=3000 \\\n  --learning_rate=2.5e-03 --scale_lr \\\n  --output_dir=\"textual_inversion_dicoo\"\n```", "# [Deprecated] Multi Token Textual Inversion\n\n**IMPORTART: This research project is deprecated. Multi Token Textual Inversion is now supported natively in [the official textual inversion example](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion#running-locally-with-pytorch).**\n\nThe author of this project is [Isamu Isozaki](https://github.com/isamu-isozaki) - please make sure to tag the author for issue and PRs as well as @patrickvonplaten.\n\nWe add multi token support to textual inversion. I added\n1. num_vec_per_token for the number of used to reference that token\n2. progressive_tokens for progressively training the token from 1 token to 2 token etc\n3. progressive_tokens_max_steps for the max number of steps until we start full training\n4. vector_shuffle to shuffle vectors\n\nFeel free to add these options to your training! In practice num_vec_per_token around 10+vector shuffle works great!\n\n## Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)", "## Textual Inversion\n\n[Paper](https://arxiv.org/abs/2208.01618)\n\n[Textual Inversion](../training/text_inversion) fine-tunes a model to teach it about a new concept. I.e. a few pictures of a style of artwork can be used to generate images in that style.\n\n## ControlNet\n\n[Paper](https://arxiv.org/abs/2302.05543)\n\n[ControlNet](../api/pipelines/controlnet) is an auxiliary network which adds an extra condition.\nThere are 8 canonical pre-trained ControlNets trained on different conditionings such as edge detection, scribbles,\ndepth maps, and semantic segmentations.\n\n## Prompt Weighting\n\n[Prompt weighting](../using-diffusers/weighted_prompts) is a simple technique that puts more attention weight on certain parts of the text\ninput.\n\n## Custom Diffusion\n\n[Paper](https://arxiv.org/abs/2212.04488)\n\n[Custom Diffusion](../training/custom_diffusion) only fine-tunes the cross-attention maps of a pre-trained\ntext-to-image diffusion model. It also allows for additionally performing Textual Inversion. It supports\nmulti-concept training by design. Like DreamBooth and Textual Inversion, Custom Diffusion is also used to\nteach a pre-trained text-to-image diffusion model about new concepts to generate outputs involving the\nconcept(s) of interest.\n\n## Model Editing\n\n[Paper](https://arxiv.org/abs/2303.08084)", "* [Textual Inversion](../training/text_inversion), an algorithm that teaches a model a specific visual concept and integrates it into the generated image.\n* [DreamBooth](../training/dreambooth), a technique for generating personalized images of a subject given several input images of the subject.\n* [Guide](../training/text2image) to finetuning a Stable Diffusion model on your own dataset.\n* [Guide](../training/lora) to using LoRA, a memory-efficient technique for finetuning really large models faster.", ". As for the offset mappings, it will be super useful when we need to convert token positions in a sentence into a span of text, which we will need to know when looking at question answering or when grouping the tokens corresponding to the same entity in token classification. To have a look at these tasks, check the videos linked below!", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nThen cd in the example folder and run:\n```bash\npip install -r requirements.txt\n```"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It correctly identifies Textual Inversion as a technique used to personalize text-to-image models and mentions the use of a small number of examples to teach the model a new visual concept. It also accurately describes the purpose of generating images involving the concept(s) of interest by fine-tuning a pre-trained model. However, the response could be more concise and focused on the definition of Textual Inversion as a training method rather than the broader application of the technique. The reference answer is more succinct and directly addresses the definition of Textual Inversion. Therefore, the response aligns well with the reference answer but includes additional context that is not necessary for a complete understanding of the concept."}, {"question": "What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n", "true_answer": "64", "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md", "generated_answer": "Based on the benchmark results provided, it seems that a recommended multiple of batch size for fp16 data type on an A100 GPU is around 8-16. This can be seen in the \"Batch size = 86\" section, where the throughput for the \"None\" quantization with fp16 data type is around 229 tok/s for a batch size of 1, and around 140 tok/s for a batch", "retrieved_docs": ["Bitsandbytes uses the fp4 scheme, with the compute in fp16.\n\n### Batch size = 1\n\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|None|None     |None|None      |None  |26.0         |36.958                |27.058            |29152.98        |\n| gptq | False | 4 | 128 | exllamav2 | 36.07 | 32.25 | 31.01 | 11313.75 |\n|gptq |False    |4   |128       |exllama|36.2         |33.711                |29.663            |10484.34        |\n|gptq |False    |4   |128       |autogptq-cuda-old|36.2         |46.44                 |21.53             |10344.62        |\n|bitsandbytes|None     |None|None      |None  |37.64        |52.00                 |19.23             |11018.36       |\n\n### Batch size = 2", "In float16, the component sizes are 11GB, 8.6GB, and 1.25GB for T5,\nStage1 and Stage2 UNets, respectively, which is doable for the GPU, but\nwe're still running into CPU memory overflow errors when loading the T5\n(some CPU is occupied by other processes).\n\nTherefore, we lower the precision of T5 even more by using\n`bitsandbytes` 8bit quantization, which allows saving the T5 checkpoint\nwith as little as [8\nGB](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0/blob/main/text_encoder/model.8bit.safetensors).\n\nNow that each component fits individually into both CPU and GPU memory,\nwe need to make sure that components have all the CPU and GPU memory for\nthemselves when needed.\n\nDiffusers supports modularly loading individual components i.e. we can\nload the text encoder without loading the UNet. This modular loading\nwill ensure that we only load the component we need at a given step in\nthe pipeline to avoid exhausting the available CPU RAM and GPU VRAM.\n\nLet\\'s give it a try \ud83d\ude80\n\n![t2i_64](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/if/t2i_64.png)\n\n## Available resources\n\nThe free-tier Google Colab comes with around 13 GB CPU RAM:\n\n``` python\n!grep MemTotal /proc/meminfo\n```\n\n```bash\nMemTotal:       13297192 kB\n```\n\nAnd an NVIDIA T4 with 15 GB VRAM:\n\n``` python\n!nvidia-smi\n```", "### Batch size = 2\n\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tokens|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Max memory (MB)|\n|-----|---------|----|----------|-----------------|-------------|----------|-------------|----------------------|------------------|---------------|\n|None|None     |None|None      |None             |512          |1         |27.22        |169.95                |11.77             |28524.37       |\n| gptq | False | 4 | 128 | exllamav2 | 512 | 1 | 6.63 | 212.07  | 9.43  | 10783.60 |\n|gptq |False    |4   |128       |exllama          |512          |1         |38.35        |190.44                |10.50             |9855.71        |\n|gptq |False    |4   |128       |autogptq-cuda-old|512          |1         |43.94        |443.80                |4.51              |9928.23        |\n|bitsandbytes|None|None|None|None|512|1  |37.46|212.76 |9.40 |10421.89|\n\n### Batch size = 4", "### Batch size = 8\n\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tokens|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Max memory (MB)|\n|-----|---------|----|----------|-----------------|-------------|----------|-------------|----------------------|------------------|---------------|\n|None|None     |None|None      |None             |512          |1         |27.22        |600.47                |13.32             |31673.30       |\n| gptq | False | 4 | 128 | exllamav2 | 512 | 1 | 6.63 | 753.06  | 10.62 | 13920.50 |\n|gptq |False    |4   |128       |exllama          |512          |1         |38.35        |659.61                |12.13             |13004.64       |\n|gptq |False    |4   |128       |autogptq-cuda-old|512          |1         |43.94        |909.09                |8.80              |12862.18       |\n|bitsandbytes|None|None|None|None|512|1  |37.46|643.42 |12.43|13539.37|\n\n### Batch size = 16", "### Batch size = 1\n\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tokens|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Max memory (MB)|\n|-----|---------|----|----------|-----------------|-------------|----------|-------------|----------------------|------------------|---------------|\n|None|None     |None|None      |None             |512          |1         |27.22        |96.38                 |10.38             |27999.54       |\n| gptq | False | 4 | 128 | exllamav2 | 512 | 1 | 6.63 | 116.07  | 8.62  | 10260.35 |\n|gptq |False    |4   |128       |exllama          |512          |1         |38.35        |112.54                |8.89              |9330.89        |\n|gptq |False    |4   |128       |autogptq-cuda-old|512          |1         |43.94        |368.13                |2.72              |9474.19        |\n|bitsandbytes|None|None|None|None|512|1  |37.46|139.17 |7.19 |9952.65 |\n\n### Batch size = 2", "### Batch size = 4\n\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tokens|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Max memory (MB)|\n|-----|---------|----|----------|-----------------|-------------|----------|-------------|----------------------|------------------|---------------|\n|None|None     |None|None      |None             |512          |1         |27.22        |305.99                |13.07             |29574.01       |\n| gptq | False | 4 | 128 | exllamav2 | 512 | 1 | 6.63 | 385.58  | 10.37 | 11829.59 |\n|gptq |False    |4   |128       |exllama          |512          |1         |38.35        |345.54                |11.58             |10905.35       |\n|gptq |False    |4   |128       |autogptq-cuda-old|512          |1         |43.94        |597.24                |6.70              |10838.42       |\n|bitsandbytes|None|None|None|None|512|1  |37.46|349.18 |11.46|11440.08|\n\n### Batch size = 8", "### Batch size = 16\n\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|None|None     |None|None      |None  |26.0         |69.94                 |228.76            |53986.51        |\n| gptq | False | 4 | 128 | exllamav2 | 36.07 | 83.09 | 192.55 | 35740.95 |\n|gptq |False    |4   |128       |exllama|36.2         |95.41                 |167.68            |34777.04        |\n|gptq |False    |4   |128       |autogptq-cuda-old|36.2         |192.48                |83.12             |35497.62        |\n|bitsandbytes|None     |None|None      |None  |37.64        |113.98                |140.38            |35532.37       |\n\n## Prefill-only benchmark results\n\nRun\n\n```shell\n# pytorch fp16\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model meta-llama/Llama-2-13b-chat-hf --sweep --num-batches 10 --task text-generation --prefill --generate\n\n# GPTQ with exllamav2 kernel (int4/fp16)\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model TheBloke/Llama-2-13B-chat-GPTQ --sweep --num-batches 10 --gptq --task text-generation --prefill --use-exllama --exllama-version 2 --generate \n\n# GPTQ with exllamav kernel (int4/fp16)\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model TheBloke/Llama-2-13B-chat-GPTQ --sweep --num-batches 10 --gptq --task text-generation --prefill --use-exllama --generate"], "eval_score_GPT4": "2", "eval_feedback_GPT4": "Feedback: The response provided does not accurately state a specific recommended multiple of batch size for fp16 data type on an A100 GPU. Instead, it gives a range (8-16) and discusses benchmark results that are not directly related to the question about the recommended multiple. The reference answer is a specific number (64), which is not mentioned in the response. Therefore, the response is mostly incorrect and not factual as per the reference answer."}, {"question": "How do you run a Gradio Blocks app in reload mode using a Python IDE?\n", "true_answer": "Run `gradio run.py` in the terminal.", "source_doc": "gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md", "generated_answer": "To run a Gradio Blocks app in reload mode using a Python IDE, follow these steps:\n\n1. Make sure you have installed Gradio using pip.\n2. Create a new Python file (e.g., `run.py`) and import Gradio at the top: `import gradio as gr`.\n3. Define your Gradio app inside a `gr.Blocks()` function, just like you would normally.\n4. Launch your app", "retrieved_docs": ["Developing Faster with Auto-Reloading\n\n**Prerequisite**: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).\n\nThis guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\n\n## Why Auto-Reloading?\n\nWhen you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.\n\nTo make it faster and more convenient to write your code, we've made it easier to \"reload\" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \"magic command\" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).\n\nThis short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster.\n\n## Python IDE Reload \ud83d\udd25\n\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()", "There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as my_demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    my_demo.launch()\n```\n\nThen you would launch it in reload mode like this: `gradio run.py my_demo`.\n\nBy default, the Gradio use UTF-8 encoding for scripts. **For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n\n1. Configure encoding declaration of python script, for example: `# -*- coding: cp1252 -*-`\n2. Confirm that your code editor has identified that encoding format. \n3. Run like this: `gradio run.py --encoding cp1252`\n\n\ud83d\udd25 If your application accepts command line arguments, you can pass them in as well. Here's an example:\n\n```python\nimport gradio as gr\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--name\", type=str, default=\"User\")\nargs, unknown = parser.parse_known_args()", "###### 2. Reload Mode \ud83d\udc68\u200d\ud83d\udcbb\n\nReload mode helps developers create gradio demos faster by automatically reloading the demo whenever the code changes. It can support development on Python IDEs (VS Code, PyCharm, etc), the terminal, as well as Jupyter notebooks.\n\nIf your demo code is in a script named `app.py`, instead of running `python app.py` you can now run `gradio app.py` and that will launch the demo in reload mode:\n\n```bash\nLaunching in reload mode on: http://127.0.0.1:7860 (Press CTRL+C to quit)\nWatching...\nWARNING: The --reload flag should not be used in production on Windows.\n```\n\nIf you're working from a Jupyter or Colab Notebook, use these magic commands instead: `%load_ext gradio` when you import gradio, and `%%blocks` in the top of the cell with the demo code. Here's an example that shows how much faster the development becomes:\n\n![Blocks](https://user-images.githubusercontent.com/9021060/178986488-ed378cc8-5141-4330-ba41-672b676863d0.gif)\n\n###### 3. Inpainting Support on `gr.Image()` \ud83c\udfa8\n\nWe updated the Image component to add support for inpainting demos. It works by adding `tool=\"sketch\"` as a parameter, that passes both an image and a sketchable mask to your prediction function.\n\nHere's an example from the [LAMA space](https://huggingface.co/spaces/akhaliq/lama):\n\n![FXApVlFVsAALSD-](https://user-images.githubusercontent.com/9021060/178989479-549867c8-7fb0-436a-a97d-1e91c9f5e611.jpeg)\n\n###### 4. Markdown and HTML support in Dataframes \ud83d\udd22", "###### 2. Reload Mode \ud83d\udc68\u200d\ud83d\udcbb\n\nReload mode helps developers create gradio demos faster by automatically reloading the demo whenever the code changes. It can support development on Python IDEs (VS Code, PyCharm, etc), the terminal, as well as Jupyter notebooks.\n\nIf your demo code is in a script named `app.py`, instead of running `python app.py` you can now run `gradio app.py` and that will launch the demo in reload mode:\n\n```bash\nLaunching in reload mode on: http://127.0.0.1:7860 (Press CTRL+C to quit)\nWatching...\nWARNING: The --reload flag should not be used in production on Windows.\n```\n\nIf you're working from a Jupyter or Colab Notebook, use these magic commands instead: `%load_ext gradio` when you import gradio, and `%%blocks` in the top of the cell with the demo code. Here's an example that shows how much faster the development becomes:\n\n![Blocks](https://user-images.githubusercontent.com/9021060/178986488-ed378cc8-5141-4330-ba41-672b676863d0.gif)\n\n###### 3. Inpainting Support on `gr.Image()` \ud83c\udfa8\n\nWe updated the Image component to add support for inpainting demos. It works by adding `tool=\"sketch\"` as a parameter, that passes both an image and a sketchable mask to your prediction function.\n\nHere's an example from the [LAMA space](https://huggingface.co/spaces/akhaliq/lama):\n\n![FXApVlFVsAALSD-](https://user-images.githubusercontent.com/9021060/178989479-549867c8-7fb0-436a-a97d-1e91c9f5e611.jpeg)\n\n###### 4. Markdown and HTML support in Dataframes \ud83d\udd22", "inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\n\nInstead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:\n\nIn the terminal, run `gradio run.py`. That's it!\n\nNow, you'll see that after you'll see something like this:\n\n```bash\nWatching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'\n\nRunning on local URL:  http://127.0.0.1:7860\n```\n\nThe important part here is the line that says `Watching...` What's happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically \ud83e\udd73\n\n\u26a0\ufe0f Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show_error` in `launch()` will not be reflected in the app.", "with gr.Blocks() as demo:\n    gr.Markdown(f\"# Greetings {args.name}!\")\n    inp = gr.Textbox()\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\nWhich you could run like this: `gradio run.py --name Gretel`\n\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code. Meaning that this can be useful if you decide to [contribute to Gradio itself](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md) \u2705\n\n## Jupyter Notebook Magic \ud83d\udd2e\n\nWhat about if you use Jupyter Notebooks (or Colab Notebooks, etc.) to develop code? We got something for you too!\n\nWe've developed a **magic command** that will create and run a Blocks demo for you. To use this, load the gradio extension at the top of your notebook:\n\n`%load_ext gradio`\n\nThen, in the cell that you are developing your Gradio demo, simply write the magic command **`%%blocks`** at the top, and then write the layout and components like you would normally:\n\n```py\n%%blocks\n\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(f\"# Greetings {args.name}!\")\n    inp = gr.Textbox()\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n```\n\nNotice that:\n\n- You do not need to launch your demo \u2014 Gradio does that for you automatically!", "Gradio Demo: blocks_plug\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndef change_tab():\n    return gr.Tabs(selected=2)\n\n\nidentity_demo, input_demo, output_demo = gr.Blocks(), gr.Blocks(), gr.Blocks()\n\nwith identity_demo:\n    gr.Interface(lambda x: x, \"text\", \"text\")\n\nwith input_demo:\n    t = gr.Textbox(label=\"Enter your text here\")\n    with gr.Row():\n        btn = gr.Button(\"Submit\")\n        clr = gr.ClearButton(t)\n\nwith output_demo:\n    gr.Textbox(\"This is a static output\")\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Three demos in one!\")\n    with gr.Tabs(selected=1) as tabs:\n        with gr.TabItem(\"Text Identity\", id=0) as tab0:\n            tab0.select(lambda: gr.Tabs(selected=0), None, tabs)\n            identity_demo.render()\n        with gr.TabItem(\"Text Input\", id=1) as tab1:\n            tab1.select(lambda: gr.Tabs(selected=1), None, tabs)\n            input_demo.render()\n        with gr.TabItem(\"Text Static\", id=2) as tab2:\n            tab2.select(lambda: gr.Tabs(selected=2), None, tabs)\n            output_demo.render()\n    btn = gr.Button(\"Change tab\")\n    btn.click(inputs=None, outputs=tabs, fn=change_tab)\n\nif __name__ == \"__main__\":\n    demo.launch()\n\n```"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response provided outlines the general steps to run a Gradio app but does not specifically address how to run it in reload mode, which was the main focus of the instruction. The reference answer suggests using a terminal command, which is not mentioned in the response. Therefore, the response is somewhat correct as it does provide a method to run a Gradio app but fails to include the critical aspect of running it in reload mode."}, {"question": "What command is used to install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment?\n", "true_answer": "pip install \"transformers[sentencepiece]\"", "source_doc": "huggingface/course/blob/main/chapters/en/chapter0/1.mdx", "generated_answer": "The command to install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment is:\n\n```bash\npip install -e \".[dev]\"\n```\n\nThis command should be run inside the virtual environment that was created using the `python -m venv .env` command. The `-e` flag is used to install the library in editable mode, which allows for easy development and testing of the library.", "retrieved_docs": ["## Install with pip\n\nYou should install \ud83e\udd17 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies.\n\nStart by creating a virtual environment in your project directory:\n\n```bash\npython -m venv .env\n```\n\nActivate the virtual environment. On Linux and MacOs:\n\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n\n```bash\n.env/Scripts/activate\n```\n\nNow you're ready to install \ud83e\udd17 Transformers with the following command:\n\n```bash\npip install transformers\n```\n\nFor CPU-support only, you can conveniently install \ud83e\udd17 Transformers and a deep learning library in one line. For example, install \ud83e\udd17 Transformers and PyTorch with:\n\n```bash\npip install 'transformers[torch]'\n```\n\n\ud83e\udd17 Transformers and TensorFlow 2.0:\n\n```bash\npip install 'transformers[tf-cpu]'\n```\n\n<Tip warning={true}>\n\nM1 / ARM Users\n    \nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\n\n</Tip>\n\n\ud83e\udd17 Transformers and Flax:\n\n```bash\npip install 'transformers[flax]'\n```\n\nFinally, check if \ud83e\udd17 Transformers has been properly installed by running the following command. It will download a pretrained model:", "or for an editable install:\n\n```bash\npip install -e .[dev]\n```\n\ninside the Transformers repo. Since the number of optional dependencies of Transformers has grown a lot, it's possible you don't manage to get all of them. If the dev install fails, make sure to install the Deep Learning framework you are working with (PyTorch, TensorFlow and/or Flax) then do\n\n```bash\npip install transformers[quality]\n```\n\nor for an editable install:\n\n```bash\npip install -e .[quality]\n```\n\n\n## Tests\n\nAll the jobs that begin with `ci/circleci: run_tests_` run parts of the Transformers testing suite. Each of those jobs focuses on a part of the library in a certain environment: for instance `ci/circleci: run_tests_pipelines_tf` runs the pipelines test in an environment where TensorFlow only is installed.\n\nNote that to avoid running tests when there is no real change in the modules they are testing, only part of the test suite is run each time: a utility is run to determine the differences in the library between before and after the PR (what GitHub shows you in the \"Files changes\" tab) and picks the tests impacted by that diff. That utility can be run locally with:\n\n```bash\npython utils/tests_fetcher.py\n```\n\nfrom the root of the Transformers repo. It will:", "This installs a very light version of \ud83e\udd17 Transformers. In particular, no specific machine learning frameworks (like PyTorch or TensorFlow) are installed. Since we'll be using a lot of different features of the library, we recommend installing the development version, which comes with all the required dependencies for pretty much any imaginable use case:\n\n```\n!pip install transformers[sentencepiece]\n```\n\nThis will take a bit of time, but then you'll be ready to go for the rest of the course!\n\n## Using a Python virtual environment[[using-a-python-virtual-environment]]\n\nIf you prefer to use a Python virtual environment, the first step is to install Python on your system. We recommend following [this guide](https://realpython.com/installing-python/) to get started.\n\nOnce you have Python installed, you should be able to run Python commands in your terminal. You can start by running the following command to ensure that it is correctly installed before proceeding to the next steps: `python --version`. This should print out the Python version now available on your system.", "```bash\n    git clone https://github.com/[your Github handle]/transformers.git\n    cd transformers\n    git remote add upstream https://github.com/huggingface/transformers.git\n    ```\n\n3.  Set up a development environment, for instance by running the\n    following command:\n\n    ```bash\n    python -m venv .env\n    source .env/bin/activate\n    pip install -e \".[dev]\"\n    ```\n\nand return to the parent directory\n\n```bash\ncd ..\n```\n\n4.  We recommend adding the PyTorch version of *[camelcase name of model]* to\n    Transformers. To install PyTorch, please follow the instructions [here](https://pytorch.org/get-started/locally/).\n\n**Note:** You don't need to have CUDA installed. Making the new model\nwork on CPU is sufficient.\n\n5.  To port *[camelcase name of model]*, you will also need access to its\n    original repository:\n\n```bash\ngit clone [link to original repo].git \ncd [lowercase name of model]\npip install -e .\n```\n\nNow you have set up a development environment to port *[camelcase name of model]*\nto \ud83e\udd17 Transformers.\n\n### Run a pretrained checkpoint using the original repository\n\n**3. Set up debugging environment**", "Clone the repository and install \ud83e\udd17 Transformers with the following commands:\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n\n<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.\n\n</Tip>\n\nNow you can easily update your clone to the latest version of \ud83e\udd17 Transformers with the following command:\n\n```bash\ncd ~/transformers/\ngit pull\n```\n\nYour Python environment will find the `main` version of \ud83e\udd17 Transformers on the next run.\n\n## Install with conda\n\nInstall from the conda channel `huggingface`:\n\n```bash\nconda install -c huggingface transformers\n```\n\n## Cache setup\n\nPretrained models are downloaded and locally cached at: `~/.cache/huggingface/hub`. This is the default directory given by the shell environment variable `TRANSFORMERS_CACHE`. On Windows, the default directory is given by `C:\\Users\\username\\.cache\\huggingface\\hub`. You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:", "```\nls -a\n```\n\n```out\n.      ..    .env\n```\n\nYou can jump in and out of your virtual environment with the `activate` and `deactivate` scripts:\n\n```\n# Activate the virtual environment\nsource .env/bin/activate\n\n# Deactivate the virtual environment\nsource .env/bin/deactivate\n```\n\nYou can make sure that the environment is activated by running the `which python` command: if it points to the virtual environment, then you have successfully activated it!\n\n```\nwhich python\n```\n\n```out\n/home/<user>/transformers-course/.env/bin/python\n```\n\n### Installing dependencies[[installing-dependencies]]\n\nAs in the previous section on using Google Colab instances, you'll now need to install the packages required to continue. Again, you can install the development version of \ud83e\udd17 Transformers using the `pip` package manager:\n\n```\npip install \"transformers[sentencepiece]\"\n```\n\nYou're now all set up and ready to go!", "2.  Clone your `transformers` fork to your local disk, and add the base\n    repository as a remote:\n\n    ```bash\n    git clone https://github.com/[your Github handle]/transformers.git\n    cd transformers\n    git remote add upstream https://github.com/huggingface/transformers.git\n    ```\n\n3.  Set up a development environment, for instance by running the\n    following command:\n\n    ```bash\n    python -m venv .env\n    source .env/bin/activate\n    pip install -e \".[dev]\"\n    ```\n\nand return to the parent directory\n\n```bash\ncd ..\n```\n\n4.  We recommend adding the PyTorch version of *BigBird* to\n    Transformers. To install PyTorch, please follow the instructions [here](https://pytorch.org/get-started/locally/).\n\n**Note:** You don't need to have CUDA installed. Making the new model\nwork on CPU is sufficient.\n\n5.  To port *BigBird*, you will also need access to its\n    original repository:\n\n```bash\ngit clone https://github.com/google-research/bigbird.git \ncd big_bird\npip install -e .\n```\n\nNow you have set up a development environment to port *BigBird*\nto \ud83e\udd17 Transformers.\n\n### Run a pretrained checkpoint using the original repository\n\n**3. Set up debugging environment**"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided is not correct when compared to the reference answer. The reference answer suggests using the command `pip install \"transformers[sentencepiece]\"` to install the development version of the \ud83e\udd17 Transformers library, which is not the same as the command provided in the response. The response suggests using `pip install -e \".[dev]\"`, which is a general command for installing a Python package in editable mode and does not specifically install the development version of the Transformers library. Therefore, the response does not meet the criteria for being completely correct, accurate, and factual."}, {"question": "How can you install the Hugging Face Unity API in your Unity project?\n", "true_answer": "To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.", "source_doc": "huggingface/blog/blob/main/unity-api.md", "generated_answer": "To install the Hugging Face Unity API in your Unity project, follow these steps:\n\n1. Open your Unity project.\n2. Go to \"Window\" > \"Package Manager\".\n3. Click the \"+\" button and select \"Add Package from git URL\".\n4. Enter \"https://github.com/huggingface/unity-api.git\" and click \"Add\".\n5. Once the package is installed, the Hugging Face API", "retrieved_docs": ["## Conclusion\n\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!", "--\ntitle: \"How to Install and Use the Hugging Face Unity API\"\nthumbnail: /blog/assets/124_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n\n# How to Install and Use the Hugging Face Unity API\n\n<!-- {authors} --> \n\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API.\n\n## Installation\n\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif\">\n</figure>", "## Step 6: Optionally, Download the Hugging Face Unity WebGL Template\n\nYou can enhance your game's appearance in a Space by downloading the Hugging Face Unity WebGL template, available [here](https://github.com/huggingface/Unity-WebGL-template-for-Hugging-Face-Spaces). Just download the repository and drop it in your project files.\n\nThen, in the Player Settings panel, switch the WebGL template to Hugging Face. To do so, in Player Settings, click \"Resolution and Presentation\", then select the Hugging Face WebGL template.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/6.png\">\n</figure> \n\n## Step 7: Change the Compression Format to Disabled\n\nIn the Player Settings panel, navigate to the \"Publishing Settings\" section and change the Compression Format to \"Disabled\".\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/7.png\">\n</figure> \n\n## Step 8: Build your Project\n\nReturn to the Build Settings window and click the \"Build\" button. Choose a location to save your build files, and Unity will build the project for WebGL.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/8.png\">\n</figure> \n\n## Step 9: Copy the Contents of the Build Folder", "6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n7. Test the API key by clicking `Test API key` in the API Wizard.\n8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.\n9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png\">\n</figure> \n\nNow that the API is set up, you can make calls from your scripts to the API. Let's look at an example of performing a Sentence Similarity task:\n\n```\nusing HuggingFace.API;\n\n/* other code */\n\n// Make a call to the API\nvoid Query() {\n    string inputText = \"I'm on my way to the forest.\";\n    string[] candidates = {\n        \"The player is going to the city\",\n        \"The player is going to the wilderness\",\n        \"The player is wandering aimlessly\"\n    };\n    HuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\n}", "## Next steps\n\nOnce `huggingface_hub` is properly installed on your machine, you might want\n[configure environment variables](package_reference/environment_variables) or [check one of our guides](guides/overview) to get started.", "If you need help with the integration, feel free to open an [issue](https://github.com/huggingface/huggingface_hub/issues/new/choose), and we would be more than happy to help you.\n\n## Installation\n\n1. Install the `huggingface_hub` library with pip in your environment:\n\n   ```bash\n   python -m pip install huggingface_hub\n   ```\n\n2. Once you have successfully installed the `huggingface_hub` library, log in to your Hugging Face account:\n\n   ```bash\n   huggingface-cli login\n   ```\n\n   ```bash\n        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n        \n   Username: \n   Password:\n   ```\n\n3. Alternatively, if you prefer working from a Jupyter or Colaboratory notebook, login with `notebook_login`:\n\n   ```python\n   >>> from huggingface_hub import notebook_login\n   >>> notebook_login()\n   ```\n\n   `notebook_login` will launch a widget in your notebook from which you can enter your Hugging Face credentials.\n\n## Download files from the Hub", "These lightly wrap around the API Endpoints. Documentation for valid parameters and descriptions can be found [here](https://huggingface.co/docs/hub/endpoints).\n  \n\n### Advanced programmatic repository management \n\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed.\n\nInstantiate a `Repository` object by calling it with a path to a local Git\nclone/repository:\n\n```python\n>>> from huggingface_hub import Repository\n>>> repo = Repository(\"<path>/<to>/<folder>\")\n```\n\nThe `Repository` takes a `clone_from` string as parameter. This can stay as\n`None` for offline management, but can also be set to any URL pointing to a Git\nrepo to clone that repository in the specified directory:\n\n```python\n>>> repo = Repository(\"huggingface-hub\", clone_from=\"https://github.com/huggingface/huggingface_hub\")\n```\n\nThe `clone_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n\n```python\n>>> repo = Repository(\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\n```\n\nIf the repository you're cloning is one of yours or one of your organisation's,\nthen having the ability to commit and push to that repository is important. In\norder to do that, you should make sure to be logged-in using `huggingface-cli\nlogin`, and to have the `token` parameter set to `True` (the default)\nwhen  instantiating the `Repository` object:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It follows the same steps as the reference answer, including opening the Unity project, navigating to the \"Package Manager\" through the \"Window\" menu, and adding a package from a git URL. The URL provided is accurate and would lead to the installation of the Hugging Face Unity API. However, the response is incomplete as it ends abruptly without finishing the last sentence. This leaves out any final confirmation or steps that might be needed after the package installation. Based on the score rubric, the response is not completely correct due to this incomplete information, but it is mostly correct as it covers the essential steps accurately."}, {"question": "What is the pretraining objective of the Wav2Vec2 context network?\n", "true_answer": "The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks_explained.md", "generated_answer": "The pretraining objective of the Wav2Vec2 context network is contrastive learning over a quantization of the latent representations, which is solved by masks in the latent space. This is described in the abstract of the paper \"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\" (https://arxiv.org/abs/2006.11477). The pretrained Wav2", "retrieved_docs": ["The abstract from the paper is the following:\n\n*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\nrecognition with limited amounts of labeled data.*\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n\n## Usage tips\n\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n- Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\n  using [`Wav2Vec2CTCTokenizer`].\n\n## Resources", "--\ntitle: \"Boosting Wav2Vec2 with n-grams in \ud83e\udd17 Transformers\"\nthumbnail: /blog/assets/44_boost_wav2vec2_ngram/wav2vec2_ngram.png\nauthors:\n- user: patrickvonplaten\n---\n\n# Boosting Wav2Vec2 with n-grams in \ud83e\udd17 Transformers\n\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Boosting_Wav2Vec2_with_n_grams_in_Transformers.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n**Wav2Vec2** is a popular pre-trained model for speech recognition.\nReleased in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Meta AI Research, the novel architecture catalyzed progress in\nself-supervised pretraining for speech recognition, *e.g.* [*G. Ng et\nal.*, 2021](https://arxiv.org/pdf/2104.03416.pdf), [*Chen et al*,\n2021](https://arxiv.org/abs/2110.13900), [*Hsu et al.*,\n2021](https://arxiv.org/abs/2106.07447) and [*Babu et al.*,\n2021](https://arxiv.org/abs/2111.09296). On the Hugging Face Hub,\nWav2Vec2's most popular pre-trained checkpoint currently amounts to\nover [**250,000** monthly\ndownloads](https://huggingface.co/facebook/wav2vec2-base-960h).\n\nUsing Connectionist Temporal Classification (CTC), pre-trained\nWav2Vec2-like checkpoints are extremely easy to fine-tune on downstream\nspeech recognition tasks. In a nutshell, fine-tuning pre-trained\nWav2Vec2 checkpoints works as follows:", "### Create Wav2Vec2CTCTokenizer\n\nThe [pretrained Wav2Vec2 checkpoint](https://huggingface.co/facebook/wav2vec2-base) maps the speech signal to a\nsequence of context representations as illustrated in the figure above.\nA fine-tuned Wav2Vec2 checkpoint needs to map this sequence of context\nrepresentations to its corresponding transcription so that a linear\nlayer has to be added on top of the transformer block (shown in yellow).\nThis linear layer is used to classifies each context representation to a\ntoken class analogous how, *e.g.*, after pretraining a linear layer is\nadded on top of BERT\\'s embeddings for further classification - *cf.*\nwith *\\\"BERT\\\"* section of this [blog post](https://huggingface.co/blog/warm-starting-encoder-decoder).\n\nThe output size of this layer corresponds to the number of tokens in the\nvocabulary, which does **not** depend on Wav2Vec2\\'s pretraining task,\nbut only on the labeled dataset used for fine-tuning. So in the first\nstep, we will take a look at Timit and define a vocabulary based on the\ndataset\\'s transcriptions.\n\nLet\\'s start by loading the dataset and taking a look at its structure.\n\n```python\nfrom datasets import load_dataset, load_metric\n\ntimit = load_dataset(\"timit_asr\")\n\nprint(timit)\n```", "--\ntitle: \"Making automatic speech recognition work on large files with Wav2Vec2 in \ud83e\udd17 Transformers\"\nthumbnail: /blog/assets/49_asr_chunking/thumbnail.png\nauthors:\n- user: Narsil\n---\n\n# Making automatic speech recognition work on large files with Wav2Vec2 in \ud83e\udd17 Transformers\n\n\n```\nTl;dr: This post explains how to use the specificities of the Connectionist\nTemporal Classification (CTC) architecture in order to achieve very good\nquality automatic speech recognition (ASR) even on arbitrarily long files or \nduring live inference.\n```\n\n\n**Wav2Vec2** is a popular pre-trained model for speech recognition.\nReleased in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Meta AI Research, the novel architecture catalyzed progress in\nself-supervised pretraining for speech recognition, *e.g.* [*G. Ng et\nal.*, 2021](https://arxiv.org/pdf/2104.03416.pdf), [*Chen et al*,\n2021](https://arxiv.org/abs/2110.13900), [*Hsu et al.*,\n2021](https://arxiv.org/abs/2106.07447) and [*Babu et al.*,\n2021](https://arxiv.org/abs/2111.09296). On the Hugging Face Hub,\nWav2Vec2's most popular pre-trained checkpoint currently amounts to\nover [**250,000** monthly\ndownloads](https://huggingface.co/facebook/wav2vec2-base-960h).", "For the first time, it has been shown that pretraining, followed by\nfine-tuning on very little labeled speech data achieves competitive\nresults to state-of-the-art ASR systems. Using as little as 10 minutes\nof labeled data, Wav2Vec2 yields a word error rate (WER) of less than 5%\non the clean test set of\n[LibriSpeech](https://huggingface.co/datasets/librispeech_asr) - *cf.*\nwith Table 9 of the [paper](https://arxiv.org/pdf/2006.11477.pdf).\n\nIn this notebook, we will give an in-detail explanation of how\nWav2Vec2\\'s pretrained checkpoints can be fine-tuned on any English ASR\ndataset. Note that in this notebook, we will fine-tune Wav2Vec2 without\nmaking use of a language model. It is much simpler to use Wav2Vec2\nwithout a language model as an end-to-end ASR system and it has been\nshown that a standalone Wav2Vec2 acoustic model achieves impressive\nresults. For demonstration purposes, we fine-tune the \\\"base\\\"-sized\n[pretrained checkpoint](https://huggingface.co/facebook/wav2vec2-base)\non the rather small [Timit](https://huggingface.co/datasets/timit_asr)\ndataset that contains just 5h of training data.\n\nWav2Vec2 is fine-tuned using Connectionist Temporal Classification\n(CTC), which is an algorithm that is used to train neural networks for\nsequence-to-sequence problems and mainly in Automatic Speech Recognition\nand handwriting recognition.", "-   [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n    Representations](https://arxiv.org/abs/2006.11477)\n-   [Fine-Tune Wav2Vec2 for English ASR with \ud83e\udd17\n    Transformers](https://huggingface.co/blog/fine-tune-wav2vec2-english)\n-   [An Illustrated Tour of Wav2vec\n    2.0](https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html)\n\n## **1. Decoding audio data with Wav2Vec2 and a language model**\n\nAs shown in \ud83e\udd17 Transformers [exemple docs of\nWav2Vec2](https://huggingface.co/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC),\naudio can be transcribed as follows.\n\nFirst, we install `datasets` and `transformers`.\n\n```bash\npip install datasets transformers\n```\n\nLet's load a small excerpt of the [Librispeech\ndataset](https://huggingface.co/datasets/librispeech_asr) to demonstrate\nWav2Vec2's speech transcription capabilities.\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\ndataset\n```\n\n**Output:**\n```bash\n    Reusing dataset librispeech_asr (/root/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)\n\n    Dataset({\n        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n        num_rows: 73\n    })\n```\n\nWe can pick one of the 73 audio samples and listen to it.\n\n```python\naudio_sample = dataset[2]\naudio_sample[\"text\"].lower()\n```", "The abstract from the paper is the following:\n\n*This paper is a study of performance-efficiency trade-offs in pre-trained models for automatic speech recognition\n(ASR). We focus on wav2vec 2.0, and formalize several architecture designs that influence both the model performance\nand its efficiency. Putting together all our observations, we introduce SEW (Squeezed and Efficient Wav2vec), a\npre-trained model architecture with significant improvements along both performance and efficiency dimensions across a\nvariety of training setups. For example, under the 100h-960h semi-supervised setup on LibriSpeech, SEW achieves a 1.9x\ninference speedup compared to wav2vec 2.0, with a 13.5% relative reduction in word error rate. With a similar inference\ntime, SEW reduces word error rate by 25-50% across different model sizes.*\n\nThis model was contributed by [anton-l](https://huggingface.co/anton-l).\n\n## Usage tips\n\n- SEW-D is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n- SEWDForCTC is fine-tuned using connectionist temporal classification (CTC) so the model output has to be decoded\n  using [`Wav2Vec2CTCTokenizer`].\n\n## Resources\n\n- [Audio classification task guide](../tasks/audio_classification)\n- [Automatic speech recognition task guide](../tasks/asr)\n\n## SEWDConfig\n\n[[autodoc]] SEWDConfig\n\n## SEWDModel\n\n[[autodoc]] SEWDModel\n    - forward\n\n## SEWDForCTC\n\n[[autodoc]] SEWDForCTC\n    - forward\n\n## SEWDForSequenceClassification"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the pretraining objective of the Wav2Vec2 context network as contrastive learning and mentions the use of masks in the latent space, which aligns with the reference answer's mention of a contrastive task involving masked predictions. However, the response does not explicitly mention the prediction of the true quantized speech representation from a set of false ones, which is a key detail in the reference answer. Therefore, while the response is mostly correct, it lacks the specificity of the reference answer in describing the contrastive task."}, {"question": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n", "true_answer": "distilbert base uncased finetuned sst2 english", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md", "generated_answer": "The default checkpoint used by the sentiment analysis pipeline in the Transformers library is distilbert base uncased finetuned sst2 english. (Answer found in both context extracts provided.)", "retrieved_docs": [". Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem", ". Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a TensorFlow tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an TFAutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the TFAutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem", "## The Transformers library\nI was a bit confused at the beginning when I downloaded the .h5 file. I thought it would be compatible with `tensorflow.keras.models.load_model`, but this wasn't the case. After a few minutes of research I was able to figure out that the file was a weights checkpoint rather than a Keras model.\nAfter that, I tried out the API that Hugging Face offers and read a bit more about the pipeline feature they offer. Since the results of the API & the pipeline were great, I decided that I could serve the model through the pipeline on my own server.\n\nBelow is the [official example](https://github.com/huggingface/transformers#quick-tour) from the Transformers GitHub page.\n\n```python\nfrom transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('We are very happy to include pipeline into the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9978193640708923}]\n```\n\n\n## Deploy transformers to Google Cloud\n> GCP is chosen as it is the cloud environment I am using in my personal organization.", "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint", "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint", "```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\"\n```\n\nThen print out the label and score:\n\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n\n## Install from source\n\nInstall \ud83e\udd17 Transformers from source with the following command:\n\n```bash\npip install git+https://github.com/huggingface/transformers\n```\n\nThis command installs the bleeding edge `main` version rather than the latest `stable` version. The `main` version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn't been rolled out yet. However, this means the `main` version may not always be stable. We strive to keep the `main` version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an [Issue](https://github.com/huggingface/transformers/issues) so we can fix it even sooner!\n\nCheck if \ud83e\udd17 Transformers has been properly installed by running the following command:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n```\n\n## Editable install\n\nYou will need an editable install if you'd like to:\n\n* Use the `main` version of the source code.\n* Contribute to \ud83e\udd17 Transformers and need to test changes in the code.\n\nClone the repository and install \ud83e\udd17 Transformers with the following commands:", "Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this guide, you'll use the [`pipeline`] for sentiment analysis as an example:\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(\"sentiment-analysis\")\n```\n\nThe [`pipeline`] downloads and caches a default [pretrained model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) and tokenizer for sentiment analysis. Now you can use the `classifier` on your target text:\n\n```py\n>>> classifier(\"We are very happy to show you the \ud83e\udd17 Transformers library.\")\n[{'label': 'POSITIVE', 'score': 0.9998}]\n```\n\nIf you have more than one input, pass your inputs as a list to the [`pipeline`] to return a list of dictionaries:\n\n```py\n>>> results = classifier([\"We are very happy to show you the \ud83e\udd17 Transformers library.\", \"We hope you don't hate it.\"])\n>>> for result in results:\n...     print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\nlabel: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.5309\n```\n\nThe [`pipeline`] can also iterate over an entire dataset for any task you like. For this example, let's choose automatic speech recognition as our task:\n\n```py\n>>> import torch\n>>> from transformers import pipeline\n\n>>> speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual as it matches the reference answer exactly."}, {"question": "What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n", "true_answer": "To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.", "source_doc": "huggingface/optimum/blob/main/docs/source/notebooks.md", "generated_answer": "The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to guide users on how to leverage DeepSpeed, an open-source deep learning optimization library integrated with \ud83e\udd17 Transformers and \ud83e\udd17 Accelerate, for training large models with billions of parameters on Habana Gaudi, a high-performance computing accelerator. The notebook provides instructions on how to configure the script", "retrieved_docs": ["--\ntitle: \"Accelerate Large Model Training using DeepSpeed\"\nthumbnail: /blog/assets/83_accelerate_deepspeed/deepspeed-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n\n# Accelerate Large Model Training using DeepSpeed\n\n\nIn this post we will look at how we can leverage the **[Accelerate](https://github.com/huggingface/accelerate)** library for training large models which enables users to leverage the ZeRO features of **[DeeSpeed](https://www.deepspeed.ai)**.\n\n# Motivation \ud83e\udd17\n**Tired of Out of Memory (OOM) errors while trying to train large models? We've got you covered. Large models are very performant [1] but difficult to train with the available hardware. To get the most of the available hardware for training large models one can leverage Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]**. \n\nBelow is a short description of Data Parallelism using ZeRO with diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)\n![ZeRO Data Parallelism](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png)\n\n(Source: [link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/))\n\n a. **Stage 1** : Shards optimizer states across data parallel workers/GPUs", "## DeepSpeed ZeRO\n\nDeepSpeed is an open-source deep learning optimization library that is integrated with \ud83e\udd17 Transformers and \ud83e\udd17 Accelerate.\nIt provides a wide range of features and optimizations designed to improve the efficiency and scalability of large-scale \ndeep learning training.\n\nIf your model fits onto a single GPU and you have enough space to fit a small batch size, you don't need to use DeepSpeed\nas it'll only slow things down. However, if the model doesn't fit onto a single GPU or you can't fit a small batch, you can \nleverage DeepSpeed ZeRO + CPU Offload, or NVMe Offload for much larger models. In this case, you need to separately\n[install the library](main_classes/deepspeed#installation), then follow one of the guides to create a configuration file \nand launch DeepSpeed: \n \n* For an in-depth guide on DeepSpeed integration with [`Trainer`], review [the corresponding documentation](main_classes/deepspeed), specifically the \n[section for a single GPU](main_classes/deepspeed#deployment-with-one-gpu). Some adjustments are required to use DeepSpeed in a notebook; please take a look at the [corresponding guide](main_classes/deepspeed#deployment-in-notebooks).\n* If you prefer to use \ud83e\udd17 Accelerate, refer to [\ud83e\udd17 Accelerate DeepSpeed guide](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed).\n\n## Using torch.compile", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# DeepSpeed\n\n[DeepSpeed](https://www.deepspeed.ai/) is a library designed for speed and scale for distributed training of large models with billions of parameters. At its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes. This drastically reduces memory usage, allowing you to scale your training to billion parameter models. To unlock even more memory efficiency, ZeRO-Offload reduces GPU compute and memory by leveraging CPU resources during optimization.\n\nBoth of these features are supported in \ud83e\udd17 Accelerate, and you can use them with \ud83e\udd17 PEFT. This guide will help you learn how to use our DeepSpeed [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py). You'll configure the script to train a large model for conditional generation with ZeRO-3 and ZeRO-Offload.\n\n<Tip>", "* Paper: [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054). The paper is very interesting, but it's very terse.\n* Here is a good [video discussion](https://www.youtube.com/watch?v=tC01FRB0M7w) of the paper with visuals\n* Paper: [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840). Just published - this one goes into the details of ZeRO Offload feature.\n* DeepSpeed [configuration and tutorials](https://www.deepspeed.ai/getting-started/)\n* In addition to the paper, I highly recommend to read the following detailed blog posts with diagrams:\n  - [DeepSpeed: Extreme-scale model training for everyone]( https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n  - [ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)\n  - [Turing-NLG: A 17-billion-parameter language model by Microsoft](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)\n* DeepSpeed [examples on GitHub](https://github.com/microsoft/DeepSpeedExamples)\n\n# Gratitude", "### Fine-tuning T5-3B\n\nWith 96 GB of memory per device, Gaudi2 enables running much bigger models. For instance, we managed to fine-tune T5-3B (containing 3 billion parameters) with gradient checkpointing being the only applied memory optimization. This is not possible on first-gen Gaudi.\n[Here](https://huggingface.co/regisss/t5-3b-summarization-gaudi-2/tensorboard?scroll=1#scalars) are the logs of this run where the model was fine-tuned on the CNN DailyMail dataset for text summarization using [this script](https://github.com/huggingface/optimum-habana/tree/main/examples/summarization).\n\nThe results we achieved are presented in the table below. **Gaudi2 is x2.44 faster than A100 80GB.** We observe that we cannot fit a batch size larger than 1 on Gaudi2 here. This is due to the memory space taken by the graph where operations are accumulated during the first iteration of the run. Habana is working on optimizing the memory footprint in future releases of SynapseAI. We are looking forward to expanding this benchmark using newer versions of Habana's SDK and also using [DeepSpeed](https://www.deepspeed.ai/) to see if the same trend holds.\n\n<center>\n\n|   | First-gen Gaudi | Gaudi2 (BS=1) | A100 (BS=16) |\n|:-:|:-------:|:--------------:|:------------:|\n| Throughput (samples/s) | N/A | 19.7 | 8.07 |\n| Speedup | / | x2.44 | x1.0 |\n\n</center>", "Moreover, support for [HPU graphs](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Using_HPU_Graphs.html) and [DeepSpeed-inference](https://docs.habana.ai/en/latest/PyTorch/DeepSpeed/Inference_Using_DeepSpeed.html) have just recently been introduced in SynapseAI, and these are well-suited for latency-sensitive applications as shown in our benchmark below.\n\nAll these features are integrated into the \ud83e\udd17 [Optimum Habana](https://github.com/huggingface/optimum-habana) library so that deploying your model on Gaudi is very simple. Check out the quick-start page [here](https://huggingface.co/docs/optimum/habana/quickstart).\n\nIf you would like to get access to Gaudi2, go to the [Intel Developer Cloud](https://www.intel.com/content/www/us/en/secure/developer/devcloud/cloud-launchpad.html) and follow [this guide](https://huggingface.co/blog/habana-gaudi-2-benchmark#how-to-get-access-to-gaudi2).\n\n\n## Benchmarks", "The Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. **For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB**. Smaller checkpoints present interesting results too. **Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!** It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device.\n\nWe also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, **first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100** which costs more than 30\\$ per hour!\n\nWe expect the Habana team will optimize the performance of these models in the upcoming SynapseAI releases. For example, in our last benchmark, we saw that [Gaudi2 performs Stable Diffusion inference 2.2x faster than A100](https://huggingface.co/blog/habana-gaudi-2-benchmark#generating-images-from-text-with-stable-diffusion) and this has since been improved further to 2.37x with the latest optimizations provided by Habana. We will update these numbers as new versions of SynapseAI are released and integrated within Optimum Habana.\n\n\n### Running inference on a complete dataset"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response correctly identifies the purpose of the notebook as a guide for using DeepSpeed to train large models on Habana Gaudi. However, it lacks the specificity of the reference answer, which mentions pre-training/fine-tuning a specific model (1.6B-parameter GPT2-XL) and the task (causal language modeling). The evaluated response is somewhat correct but not as detailed and precise as the reference answer."}, {"question": "What command line module does PyTorch provide to run a script on multiple GPUs?\n", "true_answer": "torchrun", "source_doc": "huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md", "generated_answer": "PyTorch provides a convenient `torchrun` command line module to run a script on multiple GPUs. The command should be passed the number of nodes and the script to run, like this: `torchrun --nproc_per_node=2 --nnodes=1 example_script.py`. This will run the training script on two GPUs that live on a single machine, and this is the barebones for performing only distributed training with PyTorch.", "retrieved_docs": ["Lastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\n\n```bash\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\n```\n\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\n\nNow let's talk about Accelerate, a library aimed to make this process more seameless and also help with a few best practices\n\n## \ud83e\udd17 Accelerate\n\n[Accelerate](https://huggingface.co/docs/accelerate) is a library designed to allow you to perform what we just did above, without needing to modify your code greatly. On top of this, the data pipeline innate to Accelerate can also improve performance to your code as well.\n\nFirst, let's wrap all of the above code we just performed into a single function, to help us visualize the difference:\n\n```python\ndef train_ddp(rank, world_size):\n    setup(rank, world_size)\n    # Build DataLoaders\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307), (0.3081))\n    ])\n\n    train_dset = datasets.MNIST('data', train=True, download=True, transform=transform)\n    test_dset = datasets.MNIST('data', train=False, transform=transform)", "```bash\n# adapt the path to your system\nsudo ln -s /usr/bin/gcc-7  /usr/local/cuda-10.2/bin/gcc\nsudo ln -s /usr/bin/g++-7  /usr/local/cuda-10.2/bin/g++\n```\n\n## Multi-GPU Network Issues Debug\n\nWhen training or inferencing with `DistributedDataParallel` and multiple GPU, if you run into issue of inter-communication between processes and/or nodes, you can use the following script to diagnose network issues.\n\n```bash\nwget https://raw.githubusercontent.com/huggingface/transformers/main/scripts/distributed/torch-distributed-gpu-test.py\n```\n\nFor example to test how 2 GPUs interact do:\n\n```bash\npython -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py\n```\nIf both processes can talk to each and allocate GPU memory each will print an OK status.\n\nFor more GPUs or nodes adjust the arguments in the script.\n\nYou will find a lot more details inside the diagnostics script and even a recipe to how you could run it in a SLURM environment.\n\nAn additional level of debug is to add `NCCL_DEBUG=INFO` environment variable as follows:\n\n```bash\nNCCL_DEBUG=INFO python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py\n```\n\nThis will dump a lot of NCCL-related debug information, which you can then search online if you find that some problems are reported. Or if you're not sure how to interpret the output you can share the log file in an Issue.\n\n\n\n## Underflow and Overflow Detection\n\n<Tip>", "Now we can fit very large models into a single GPU, but the training might still be very slow. The simplest strategy in this scenario is data parallelism: we replicate the same training setup into separate GPUs and pass different batches to each GPU. With this, you can parallelize the forward/backward passes of the model and scale with the number of GPUs. \n\n![chapter10_ddp.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/chapter10_ddp.png)\n\nWe use either the `transformers.Trainer` or `accelerate`, which both support data parallelism without any code changes, by simply passing arguments when calling the scripts with `torchrun` or `accelerate launch`. The following runs a training script with 8 GPUs on a single machine with `accelerate` and `torchrun`, respectively.\n\n```bash\naccelerate launch --multi_gpu --num_machines 1  --num_processes 8 my_accelerate_script.py\ntorchrun --nnodes 1  --nproc_per_node 8 my_torch_script.py\n```\n\n## Supervised fine-tuning", "</Tip>\n\n## PyTorch Distributed\n\nPyTorch supports [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) which enables data parallelism.\n\nTo start, create a Python file and import `torch.distributed` and `torch.multiprocessing` to set up the distributed process group and to spawn the processes for inference on each GPU. You should also initialize a [`DiffusionPipeline`]:\n\n```py\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\nfrom diffusers import DiffusionPipeline\n\nsd = DiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n)\n```\n\nYou'll want to create a function to run inference; [`init_process_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group) handles creating a distributed environment with the type of backend to use, the `rank` of the current process, and the `world_size` or the number of processes participating. If you're running inference in parallel over 2 GPUs, then the `world_size` is 2.\n\nMove the [`DiffusionPipeline`] to `rank` and use `get_rank` to assign a GPU to each process, where each process handles a different prompt:\n\n```py\ndef run_inference(rank, world_size):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n    sd.to(rank)", "Build a PyTorch optimizer:\n\n```python\noptimizer = optim.AdamW(model.parameters(), lr=1e-3)\n```\n\nBefore finally creating a simplistic training and evaluation loop that performs one full iteration over the dataset and calculates the test accuracy:\n\n```python\nmodel.train()\nfor batch_idx, (data, target) in enumerate(train_loader):\n    data, target = data.to(device), target.to(device)\n    output = model(data)\n    loss = F.nll_loss(output, target)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\nmodel.eval()\ncorrect = 0\nwith torch.no_grad():\n    for data, target in test_loader:\n        output = model(data)\n        pred = output.argmax(dim=1, keepdim=True)\n        correct += pred.eq(target.view_as(pred)).sum().item()\nprint(f'Accuracy: {100. * correct / len(test_loader.dataset)}')\n```\n\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook.\n\nHowever, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through *distributed* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\n\n## PyTorch Distributed Data Parallelism", "loss.backward()\n\n          optimizer.step()\n```\n\nThese changes will make your training script work for multiple GPUs, but your script will then stop working on CPU or one GPU (unless you start adding if statements everywhere). Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of \ud83e\udd17 Accelerate is:\n- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible.\n- to have the same functions work for any distributed setup, so only have to learn one API.\n\n### How does it work?\n\nTo see how the library works in practice, let's have a look at each line of code we need to add to a training loop.\n\n```python\naccelerator = Accelerator()\n```\n\nOn top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script.\n\n```python\nmodel, optim, data = accelerator.prepare(model, optim, data)\n```\n\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torch.nn.Module`), optimizers (`torch.optim.Optimizer`) and dataloaders (`torch.data.dataloader.DataLoader`).\n\n#### Model", "Backends for [distributed setups](https://pytorch.org/docs/stable/distributed.html#backends) like `gloo` and `nccl` are not supported by the `mps` device which means you can only train on a single GPU with the MPS backend.\n\nYou can learn more about the MPS backend in the [Introducing Accelerated PyTorch Training on Mac](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/) blog post."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies `torchrun` as the command line module provided by PyTorch to run a script on multiple GPUs, which is in alignment with the reference answer. The response also accurately describes how to use the command by providing an example with the correct syntax for specifying the number of processes per node and the number of nodes, which demonstrates a factual understanding of the module's usage."}, {"question": "What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n", "true_answer": "google/vit-base-patch16-224", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-with-vision-transformers.md", "generated_answer": "The most popular vision transformer model for image classification on the Hugging Face Model Hub is the one contributed by nielsr, which is based on the original Vision Transformer (ViT) introduced in the paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" by Alexey Dosovitskiy et al. This model, also known as ViT, has shown excellent results compared to state-of", "retrieved_docs": ["1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n1. **[ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VitDet](https://huggingface.co/docs/transformers/model_doc/vitdet)** (from Meta AI) released with the paper [Exploring Plain Vision Transformer Backbones for Object Detection](https://arxiv.org/abs/2203.16527) by Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He.", "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n1. **[ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VitDet](https://huggingface.co/docs/transformers/model_doc/vitdet)** (from Meta AI) released with the paper [Exploring Plain Vision Transformer Backbones for Object Detection](https://arxiv.org/abs/2203.16527) by Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He.", "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n1. **[ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VitDet](https://huggingface.co/docs/transformers/model_doc/vitdet)** (from Meta AI) released with the paper [Exploring Plain Vision Transformer Backbones for Object Detection](https://arxiv.org/abs/2203.16527) by Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He.", "Image Classification with Vision Transformers\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/vision-transformer\nTags: VISION, TRANSFORMERS, HUB\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control.\n\nState-of-the-art image classifiers are based on the _transformers_ architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like the demo on the bottom of the page.\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started).\n\n## Step 1 \u2014 Choosing a Vision Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.", "The abstract from the paper is the following:\n\n*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its\napplications to computer vision remain limited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional networks while keeping their overall\nstructure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),\nVision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring\nsubstantially fewer computational resources to train.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code (written in JAX) can be\nfound [here](https://github.com/google-research/vision_transformer).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with ViT Hybrid.\n\n<PipelineTag pipeline=\"image-classification\"/>", "The abstract from the paper is the following:\n\n*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its\napplications to computer vision remain limited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional networks while keeping their overall\nstructure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),\nVision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring\nsubstantially fewer computational resources to train.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> ViT architecture. Taken from the <a href=\"https://arxiv.org/abs/2010.11929\">original paper.</a> </small>\n\nFollowing the original Vision Transformer, some follow-up works have been made:", "## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive."], "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response does not provide the specific model name as found in the reference answer, which is \"google/vit-base-patch16-224\". Instead, it gives a general description of the type of model and references the original paper, which does not directly answer the question about the most popular model on the Hugging Face Model Hub. The response is somewhat correct in that it identifies the model as a Vision Transformer (ViT) and mentions its popularity, but it fails to provide the exact model name that is requested. Therefore, the response is somewhat correct, accurate, and factual, but not completely as per the score rubric."}, {"question": "What is the command to upload an ESPnet model to a Hugging Face repository?\n", "true_answer": "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/espnet.md", "generated_answer": "To upload an ESPnet model to a Hugging Face repository, you can use the `mlagents-push-to-hf` command provided by the ML-Agents library. Here's the command:\n\n```bash\nmlagents-push-to-hf --run-id=\"First Training\" --local-dir=\"results/First Training\" --repo-id=\"ThomasSimonini/MLAgents-Pyramids\" --commit-", "retrieved_docs": ["Using ESPnet at Hugging Face\n\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\n\n## Exploring ESPnet in the Hub\n\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). \n\nAll models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more.\n2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\n3. An interactive widget you can use to play out with the model directly in the browser.\n4. An Inference API that allows to make inference requests.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png\"/>\n</div>\n\n## Using existing models\n\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo)).", "You need to define two parameters:\n- `--repo-id`: the name of the Hugging Face repo you want to download.\n- `--local-dir`: the path to download the model.\n\n## Visualize an agent playing\n\nYou can easily watch any model playing directly in your browser:\n\n1. Go to your model repo.\n2. In the `Watch Your Agent Play` section, click on the link.\n3. In the demo, on step 1, choose your model repository, which is the model id.\n4. In step 2, choose what model you want to replay.\n\n## Sharing your models\n\nYou can easily upload your models using `mlagents-push-to-hf`:\n\n```\nmlagents-push-to-hf --run-id=\"First Training\" --local-dir=\"results/First Training\" --repo-id=\"ThomasSimonini/MLAgents-Pyramids\" --commit-message=\"Pyramids\"\n```\n\nYou need to define four parameters:\n- `--run-id`: the name of the training run id.\n- `--local-dir`: where the model was saved.\n- `--repo-id`: the name of the Hugging Face repo you want to create or update. It\u2019s `<your huggingface username>/<the repo name>`.\n- `--commit-message`.\n\n\n## Additional resources\n\n* ML-Agents [documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Hugging-Face-Integration.md)\n* Official Unity ML-Agents Spaces [demos](https://huggingface.co/unity)", "Next, push it to the hub!\n\n```python\n# assuming you have ran the huggingface-cli login command in a terminal\ndataset.push_to_hub(\"name_of_your_dataset\")\n\n# if you want to push to a private repo, simply pass private=True:\ndataset.push_to_hub(\"name_of_your_dataset\", private=True)\n```\n\nand that's it! You can now train your model by simply setting the `--dataset_name` argument to the name of your dataset on the hub.\n\nMore on this can also be found in [this blog post](https://huggingface.co/blog/image-search-datasets).", "<!--End of the generated tip-->\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate\n```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load SQuAD dataset\n\nStart by loading a smaller subset of the SQuAD dataset from the \ud83e\udd17 Datasets library. This'll give you a chance to experiment and make sure everything works before spending more time training on the full dataset.\n\n```py\n>>> from datasets import load_dataset\n\n>>> squad = load_dataset(\"squad\", split=\"train[:5000]\")\n```\n\nSplit the dataset's `train` split into a train and test set with the [`~datasets.Dataset.train_test_split`] method:\n\n```py\n>>> squad = squad.train_test_split(test_size=0.2)\n```\n\nThen take a look at an example:", "## Upload the trained/fine-tuned model to the Hub\n\nAll the example scripts support automatic upload of your final model to the [Model Hub](https://huggingface.co/models) by adding a `--push_to_hub` argument. It will then create a repository with your username slash the name of the folder you are using as `output_dir`. For instance, `\"sgugger/test-mrpc\"` if your username is `sgugger` and you are working in the folder `~/tmp/test-mrpc`.\n\nTo specify a given repository name, use the `--hub_model_id` argument. You will need to specify the whole repository name (including your username), for instance `--hub_model_id sgugger/finetuned-bert-mrpc`. To upload to an organization you are a member of, just use the name of that organization instead of your username: `--hub_model_id huggingface/finetuned-bert-mrpc`.\n\nA few notes on this integration:\n\n- you will need to be logged in to the Hugging Face website locally for it to work, the easiest way to achieve this is to run `huggingface-cli login` and then type your username and password when prompted. You can also pass along your authentication token with the `--hub_token` argument.\n- the `output_dir` you pick will either need to be a new folder or a local clone of the distant repository you are using.", "<!--End of the generated tip-->\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate accelerate\n```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load IMDb dataset\n\nStart by loading the IMDb dataset from the \ud83e\udd17 Datasets library:\n\n```py\n>>> from datasets import load_dataset\n\n>>> imdb = load_dataset(\"imdb\")\n```\n\nThen take a look at an example:", "```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\n## Additional resources\n\n* ESPnet [docs](https://espnet.github.io/espnet/index.html).\n* ESPnet model zoo [repository](https://github.com/espnet/espnet_model_zoo).\n* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md)."], "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response provided is incorrect. The command `mlagents-push-to-hf` is not related to ESPnet but to the ML-Agents library, which is used for training intelligent agents in Unity. The correct command for uploading an ESPnet model to a Hugging Face repository is not mentioned in the response. The reference answer provides the correct command, which is `./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo`. The response does not align with the reference answer and fails to provide the correct information for the task of uploading an ESPnet model to a Hugging Face repository."}, {"question": "What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n", "true_answer": "requirements.txt", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/custom_dependencies.mdx", "generated_answer": "The file that should be added to a model repository to install custom Python dependencies for Inference Endpoints is a `requirements.txt` file. This file should contain a list of required Python packages and their versions that are not included in the default Inference Endpoints image. When the model repository is pushed to the Hugging Face Hub, Inference Endpoints checks for the presence of this file and installs the listed dependencies during the creation of the Image and Endpoint artifacts. This allows", "retrieved_docs": ["Add custom Dependencies\n\nInference Endpoints\u2019 base image includes all required libraries to run inference on \ud83e\udd17 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\n* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\n* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).\n\nTo add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within.\n\n```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\n\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\n* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\n\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom_container) for inference.", "Inference Endpoints Version\n\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom_handler) and implement batching.\nBelow you will find information about the installed packages and versions used.\n\nYou can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom_dependencies).\n\n## Installed packages & version\n\nThe installed packages are split into `general`, `CPU` & `GPU` packages. The `general` packages are installed in all containers, the `CPU` and `GPU` packages are only installed in the corresponding containers.\nThe Hugging Face Inference Runtime has separate versions for `PyTorch` and `TensorFlow` for `CPU` and `GPU`, which are used based on the selected `framework` when creating an Inference Endpoint. The `TensorFlow` and `PyTorch` flavors are grouped together in the list below.\n\n### General\n\n- `Python`: `3.9.13`\n\n### CPU\n\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `accelerate`: `0.17.1`\n- `sentence_transformers`: `latest`\n- `pandas`: `latest`\n- `pytorch`: `1.13.1`\n- `torchvision`: `0.14.1`\n- `tensorflow`: `2.9.1`\n\n### GPU", "### Download the Model Checkpoints\n\nTo run inference in Python, use one of the versions stored in the `packages` folders because the `compiled` ones are only compatible with Swift. You may choose whether you want to use `original` or `split_einsum` attention.\n\nThis is how you'd download the `original` attention variant from the Hub to a directory called `models`:\n\n```Python\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nrepo_id = \"apple/coreml-stable-diffusion-v1-4\"\nvariant = \"original/packages\"\n\nmodel_path = Path(\"./models\") / (repo_id.split(\"/\")[-1] + \"_\" + variant.replace(\"/\", \"_\"))\nsnapshot_download(repo_id, allow_patterns=f\"{variant}/*\", local_dir=model_path, local_dir_use_symlinks=False)\nprint(f\"Model downloaded at {model_path}\")\n```\n\n### Inference[[python-inference]]\n\nOnce you have downloaded a snapshot of the model, you can test it using Apple's Python script.\n\n```shell\npython -m python_coreml_stable_diffusion.pipeline --prompt \"a photo of an astronaut riding a horse on mars\" -i models/coreml-stable-diffusion-v1-4_original_packages -o </path/to/output/image> --compute-unit CPU_AND_GPU --seed 93\n```\n\nPass the path of the downloaded checkpoint with `-i` flag to the script. `--compute-unit` indicates the hardware you want to allow for inference. It must be one of the following options: `ALL`, `CPU_AND_GPU`, `CPU_ONLY`, `CPU_AND_NE`. You may also provide an optional output path, and a seed for reproducibility.", "# create a Repository instance\nmodel_repo = Repository(use_auth_token=token,\n                        clone_from=repo_url,\n                        local_dir=local_path,\n                        git_user=hf_username,\n                        git_email=hf_email)\n\n# push model to the hub\nmodel_repo.push_to_hub()\n```\n\n---\n\n## Test inference\n\nAfter we uploaded our model we can access it at `https://huggingface.co/{hf_username}/{repository_name}` \n\n```python\nprint(f\"https://huggingface.co/{hf_username}/{repository_name}\")\n```\n\nAnd use the \"Hosted Inference API\" widget to test it. \n\n[https://huggingface.co/philschmid/bart-large-cnn-samsum](https://huggingface.co/philschmid/bart-large-cnn-samsum)\n\n![inference](assets/19_sagemaker_distributed_training_seq2seq/inference-test.png)", "## User defined code and modules\n\nThe Hugging Face Inference Toolkit allows the user to override the default methods of the `HuggingFaceHandlerService`. You will need to create a folder named `code/` with an `inference.py` file in it. See [here](#create-a-model-artifact-for-deployment) for more details on how to archive your model artifacts. For example:  \n\n```bash\nmodel.tar.gz/\n|- pytorch_model.bin\n|- ....\n|- code/\n  |- inference.py\n  |- requirements.txt \n```\n\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:", "For example:\n\n```python\n>>> from huggingface_hub import upload_file\n>>> upload_file(\n...    path_or_fileobj=\"/home/lysandre/dummy-test/README.md\", \n...    path_in_repo=\"README.md\", \n...    repo_id=\"lysandre/test-model\"\n... )\n'https://huggingface.co/lysandre/test-model/blob/main/README.md'\n```\n\nIf you need to upload more than one file, look at the [utilities offered by the `Repository` class](https://huggingface.co/docs/huggingface_hub/package_reference/repository).\n\nOnce again, if you check your Hugging Face account, you should see the file inside your repository.\n\nLastly, it is important to add a model card so users understand how to use your model. See [here](./model-cards) for more details about how to create a model card.\n\n## Set up the Inference API\n\nOur Inference API powers models uploaded to the Hub through your library.\n\n### Create an Inference API Docker image \n\nAll third-party libraries are Dockerized, so you can install the dependencies you'll need for your library to work correctly. Add your library to the existing Docker images by navigating to the [Docker images folder](https://github.com/huggingface/api-inference-community/tree/main/docker_images).", "### Update model or hardware requirements\n\nIn some cases, you might also want to update your Inference Endpoint without creating a new one. You can either update the hosted model or the hardware requirements to run the model. You can do this using [`~InferenceEndpoint.update`]:\n\n```py\n# Change target model\n>>> endpoint.update(repository=\"gpt2-large\")\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2-large', status='pending', url=None)\n\n# Update number of replicas\n>>> endpoint.update(min_replica=2, max_replica=6)\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2-large', status='pending', url=None)\n\n# Update to larger instance\n>>> endpoint.update(accelerator=\"cpu\", instance_size=\"large\", instance_type=\"c6i\")\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2-large', status='pending', url=None)\n```\n\n### Delete the endpoint\n\nFinally if you won't use the Inference Endpoint anymore, you can simply call [`~InferenceEndpoint.delete()`].\n\n<Tip warning={true}>\n\nThis is a non-revertible action that will completely remove the endpoint, including its configuration, logs and usage metrics. You cannot restore a deleted Inference Endpoint.\n\n</Tip>\n\n\n## An end-to-end example\n\nA typical use case of Inference Endpoints is to process a batch of jobs at once to limit the infrastructure costs. You can automate this process using what we saw in this guide:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It correctly identifies the `requirements.txt` file as the one needed in a model repository to install custom Python dependencies for Inference Endpoints. The explanation given about the purpose of the file and its function within the context of the Hugging Face Hub and Inference Endpoints is also accurate and aligns with the reference answer."}, {"question": "How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n", "true_answer": "3-5 images", "source_doc": "huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md", "generated_answer": "According to the provided context, Textual Inversion is a method to teach new concepts to Stable Diffusion using just 3-5 examples. Therefore, 3-5 images are needed to teach new concepts to Stable Diffusion using Textual Inversion.", "retrieved_docs": ["## Textual Inversion\n\n[Paper](https://arxiv.org/abs/2208.01618)\n\n[Textual Inversion](../training/text_inversion) fine-tunes a model to teach it about a new concept. I.e. a few pictures of a style of artwork can be used to generate images in that style.\n\n## ControlNet\n\n[Paper](https://arxiv.org/abs/2302.05543)\n\n[ControlNet](../api/pipelines/controlnet) is an auxiliary network which adds an extra condition.\nThere are 8 canonical pre-trained ControlNets trained on different conditionings such as edge detection, scribbles,\ndepth maps, and semantic segmentations.\n\n## Prompt Weighting\n\n[Prompt weighting](../using-diffusers/weighted_prompts) is a simple technique that puts more attention weight on certain parts of the text\ninput.\n\n## Custom Diffusion\n\n[Paper](https://arxiv.org/abs/2212.04488)\n\n[Custom Diffusion](../training/custom_diffusion) only fine-tunes the cross-attention maps of a pre-trained\ntext-to-image diffusion model. It also allows for additionally performing Textual Inversion. It supports\nmulti-concept training by design. Like DreamBooth and Textual Inversion, Custom Diffusion is also used to\nteach a pre-trained text-to-image diffusion model about new concepts to generate outputs involving the\nconcept(s) of interest.\n\n## Model Editing\n\n[Paper](https://arxiv.org/abs/2303.08084)", "## Textual inversion\n\n[Textual inversion](../training/text_inversion) is a technique for learning a specific concept from some images which you can use to generate new images conditioned on that concept.\n\nCreate a pipeline and use the [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`] function to load the textual inversion embeddings (feel free to browse the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer) for 100+ trained concepts):\n\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom compel import Compel, DiffusersTextualInversionManager\n\npipe = StableDiffusionPipeline.from_pretrained(\n  \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16,\n  use_safetensors=True, variant=\"fp16\").to(\"cuda\")\npipe.load_textual_inversion(\"sd-concepts-library/midjourney-style\")\n```\n\nCompel provides a `DiffusersTextualInversionManager` class to simplify prompt weighting with textual inversion. Instantiate `DiffusersTextualInversionManager` and pass it to the `Compel` class:\n\n```py\ntextual_inversion_manager = DiffusersTextualInversionManager(pipe)\ncompel_proc = Compel(\n    tokenizer=pipe.tokenizer,\n    text_encoder=pipe.text_encoder,\n    textual_inversion_manager=textual_inversion_manager)\n```\n\nIncorporate the concept to condition a prompt with using the `<concept>` syntax:\n\n```py\nprompt_embeds = compel_proc('(\"A red cat++ playing with a ball <midjourney-style>\")')", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nThen cd in the example folder  and run\n```bash\npip install -r requirements.txt\n```", "## Other Methods\n\nThe quest for easy fine-tuning is not new. In addition to Dreambooth, [_textual inversion_](https://huggingface.co/docs/diffusers/main/en/training/text_inversion) is another popular method that attempts to teach new concepts to a trained Stable Diffusion Model. One of the main reasons for using Textual Inversion is that trained weights are also small and easy to share. However, they only work for a single subject (or a small handful of them), whereas LoRA can be used for general-purpose fine-tuning, meaning that it can be adapted to new domains or datasets.\n\n[Pivotal Tuning](https://arxiv.org/abs/2106.05744) is a method that tries to combine Textual Inversion with LoRA. First, you teach the model a new concept using Textual Inversion techniques, obtaining a new token embedding to represent it. Then, you train that token embedding using LoRA to get the best of both worlds.\n\nWe haven't explored Pivotal Tuning with LoRA yet. Who's up for the challenge? \ud83e\udd17", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nThen cd in the example folder and run:\n```bash\npip install -r requirements.txt\n```", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"\n\npython textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --seed=7 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --max_train_steps=3000 \\\n  --learning_rate=2.5e-03 --scale_lr \\\n  --output_dir=\"textual_inversion_dicoo\"\n```", "Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare Datasets\n\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n\n<a href=\"https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg\">\n    <img src=\"https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg\" width = \"300\" height=\"300\">\n</a>\n\n## Get a FP32 Textual Inversion model\n\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns perfectly with the reference answer, stating that 3-5 images are needed to teach new concepts to Stable Diffusion using Textual Inversion."}, {"question": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n", "true_answer": "10GB", "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md", "generated_answer": "The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB. This feature was introduced in version 4.18.0, and the maximum size can be controlled with the `max_shard_size` parameter. For example, to shard a model checkpoint with a maximum size of 200MB, you can use `max_shard_size=\"200", "retrieved_docs": ["<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```", "```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)", "Finally, you should also check that **all** required weights are\ninitialized and print out all checkpoint weights that were not used for\ninitialization to make sure the model is correctly converted. It is\ncompletely normal, that the conversion trials fail with either a wrong\nshape statement or wrong name assignment. This is most likely because\neither you used incorrect parameters in `BigBirdConfig()`, have a\nwrong architecture in the \ud83e\udd17 Transformers implementation, you have a bug\nin the `init()` functions of one of the components of the \ud83e\udd17 Transformers\nimplementation or you need to transpose one of the checkpoint weights.\n\nThis step should be iterated with the previous step until all weights of\nthe checkpoint are correctly loaded in the Transformers model. Having\ncorrectly loaded the checkpoint into the \ud83e\udd17 Transformers implementation,\nyou can then save the model under a folder of your choice\n`/path/to/converted/checkpoint/folder` that should then contain both a\n`pytorch_model.bin` file and a `config.json` file:\n\n```python\nmodel.save_pretrained(\"/path/to/converted/checkpoint/folder\")\n```\n\n**7. Implement the forward pass**", "| Precision | Required VRAM |\n| --------- | ------------- |\n| float16   | >90 GB        |\n| 8-bit     | >45 GB        |\n| 4-bit     | >23 GB        |\n\n### Using \ud83e\udd17\u00a0Transformers\n\nWith transformers\u00a0[release 4.36](https://github.com/huggingface/transformers/releases/tag/v4.36.0), you can use Mixtral and leverage all the tools within the Hugging Face ecosystem, such as:\n\n- training and inference scripts and examples\n- safe file format (`safetensors`)\n- integrations with tools such as bitsandbytes (4-bit quantization), PEFT (parameter efficient fine-tuning), and Flash Attention 2\n- utilities and helpers to run generation with the model\n- mechanisms to export the models to deploy\n\nMake sure to use the latest\u00a0`transformers`\u00a0release:\n\n```bash\npip install -U \"transformers==4.36.0\" --upgrade\n```\n\nIn the following code snippet, we show how to run inference with \ud83e\udd17 Transformers and 4-bit quantization. Due to the large size of the model, you\u2019ll need a card with at least 30 GB of RAM to run it. This includes cards such as A100 (80 or 40GB versions), or A6000 (48 GB).\n\n```python\nfrom transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_4bit\": True},\n)", "To load such a sharded checkpoint into a model, we just need to loop over the various shards. Accelerate provides a function called `load_checkpoint_in_model` that will do this for you if you have cloned one of the repos of the Hub, or you can directly use the `from_pretrained` method of Transformers, which will handle the downloading and caching for you:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM\n\n# Will error\ncheckpoint = \"facebook/opt-13b\"\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.float16)\n```\n\nIf the device map computed automatically requires some weights to be offloaded on disk because you don't have enough GPU and CPU RAM, you will get an error indicating you need to pass an folder where the weights that should be stored on disk will be offloaded:\n\n```python out\nValueError: The current `device_map` had weights offloaded to the disk. Please provide an \n`offload_folder` for them.\n```\n\nAdding this argument should resolve the error:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM\n\n# Will go out of RAM on Colab\ncheckpoint = \"facebook/opt-13b\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint, device_map=\"auto\", offload_folder=\"offload\", torch_dtype=torch.float16\n)\n```", "Finally, you should also check that **all** required weights are\ninitialized and print out all checkpoint weights that were not used for\ninitialization to make sure the model is correctly converted. It is\ncompletely normal, that the conversion trials fail with either a wrong\nshape statement or wrong name assignment. This is most likely because\neither you used incorrect parameters in `[camelcase name of model]Config()`, have a\nwrong architecture in the \ud83e\udd17 Transformers implementation, you have a bug\nin the `init()` functions of one of the components of the \ud83e\udd17 Transformers\nimplementation or you need to transpose one of the checkpoint weights.\n\nThis step should be iterated with the previous step until all weights of\nthe checkpoint are correctly loaded in the Transformers model. Having\ncorrectly loaded the checkpoint into the \ud83e\udd17 Transformers implementation,\nyou can then save the model under a folder of your choice\n`/path/to/converted/checkpoint/folder` that should then contain both a\n`pytorch_model.bin` file and a `config.json` file:\n\n```python\nmodel.save_pretrained(\"/path/to/converted/checkpoint/folder\")\n```\n\n[TODO FILL: Here the mentor should add very specific information on what exactly has to be done for the conversion of this model]\n[...]\n[...]\n\n**7. Implement the forward pass**", "Finally, you should also check that **all** required weights are initialized and print out all checkpoint weights that\nwere not used for initialization to make sure the model is correctly converted. It is completely normal, that the\nconversion trials fail with either a wrong shape statement or a wrong name assignment. This is most likely because either\nyou used incorrect parameters in `BrandNewBertConfig()`, have a wrong architecture in the \ud83e\udd17 Transformers\nimplementation, you have a bug in the `init()` functions of one of the components of the \ud83e\udd17 Transformers\nimplementation or you need to transpose one of the checkpoint weights.\n\nThis step should be iterated with the previous step until all weights of the checkpoint are correctly loaded in the\nTransformers model. Having correctly loaded the checkpoint into the \ud83e\udd17 Transformers implementation, you can then save\nthe model under a folder of your choice `/path/to/converted/checkpoint/folder` that should then contain both a\n`pytorch_model.bin` file and a `config.json` file:\n\n```python\nmodel.save_pretrained(\"/path/to/converted/checkpoint/folder\")\n```\n\n**7. Implement the forward pass**"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the maximum size of a model checkpoint before it is automatically sharded as 10GB, which is in line with the reference answer. The additional information provided about the feature being introduced in version 4.18.0 and the use of the `max_shard_size` parameter is accurate and relevant, although not required by the instruction. The response goes beyond the reference answer but remains factual and enhances the understanding of the checkpoint sharding feature."}, {"question": "What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n", "true_answer": "To track their machine learning experiments at every stage, from training to production.", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md", "generated_answer": "The context provided does not directly answer the question about the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists. However, based on the information available, it can be inferred that W&B is not directly related to the discussion about biases in machine learning. Therefore, an answer cannot be deduced from the context.", "retrieved_docs": ["--\ntitle: \"Let's talk about biases in machine learning! Ethics and Society Newsletter #2\" \nthumbnail: /blog/assets/122_ethics_soc_2/thumbnail-solstice.png\nauthors:\n- user: yjernite\n---\n\n# Machine Learning in development: Let's talk about bias!\n\n\n_Bias in ML is ubiquitous, and Bias in ML is complex; so complex in fact that no single technical intervention is likely to meaningfully address the problems it engenders. ML models, as sociotechnical systems, amplify social trends that may exacerbate inequities and harmful biases in ways that depend on their deployment context and are constantly evolving._\n\n_This means that developing ML systems with care requires vigilance and responding to feedback from those deployment contexts, which in turn we can facilitate by sharing lessons across contexts and developing tools to analyze signs of bias at every level of ML development._", "## Limitations and Bias\n*Note any limitations or biases that the metric has.*\n\n\n## Citation\n```bibtex\n@article{scikit-learn,\n  title={Scikit-learn: Machine Learning in {P}ython},\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n  journal={Journal of Machine Learning Research},\n  volume={12},\n  pages={2825--2830},\n  year={2011}\n}\n```\n\n## Further References\n\n- This Hugging Face implementation uses [this scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)", "## Limitations and Bias\n\n\n## Citation(s)\n```bibtex\n@article{scikit-learn, title={Scikit-learn: Machine Learning in {P}ython}, author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.}, journal={Journal of Machine Learning Research}, volume={12}, pages={2825--2830}, year={2011}\n```\n\n\n## Further References", "## Limitations and Bias\n\n\n## Citation(s)\n```bibtex\n@article{scikit-learn, title={Scikit-learn: Machine Learning in {P}ython}, author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.}, journal={Journal of Machine Learning Research}, volume={12}, pages={2825--2830}, year={2011}\n```\n\n\n## Further References", "So, who\u2019s on the hook for machine biases in ML? These three cases illustrate one of the reasons why discussions about the responsibility of ML developers in addressing bias can get so complicated: depending on decisions made at other points in the ML system development process by other people, the biases in an ML dataset or model may land anywhere between being irrelevant to the application settings and directly leading to grievous harm. However, in all of these cases, **stronger biases in the model/dataset increase the risk of negative outcomes**. The European Union has started to develop frameworks that address this phenomenon in [recent regulatory efforts](https://ec.europa.eu/info/business-economy-euro/doing-business-eu/contract-rules/digital-contracts/liability-rules-artificial-intelligence_en): in short, a company that deploys an AI system based on a measurably biased model is liable for harm caused by the system.\n\nConceptualizing bias as a risk factor then allows us to better understand the **shared responsibility** for machine biases between developers at all stages. Bias can never be fully removed, not least because the definitions of social biases and the power dynamics that tie them to discrimination vary vastly across social contexts. However:", "A promising bias mitigation strategy at this stage has been to reframe the task to explicitly [model both engagement and diversity](https://dl.acm.org/doi/10.1145/3437963.3441775) when applying ML to algorithmic content recommendation. Users are likely to get more long-term satisfaction and the risk of exacerbating biases as outlined above is reduced!\n\nThis example serves to illustrate that the impact of machine biases in an ML-supported product depends not just on where we decide to leverage ML, but also on how ML techniques are integrated into the broader technical system, and with what objective. When first investigating how ML can fit into a product or a use case you are interested in, we first recommend looking for the failure modes of the system through the lens of bias before even diving into the available models or datasets - which behaviors of existing systems in the space will be particularly harmful or more likely to occur if bias is exacerbated by ML predictions?", "Bias and limitations[[bias-and-limitations]]\n\n<CourseFloatingBanner chapter={1}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section8.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter1/section8.ipynb\"},\n]} />\n\nIf your intent is to use a pretrained model or a fine-tuned version in production, please be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet. \n\nTo give a quick illustration, let's go back the example of a `fill-mask` pipeline with the BERT model:\n\n```python\nfrom transformers import pipeline\n\nunmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\nresult = unmasker(\"This man works as a [MASK].\")\nprint([r[\"token_str\"] for r in result])\n\nresult = unmasker(\"This woman works as a [MASK].\")\nprint([r[\"token_str\"] for r in result])\n```\n\n```python out\n['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']\n['nurse', 'waitress', 'teacher', 'maid', 'prostitute']\n```"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response does not provide any factual information about the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists. It fails to mention that W&B is used to track machine learning experiments, which is the core purpose as per the reference answer. Instead, the response seems to confuse the tool with the concept of biases in machine learning, which is unrelated to the actual functionality of W&B. Therefore, the response is completely incorrect and not factual in the context of the question asked."}, {"question": "What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n", "true_answer": "Optimum", "source_doc": "huggingface/blog/blob/main/intel.md", "generated_answer": "The open-source library created by Hugging Face to simplify Transformer acceleration is called Optimum. (Source: Extract 1)\n\nAnswer: \n\nThe question asks for the name of the open-source library created by Hugging Face to simplify Transformer acceleration. The answer is Optimum, as mentioned in Extract 1.\n\nNote: If the question had asked for the name of the library that delivers unified interfaces across", "retrieved_docs": ["## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.", "The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let\u2019s take a deeper look at it.\n\n## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance. \n\n[Optimum Intel](https://github.com/huggingface/optimum-intel) is part of Optimum and builds on top of the [Intel Neural Compressor](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) (INC). INC is an [open-source library](https://github.com/intel/neural-compressor) that delivers unified interfaces across multiple deep learning frameworks for popular network compression technologies, such as quantization, pruning, and knowledge distillation. This tool supports automatic accuracy-driven tuning strategies to help users quickly build the best quantized model.", "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Philosophy\n\n\ud83e\udd17 Transformers is an opinionated library built for:\n\n- machine learning researchers and educators seeking to use, study or extend large-scale Transformers models.\n- hands-on practitioners who want to fine-tune those models or serve them in production, or both.\n- engineers who just want to download a pretrained model and use it to solve a given machine learning task.\n\nThe library was designed with two strong goals in mind:\n\n1. Be as easy and fast to use as possible:", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Distributed training with \ud83e\udd17 Accelerate\n\nAs models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude. At Hugging Face, we created the [\ud83e\udd17 Accelerate](https://huggingface.co/docs/accelerate) library to help users easily train a \ud83e\udd17 Transformers model on any type of distributed setup, whether it is multiple GPU's on one machine or multiple GPU's across several machines. In this tutorial, learn how to customize your native PyTorch training loop to enable training in a distributed environment.\n\n## Setup\n\nGet started by installing \ud83e\udd17 Accelerate:\n\n```bash\npip install accelerate\n```", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Contribute to \ud83e\udd17 Transformers\n\nEveryone is welcome to contribute, and we value everybody's contribution. Code\ncontributions are not the only way to help the community. Answering questions, helping\nothers, and improving the documentation are also immensely valuable.\n\nIt also helps us if you spread the word! Reference the library in blog posts\nabout the awesome projects it made possible, shout out on Twitter every time it has\nhelped you, or simply \u2b50\ufe0f the repository to say thank you.\n\nHowever you choose to contribute, please be mindful and respect our\n[code of conduct](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md).\n\n**This guide was heavily inspired by the awesome [scikit-learn guide to contributing](https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md).**\n\n## Ways to contribute\n\nThere are several ways you can contribute to \ud83e\udd17 Transformers:", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Contribute to \ud83e\udd17 Transformers\n\nEveryone is welcome to contribute, and we value everybody's contribution. Code\ncontributions are not the only way to help the community. Answering questions, helping\nothers, and improving the documentation are also immensely valuable.\n\nIt also helps us if you spread the word! Reference the library in blog posts\nabout the awesome projects it made possible, shout out on Twitter every time it has\nhelped you, or simply \u2b50\ufe0f the repository to say thank you.\n\nHowever you choose to contribute, please be mindful and respect our\n[code of conduct](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md).\n\n**This guide was heavily inspired by the awesome [scikit-learn guide to contributing](https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md).**\n\n## Ways to contribute\n\nThere are several ways you can contribute to \ud83e\udd17 Transformers:", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Adding a new model\n\nThis folder contains templates to generate new models that fit the current API and pass all tests. It generates\nmodels in both PyTorch, TensorFlow, and Flax and completes the `__init__.py` and auto-modeling files, and creates the\ndocumentation. Their use is described in the [next section](#cookiecutter-templates).\n\nThere is also a CLI tool to generate a new model like an existing one called `transformers-cli add-new-model-like`.\nJump to the [Add new model like section](#add-new-model-like-command) to learn how to use it.\n\n## Cookiecutter Templates\n\nUsing the `cookiecutter` utility requires to have all the `dev` dependencies installed. Let's first clone the \nrepository and install it in our environment:\n\n```shell script\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install -e \".[dev]\"\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that Optimum is the open-source library created by Hugging Face to simplify Transformer acceleration."}, {"question": "What parameter is used to ensure that elements in a row have the same height in Gradio?\n", "true_answer": "equal_height", "source_doc": "gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md", "generated_answer": "The `scale` parameter is used to ensure that elements in a row have the same height in Gradio, by setting it to the same value for all components in the row. However, this will only work if the components have the same content and style. If you want to ensure that all components in a row have the same height regardless of their content and style, you can use the `min_width` parameter for the Row component to set a minimum width for all components in the row,", "retrieved_docs": ["```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn0 = gr.Button(\"Button 0\", scale=0)\n        btn1 = gr.Button(\"Button 1\", scale=1)\n        btn2 = gr.Button(\"Button 2\", scale=2)\n```\n\n- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space to satisfy all `min_width` values.\n\nLearn more about Rows in the [docs](https://gradio.app/docs/#row).\n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n\n$code_rows_and_columns\n$demo_rows_and_columns\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/#column).\n\n# Dimensions\n\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases.", "Below is an example illustrating the use of viewport width (vw):\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    im = gr.ImageEditor(\n        width=\"50vw\",\n    )\n\ndemo.launch()\n```\n\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n\n\n```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    with gr.Column(elem_classes=[\"container\"]):\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\n\ndemo.launch()\n```\n\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.\n\nYou can apply any valid CSS unit for these parameters. For a comprehensive list of CSS units, refer to [this guide](https://www.w3schools.com/cssref/css_units.php). We recommend you always consider responsiveness and test your interfaces on various screen sizes to ensure a consistent user experience.\n\n\n\n## Tabs and Accordions\n\nYou can also create Tabs using the `with gr.Tab('tab_name'):` clause. Any component created inside of a `with gr.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.\n\nFor example:", "- Fixed bug where textbox shrinks when `lines` set to larger than 20 by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3637](https://github.com/gradio-app/gradio/pull/3637)\n- Ensure CSS has fully loaded before rendering the application, by [@pngwn](https://github.com/pngwn) in [PR 3573](https://github.com/gradio-app/gradio/pull/3573)\n- Support using an empty list as `gr.Dataframe` value, by [@space-nuko](https://github.com/space-nuko) in [PR 3646](https://github.com/gradio-app/gradio/pull/3646)\n- Fixed `gr.Image` not filling the entire element size, by [@space-nuko](https://github.com/space-nuko) in [PR 3649](https://github.com/gradio-app/gradio/pull/3649)\n- Make `gr.Code` support the `lines` property, by [@space-nuko](https://github.com/space-nuko) in [PR 3651](https://github.com/gradio-app/gradio/pull/3651)\n- Fixes certain `_js` return values being double wrapped in an array, by [@space-nuko](https://github.com/space-nuko) in [PR 3594](https://github.com/gradio-app/gradio/pull/3594)\n- Correct the documentation of `gr.File` component to state that its preprocessing method converts the uploaded file to a temporary file, by @RussellLuo in [PR 3660](https://github.com/gradio-app/gradio/pull/3660)\n- Fixed bug in Serializer ValueError text by [@osanseviero](https://github.com/osanseviero) in [PR 3669](https://github.com/gradio-app/gradio/pull/3669)", "- Fixed bug where textbox shrinks when `lines` set to larger than 20 by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3637](https://github.com/gradio-app/gradio/pull/3637)\n- Ensure CSS has fully loaded before rendering the application, by [@pngwn](https://github.com/pngwn) in [PR 3573](https://github.com/gradio-app/gradio/pull/3573)\n- Support using an empty list as `gr.Dataframe` value, by [@space-nuko](https://github.com/space-nuko) in [PR 3646](https://github.com/gradio-app/gradio/pull/3646)\n- Fixed `gr.Image` not filling the entire element size, by [@space-nuko](https://github.com/space-nuko) in [PR 3649](https://github.com/gradio-app/gradio/pull/3649)\n- Make `gr.Code` support the `lines` property, by [@space-nuko](https://github.com/space-nuko) in [PR 3651](https://github.com/gradio-app/gradio/pull/3651)\n- Fixes certain `_js` return values being double wrapped in an array, by [@space-nuko](https://github.com/space-nuko) in [PR 3594](https://github.com/gradio-app/gradio/pull/3594)\n- Correct the documentation of `gr.File` component to state that its preprocessing method converts the uploaded file to a temporary file, by @RussellLuo in [PR 3660](https://github.com/gradio-app/gradio/pull/3660)\n- Fixed bug in Serializer ValueError text by [@osanseviero](https://github.com/osanseviero) in [PR 3669](https://github.com/gradio-app/gradio/pull/3669)", "* Add missing `display: flex` property to `Row` so that flex styling is applied to children by [@hannahblair] in [PR 4896](https://github.com/gradio-app/gradio/pull/4896)\n* Fixed bug where `gr.Video` could not preprocess urls by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4904](https://github.com/gradio-app/gradio/pull/4904)\n* Fixed copy button rendering in API page on Safari by [@aliabid94](https://github.com/aliabid94) in [PR 4924](https://github.com/gradio-app/gradio/pull/4924)\n* Fixed `gr.Group` and `container=False`. `container` parameter only available for `Textbox`, `Number`, and `Dropdown`, the only elements where it makes sense. By [@aliabid94](https://github.com/aliabid94) in [PR 4916](https://github.com/gradio-app/gradio/pull/4916)\n* Fixed broken image link in auto-generated `app.py` from `ThemeClass.push_to_hub` by [@deepkyu](https://github.com/deepkyu) in [PR 4944](https://github.com/gradio-app/gradio/pull/4944)\n\n### Other Changes:", "* Add missing `display: flex` property to `Row` so that flex styling is applied to children by [@hannahblair] in [PR 4896](https://github.com/gradio-app/gradio/pull/4896)\n* Fixed bug where `gr.Video` could not preprocess urls by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4904](https://github.com/gradio-app/gradio/pull/4904)\n* Fixed copy button rendering in API page on Safari by [@aliabid94](https://github.com/aliabid94) in [PR 4924](https://github.com/gradio-app/gradio/pull/4924)\n* Fixed `gr.Group` and `container=False`. `container` parameter only available for `Textbox`, `Number`, and `Dropdown`, the only elements where it makes sense. By [@aliabid94](https://github.com/aliabid94) in [PR 4916](https://github.com/gradio-app/gradio/pull/4916)\n* Fixed broken image link in auto-generated `app.py` from `ThemeClass.push_to_hub` by [@deepkyu](https://github.com/deepkyu) in [PR 4944](https://github.com/gradio-app/gradio/pull/4944)\n\n### Other Changes:", "Gradio Demo: blocks_layout\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    with gr.Row():\n        gr.Image(interactive=True, scale=2)\n        gr.Image()\n    with gr.Row():\n        gr.Textbox(label=\"Text\")\n        gr.Number(label=\"Count\", scale=2)\n        gr.Radio(choices=[\"One\", \"Two\"])\n    with gr.Row():\n        gr.Button(\"500\", scale=0, min_width=500)\n        gr.Button(\"A\", scale=0)\n        gr.Button(\"grow\")\n    with gr.Row():\n        gr.Textbox()\n        gr.Textbox()\n        gr.Button()        \n    with gr.Row():\n        with gr.Row():\n            with gr.Column():\n                gr.Textbox(label=\"Text\")\n                gr.Number(label=\"Count\")\n                gr.Radio(choices=[\"One\", \"Two\"])\n            gr.Image()\n            with gr.Column():\n                gr.Image(interactive=True)\n                gr.Image()\n    gr.Image()\n    gr.Textbox(label=\"Text\")\n    gr.Number(label=\"Count\")\n    gr.Radio(choices=[\"One\", \"Two\"])\n\n\nif __name__ == \"__main__\":\n    demo.launch()\n\n```"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided is incorrect. The correct parameter for ensuring that elements in a row have the same height in Gradio is 'equal_height', not 'scale' or 'min_width'. The 'scale' parameter is not related to the height of elements in Gradio, and 'min_width' is concerned with the width of components, not their height. Therefore, the response does not match the reference answer and is not factual in the context of the question asked."}, {"question": "What is the command to install the latest version of Optimum with OpenVINO support?\n", "true_answer": "pip install --upgrade-strategy eager optimum[\"openvino\"]", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md", "generated_answer": "Answer: To install the latest version of Optimum with OpenVINO support, you can use the following command with the `--upgrade-strategy eager` option:\n\n```bash\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\nThis will ensure that the latest version of Optimum is installed, including the necessary packages for OpenVINO support.", "retrieved_docs": ["The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nTo install from source:\n\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\n\nFor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:\n\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```\n\n## Accelerated Inference\n\n\ud83e\udd17 Optimum provides multiple tools to export and run optimized models on various ecosystems: \n\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models)\n- TensorFlow Lite\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)\n- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_inference)\n\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line.\n\n### Features summary", "### OpenVINO\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install --upgrade-strategy eager optimum[openvino,nncf]\n```\n\nIt is possible to export \ud83e\udd17 Transformers and Diffusers models to the OpenVINO format easily:\n\n```bash\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert_sst2_ov\n```\n\nIf you add `--int8`, the weights will be quantized to INT8. Static quantization can also be applied on the activations using [NNCF](https://github.com/openvinotoolkit/nncf), more information can be found in the [documentation](https://huggingface.co/docs/optimum/main/en/intel/optimization_ov).\n\nTo load a model and run inference with OpenVINO Runtime, you can just replace your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. To load a PyTorch checkpoint and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model.\n\n```diff\n- from transformers import AutoModelForSequenceClassification\n+ from optimum.intel import OVModelForSequenceClassification\n  from transformers import AutoTokenizer, pipeline\n\n  model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n- model = AutoModelForSequenceClassification.from_pretrained(model_id)\n+ model = OVModelForSequenceClassification.from_pretrained(\"distilbert_sst2_ov\")", "The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you can install the base library from source as follows:\n\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\n\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator_type]` to the `pip` command, e.g.\n\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```", "## Optimum Intel\n\n### OpenVINO", "## Optimum Intel\n\n### OpenVINO", "This first release is based on OpenVINO 2022.2 and enables inference for a large quantity of PyTorch models using our [`OVModels`](https://huggingface.co/docs/optimum/intel/inference). Post-training static quantization and quantization aware training can be applied on many encoder models (BERT, DistilBERT, etc.). More encoder models will be supported in the upcoming OpenVINO release. Currently the quantization of Encoder Decoder models is not enabled, however this restriction should be lifted with our integration of the next OpenVINO release.\n\n\u200bLet us show you how to get started in minutes!\u200b\n\n## Quantizing a Vision Transformer with Optimum Intel and OpenVINO\n\u200b\nIn this example, we will run post-training static quantization on a Vision Transformer (ViT) [model](https://huggingface.co/juliensimon/autotrain-food101-1471154050) fine-tuned for image classification on the [food101](https://huggingface.co/datasets/food101) dataset. \n\u200b\n\nQuantization is a process that lowers memory and compute requirements by reducing the bit width of model parameters. Reducing the number of bits means that the resulting model requires less memory at inference time, and that operations like matrix multiplication can be performed faster thanks to integer arithmetic.\n\nFirst, let's create a virtual environment and install all dependencies.\u200b\n\n```bash\nvirtualenv openvino\nsource openvino/bin/activate\npip install pip --upgrade\npip install optimum[openvino,nncf] torchvision evaluate\n```", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# OpenVINO\n\n\ud83e\udd17 [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html) of supported devices).\n\nYou'll need to install \ud83e\udd17 Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\n\n```bash\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO.\n\n## Stable Diffusion\n\nTo load and run inference, use the [`~optimum.intel.OVStableDiffusionPipeline`]. If you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set `export=True`:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer and includes the correct command to install the latest version of Optimum with OpenVINO support. The additional explanation about the `--upgrade-strategy eager` option is accurate and provides context to the command, which is beneficial for understanding the purpose of the option."}]